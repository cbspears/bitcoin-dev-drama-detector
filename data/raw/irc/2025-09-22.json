{
  "source": "irc",
  "channel": "#bitcoin-core-dev",
  "fetched_at": "2026-01-15T07:00:20.473520+00:00",
  "date_range": {
    "since": "2025-09-22T06:58:54.084595+00:00",
    "until": "2025-09-22T06:58:54.084595+00:00"
  },
  "logs": [
    {
      "date": "2025-09-22",
      "channel": "#bitcoin-core-dev",
      "message_count": 75,
      "participant_count": 4,
      "participants": [
        "instagibbs",
        "sipa",
        "darosior",
        "_aj_"
      ],
      "messages": [
        {
          "type": "message",
          "timestamp": "2025-09-22 02:55:00",
          "user": "sipa",
          "content": "_aj_: in what way does the formula i implemented differ from what you thought?",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 78,
            "has_nack": false,
            "has_ack": false
          }
        },
        {
          "type": "message",
          "timestamp": "2025-09-22 02:59:00",
          "user": "sipa",
          "content": "ah, the use of time-since-send, rather than a fixed % per trickle?",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 66,
            "has_nack": false,
            "has_ack": false
          }
        },
        {
          "type": "message",
          "timestamp": "2025-09-22 03:01:00",
          "user": "_aj_",
          "content": "sipa: i was making the drain rate proportional to the incoming rate, rather than the queue size?",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 96,
            "has_nack": false,
            "has_ack": false
          }
        },
        {
          "type": "message",
          "timestamp": "2025-09-22 03:03:00",
          "user": "sipa",
          "content": "08:12:33 < sipa> it should just be past_avg -= to_send",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 54,
            "has_nack": false,
            "has_ack": false
          }
        },
        {
          "type": "message",
          "timestamp": "2025-09-22 03:03:00",
          "user": "sipa",
          "content": "08:12:46 < sipa> .. and then past_avg just becomes equal to the queue size",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 74,
            "has_nack": false,
            "has_ack": false
          }
        },
        {
          "type": "message",
          "timestamp": "2025-09-22 03:04:00",
          "user": "sipa",
          "content": "i stopped using a separate variable for tracking rate here, because the queue size is an exact proxy for it",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 1,
            "text_length": 107,
            "has_nack": false,
            "has_ack": true
          }
        },
        {
          "type": "message",
          "timestamp": "2025-09-22 03:05:00",
          "user": "_aj_",
          "content": "hmm",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 3,
            "has_nack": false,
            "has_ack": false
          }
        },
        {
          "type": "message",
          "timestamp": "2025-09-22 03:05:00",
          "user": "_aj_",
          "content": "well then it must just be that i was using the average time rather than the actual time?",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 88,
            "has_nack": false,
            "has_ack": false
          }
        },
        {
          "type": "message",
          "timestamp": "2025-09-22 03:06:00",
          "user": "sipa",
          "content": "yeah",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 4,
            "has_nack": false,
            "has_ack": false
          }
        },
        {
          "type": "message",
          "timestamp": "2025-09-22 03:06:00",
          "user": "_aj_",
          "content": "right, because we stopped scaling past_avg down by q",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 52,
            "has_nack": false,
            "has_ack": false
          }
        },
        {
          "type": "message",
          "timestamp": "2025-09-22 03:06:00",
          "user": "sipa",
          "content": "also, you use max(target, queue_size * (1-q)) rather than (target + queue_size * (1-q))",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 87,
            "has_nack": false,
            "has_ack": false
          }
        },
        {
          "type": "message",
          "timestamp": "2025-09-22 03:06:00",
          "user": "_aj_",
          "content": "sure, but that should be a trivial difference",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 45,
            "has_nack": false,
            "has_ack": false
          }
        },
        {
          "type": "message",
          "timestamp": "2025-09-22 03:07:00",
          "user": "sipa",
          "content": "indeed",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 6,
            "has_nack": false,
            "has_ack": false
          }
        },
        {
          "type": "message",
          "timestamp": "2025-09-22 03:09:00",
          "user": "_aj_",
          "content": "so this is just saying either max(70, 0.08 * inv_to_send.size()) or 70 + 0.08 * inv_to_send.size() ?",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 100,
            "has_nack": false,
            "has_ack": false
          }
        },
        {
          "type": "message",
          "timestamp": "2025-09-22 03:09:00",
          "user": "_aj_",
          "content": "and a queue of 5000 gives a inv of size 400 or 470 or so",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 56,
            "has_nack": false,
            "has_ack": false
          }
        },
        {
          "type": "message",
          "timestamp": "2025-09-22 03:09:00",
          "user": "sipa",
          "content": "right",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 5,
            "has_nack": false,
            "has_ack": false
          }
        },
        {
          "type": "message",
          "timestamp": "2025-09-22 03:10:00",
          "user": "sipa",
          "content": "you think using actual time is worse than avg_time?",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 51,
            "has_nack": false,
            "has_ack": false
          }
        },
        {
          "type": "message",
          "timestamp": "2025-09-22 03:12:00",
          "user": "_aj_",
          "content": "i think having a queue of only 5000 and sending a single inv with 2000 entries seems bad/unnecessary? i can't remember what it takes for us to start ignoring invs though",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 169,
            "has_nack": false,
            "has_ack": false
          }
        },
        {
          "type": "message",
          "timestamp": "2025-09-22 03:13:00",
          "user": "_aj_",
          "content": "the more predictable the size of our invs are over some unit of time, the better for erlay, i think?",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 100,
            "has_nack": false,
            "has_ack": false
          }
        },
        {
          "type": "message",
          "timestamp": "2025-09-22 03:15:00",
          "user": "_aj_",
          "content": "so 0.08 there is just changing the current 5-per-1000 logic from an avg-time-in-q  of ~1000 seconds to 60 seconds",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 113,
            "has_nack": false,
            "has_ack": false
          }
        },
        {
          "type": "message",
          "timestamp": "2025-09-22 03:16:00",
          "user": "_aj_",
          "content": "or changing 5-per-1000 to ~5-per-60",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 35,
            "has_nack": false,
            "has_ack": false
          }
        },
        {
          "type": "message",
          "timestamp": "2025-09-22 03:16:00",
          "user": "sipa",
          "content": "right",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 5,
            "has_nack": false,
            "has_ack": false
          }
        },
        {
          "type": "message",
          "timestamp": "2025-09-22 03:18:00",
          "user": "sipa",
          "content": "i like the real-time formula better because it means the amount sent per time is less variable (because it doesn't depend on the trickle timing)",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 144,
            "has_nack": false,
            "has_ack": false
          }
        },
        {
          "type": "message",
          "timestamp": "2025-09-22 03:18:00",
          "user": "sipa",
          "content": "but of course the amount sent per trickle is more variable",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 58,
            "has_nack": false,
            "has_ack": false
          }
        },
        {
          "type": "message",
          "timestamp": "2025-09-22 03:23:00",
          "user": "_aj_",
          "content": "should just be \"0.08 * time_since / target\" without the exp() stuff in the case?",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 80,
            "has_nack": false,
            "has_ack": false
          }
        },
        {
          "type": "message",
          "timestamp": "2025-09-22 03:24:00",
          "user": "_aj_",
          "content": "\"that case\". why would my fingers type \"the\" there, seriously.",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 62,
            "has_nack": false,
            "has_ack": false
          }
        },
        {
          "type": "message",
          "timestamp": "2025-09-22 03:26:00",
          "user": "sipa",
          "content": "_aj_: sure, that also has the \"just a function of time\" property, but no longer has an average-time-in-queue that's independent of incoming rate",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 144,
            "has_nack": false,
            "has_ack": false
          }
        },
        {
          "type": "message",
          "timestamp": "2025-09-22 03:29:00",
          "user": "_aj_",
          "content": "sipa: that property feels contradictory to the goal of smoothing out spikes -- doesn't smoothing out spikes imply that a high incoming rate should be associated with a higher aveage-time-in-queue?",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 196,
            "has_nack": false,
            "has_ack": false
          }
        },
        {
          "type": "message",
          "timestamp": "2025-09-22 03:30:00",
          "user": "sipa",
          "content": "_aj_: at low/reasonable rate, yes; at high rate, i think a fixed avg-time-in-queue is desirable because the alternative is a possibility for a quickly growing queue",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 164,
            "has_nack": false,
            "has_ack": false
          }
        },
        {
          "type": "message",
          "timestamp": "2025-09-22 03:32:00",
          "user": "sipa",
          "content": "the alternative is dropping transactions at the source",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 54,
            "has_nack": false,
            "has_ack": false
          }
        },
        {
          "type": "message",
          "timestamp": "2025-09-22 03:35:00",
          "user": "sipa",
          "content": "the earlier quadratic formula i suggest was even more aggressive, in that it results in a decreasing time in queue with increasing rate",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 135,
            "has_nack": false,
            "has_ack": false
          }
        },
        {
          "type": "message",
          "timestamp": "2025-09-22 03:36:00",
          "user": "_aj_",
          "content": "i guess there's a few different rates -- ~5-7 tx/s is what we see normally; < 14 tx/s is what we cope with without this; sustained <80 tx/s would give us a queue of ~5000 which seems fine; but dumping 50k or 100k txs (12MB?) in short order is possible too...",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 258,
            "has_nack": false,
            "has_ack": false
          }
        },
        {
          "type": "message",
          "timestamp": "2025-09-22 03:40:00",
          "user": "sipa",
          "content": "should we have 3 \"parameter rates\", 1 for \"no delay\", 1 for \"smooth out\", 1 for \"drop stuff\" ?",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 94,
            "has_nack": false,
            "has_ack": false
          }
        },
        {
          "type": "message",
          "timestamp": "2025-09-22 03:42:00",
          "user": "_aj_",
          "content": "could we rate limit in RelayTransaction? use a token bucket with an 80/s rate, and just create a global queue of transactions to relay onwards once we have tokens again if we run out?",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 183,
            "has_nack": false,
            "has_ack": false
          }
        },
        {
          "type": "message",
          "timestamp": "2025-09-22 03:43:00",
          "user": "sipa",
          "content": "that lacks the nice quality-sorting we can do at inv time",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 1,
            "text_length": 57,
            "has_nack": false,
            "has_ack": true
          }
        },
        {
          "type": "message",
          "timestamp": "2025-09-22 03:43:00",
          "user": "_aj_",
          "content": "we could quality-sort the RelayTransaction queue",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 48,
            "has_nack": false,
            "has_ack": false
          }
        },
        {
          "type": "message",
          "timestamp": "2025-09-22 03:44:00",
          "user": "sipa",
          "content": "that would have the cpu-resource benefit of only doing the sorting oncew",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 72,
            "has_nack": false,
            "has_ack": false
          }
        },
        {
          "type": "message",
          "timestamp": "2025-09-22 03:44:00",
          "user": "sipa",
          "content": "rather than for every peer",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 26,
            "has_nack": false,
            "has_ack": false
          }
        },
        {
          "type": "message",
          "timestamp": "2025-09-22 03:46:00",
          "user": "_aj_",
          "content": "maybe? we'd need to go through the entire queue every now and then to drop stuff that's been removed from the mempool?",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 118,
            "has_nack": false,
            "has_ack": false
          }
        },
        {
          "type": "message",
          "timestamp": "2025-09-22 03:48:00",
          "user": "_aj_",
          "content": "maybe as a global queue the mempool could manage it and directly remove txs from it though",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 90,
            "has_nack": false,
            "has_ack": false
          }
        },
        {
          "type": "message",
          "timestamp": "2025-09-22 03:49:00",
          "user": "sipa",
          "content": "but you'd still need per-peer data to know what has been relayed to them alreasy",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 80,
            "has_nack": false,
            "has_ack": false
          }
        },
        {
          "type": "message",
          "timestamp": "2025-09-22 03:49:00",
          "user": "sipa",
          "content": "though, the alreadyknown filter already partially has this information",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 70,
            "has_nack": false,
            "has_ack": false
          }
        },
        {
          "type": "message",
          "timestamp": "2025-09-22 03:50:00",
          "user": "sipa",
          "content": "this is sounding a bit like dzxzg's idea earlier",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 48,
            "has_nack": false,
            "has_ack": false
          }
        },
        {
          "type": "message",
          "timestamp": "2025-09-22 04:01:00",
          "user": "_aj_",
          "content": "sipa: if you cap the RelayTransaction rate at 80/s, and do 0.08*q for inv, i think that caps the per-peer q at perhaps 5000 txs?",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 128,
            "has_nack": false,
            "has_ack": false
          }
        },
        {
          "type": "message",
          "timestamp": "2025-09-22 04:02:00",
          "user": "_aj_",
          "content": "sipa: (ie, 1000 in the queue due to sustained load, plus a minute's worth of 80/s txs for another 4800, where a minute between invs happens maybe once every 9 days?",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 164,
            "has_nack": false,
            "has_ack": false
          }
        },
        {
          "type": "message",
          "timestamp": "2025-09-22 04:18:00",
          "user": "_aj_",
          "content": "sipa: maybe could have a set<txiter> of unrelayed txs managed by the mempool, sorted by quality; and use the quality lookup to remove tx from the set when they're removed from the mempool? would be removing and reinserting when the quality score changes (if chunking changes?) though",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 283,
            "has_nack": false,
            "has_ack": false
          }
        },
        {
          "type": "message",
          "timestamp": "2025-09-22 04:23:00",
          "user": "sipa",
          "content": "_aj_: and refcounted by how many peers it still needs to be relayed to?",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 71,
            "has_nack": false,
            "has_ack": false
          }
        },
        {
          "type": "message",
          "timestamp": "2025-09-22 04:25:00",
          "user": "_aj_",
          "content": "sipa: i think it gets relayed to everyone that's currently connected as soon as it gets a token?",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 96,
            "has_nack": false,
            "has_ack": false
          }
        },
        {
          "type": "message",
          "timestamp": "2025-09-22 04:28:00",
          "user": "_aj_",
          "content": "sipa: i guess could track time and compare against CNode::m_connected or perhaps track VERACK time or mempool sequence?",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 1,
            "text_length": 119,
            "has_nack": false,
            "has_ack": true
          }
        },
        {
          "type": "message",
          "timestamp": "2025-09-22 04:33:00",
          "user": "sipa",
          "content": "_aj_: could have a timeout, where transactions can only stay in the set of unrelayed txn for a finite amount of time, and if they reach that timeout (not evicted by better relayable ones, and not yet relayed to everyone), just blast it out",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 239,
            "has_nack": false,
            "has_ack": false
          }
        },
        {
          "type": "message",
          "timestamp": "2025-09-22 04:37:00",
          "user": "_aj_",
          "content": "sipa: if you had a boost::multi_index instead of a set, a timeout would be easy to manage. would've gone with dropping (not relaying) rather than blasting out though. a set<txiter> is 32B per tx, so 100k txs in the queue is just 3MB, and if you've got 100k backlog dropping stuff at the bottom seems fine",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 1,
            "text_length": 304,
            "has_nack": false,
            "has_ack": true
          }
        },
        {
          "type": "message",
          "timestamp": "2025-09-22 04:37:00",
          "user": "_aj_",
          "content": "sipa: oh, the other thing you could do is increase the mempool minfee when the to-relay queue gets too big",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 106,
            "has_nack": false,
            "has_ack": false
          }
        },
        {
          "type": "message",
          "timestamp": "2025-09-22 04:39:00",
          "user": "_aj_",
          "content": "(or have a separate \"min incoming fee\" and increase that and apply it to feefilter)",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 83,
            "has_nack": false,
            "has_ack": false
          }
        },
        {
          "type": "message",
          "timestamp": "2025-09-22 06:01:00",
          "user": "sipa",
          "content": "_aj_: i'm wondering if post-cluster-mempool, we could do something like keep per-peer an iterator into the chunk index (\"mostly relayed every chunk with feerates higher than this\") + a set of unrelayed transactions before that point",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 232,
            "has_nack": false,
            "has_ack": false
          }
        },
        {
          "type": "message",
          "timestamp": "2025-09-22 06:02:00",
          "user": "sipa",
          "content": "and at inv time, first take from the unrelayed set (using the existing heapify approach, i guess), and if it empties, start advancing the iterator",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 146,
            "has_nack": false,
            "has_ack": false
          }
        },
        {
          "type": "message",
          "timestamp": "2025-09-22 06:03:00",
          "user": "sipa",
          "content": "effectively making the entire chunk index the set of to-be-relayed transactions",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 79,
            "has_nack": false,
            "has_ack": false
          }
        },
        {
          "type": "message",
          "timestamp": "2025-09-22 06:05:00",
          "user": "sipa",
          "content": "it doesn't even need to be an iterator; for layer separation it'd be fine to just store a per-peer chunk feerate, and a set of transactions whose chunk feerate is (or was, at some point) higher than that",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 203,
            "has_nack": false,
            "has_ack": false
          }
        },
        {
          "type": "message",
          "timestamp": "2025-09-22 07:39:00",
          "user": "_aj_",
          "content": "sipa: i don't see how that would work at all? we'll get new transactions both above and below the iterator, and need to keep track of anything not relayed in either case?",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 1,
            "text_length": 170,
            "has_nack": false,
            "has_ack": true
          }
        },
        {
          "type": "message",
          "timestamp": "2025-09-22 07:41:00",
          "user": "sipa",
          "content": "well you don't need to keep track of any transaction with a feerate below the \"up to this feerate\" value",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 1,
            "text_length": 104,
            "has_nack": false,
            "has_ack": true
          }
        },
        {
          "type": "message",
          "timestamp": "2025-09-22 07:41:00",
          "user": "sipa",
          "content": "because it's just implicitly all transactions whose feerate below that point that still need to be relayed",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 106,
            "has_nack": false,
            "has_ack": false
          }
        },
        {
          "type": "message",
          "timestamp": "2025-09-22 07:42:00",
          "user": "sipa",
          "content": "but it may indeed not gain us much, as in the steady state, that feerate will equal the feefilter rate",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 102,
            "has_nack": false,
            "has_ack": false
          }
        },
        {
          "type": "message",
          "timestamp": "2025-09-22 07:44:00",
          "user": "_aj_",
          "content": "sipa: if your mempool has feerates from 10sat/vb to 50sat/vb, and you've relayed everything, and your iterator is at 50sat/vb; then you get a huge mass of txs in the 20sat/vb to 120sat/vb range, your pointer will just get stuck at 20sat/vb and not do you any good?",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 264,
            "has_nack": false,
            "has_ack": false
          }
        },
        {
          "type": "message",
          "timestamp": "2025-09-22 07:45:00",
          "user": "_aj_",
          "content": "oh, below that point, so it's stuck at 10sat/vb forever",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 55,
            "has_nack": false,
            "has_ack": false
          }
        },
        {
          "type": "message",
          "timestamp": "2025-09-22 07:47:00",
          "user": "sipa",
          "content": "i guess the feerate i'm imagining being stored is \"the lowest chunk feerate of any mempool transaction which you've relayed to this peer\"",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 137,
            "has_nack": false,
            "has_ack": false
          }
        },
        {
          "type": "message",
          "timestamp": "2025-09-22 07:47:00",
          "user": "sipa",
          "content": "anything with a lower feerate you don't need to add to the set",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 62,
            "has_nack": false,
            "has_ack": false
          }
        },
        {
          "type": "message",
          "timestamp": "2025-09-22 07:48:00",
          "user": "sipa",
          "content": "but it sounds like a mess to keep track of, and i'm not sure it really helps with anything",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 1,
            "text_length": 90,
            "has_nack": false,
            "has_ack": true
          }
        },
        {
          "type": "message",
          "timestamp": "2025-09-22 07:50:00",
          "user": "_aj_",
          "content": "i just can't see how this can do anything useful? you'll have relayed txs both above and below any pointer (above because that's priority, below because they're old and there was less pressure), and you'll get new txs both above and below, so you have to track the ones below (because they're less likely to be sent) and may have to track the ones above (if the rate becomes too high)?",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 1,
            "text_length": 385,
            "has_nack": false,
            "has_ack": true
          }
        },
        {
          "type": "message",
          "timestamp": "2025-09-22 07:51:00",
          "user": "sipa",
          "content": "yeah",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 4,
            "has_nack": false,
            "has_ack": false
          }
        },
        {
          "type": "message",
          "timestamp": "2025-09-22 07:51:00",
          "user": "sipa",
          "content": "please disregard",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 16,
            "has_nack": false,
            "has_ack": false
          }
        },
        {
          "type": "message",
          "timestamp": "2025-09-22 07:51:00",
          "user": "_aj_",
          "content": "you could invert the logic at a point -- track the ones to be relayed above a point, and track the ones already relayed below that point; but that seems fiddly and probably not useful",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 1,
            "text_length": 183,
            "has_nack": false,
            "has_ack": true
          }
        },
        {
          "type": "message",
          "timestamp": "2025-09-22 08:18:00",
          "user": "instagibbs",
          "content": "https://github.com/lightning/bolts/pull/1292 another great entry in the annals of mini*",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 87,
            "has_nack": false,
            "has_ack": false
          }
        },
        {
          "type": "message",
          "timestamp": "2025-09-22 11:15:00",
          "user": "darosior",
          "content": "hah",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 3,
            "has_nack": false,
            "has_ack": false
          }
        },
        {
          "type": "message",
          "timestamp": "2025-09-22 17:37:00",
          "user": "_aj_",
          "content": "sipa: could have an iterator where \"unbroadcst txs with chunk feerates above this are been put in per-peer queues, txs below are put in a single global queue\" ? if the global queue grows too large, that iterator then immediately tells you a good value to use as feefilter, without having to gradually increase it from the bottom of the mempool",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 343,
            "has_nack": false,
            "has_ack": false
          }
        },
        {
          "type": "message",
          "timestamp": "2025-09-22 17:38:00",
          "user": "_aj_",
          "content": "not unbroadcst, un-relayed",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 26,
            "has_nack": false,
            "has_ack": false
          }
        },
        {
          "type": "message",
          "timestamp": "2025-09-22 21:13:00",
          "user": "_aj_",
          "content": "sipa: hmm, except you could do that without the iterator just by setting the feefilter to the highest fee in the global queue",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 125,
            "has_nack": false,
            "has_ack": false
          }
        }
      ],
      "threads": [
        {
          "start_time": "2025-09-22 02:55:00",
          "end_time": "2025-09-22 03:50:00",
          "message_count": 43,
          "participants": [
            "sipa",
            "_aj_"
          ],
          "participant_count": 2,
          "first_message": "_aj_: in what way does the formula i implemented differ from what you thought?",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 2,
            "nack_count": 0,
            "ack_count": 2
          }
        },
        {
          "start_time": "2025-09-22 04:18:00",
          "end_time": "2025-09-22 04:39:00",
          "message_count": 8,
          "participants": [
            "sipa",
            "_aj_"
          ],
          "participant_count": 2,
          "first_message": "sipa: maybe could have a set<txiter> of unrelayed txs managed by the mempool, sorted by quality; and use the quality lookup to remove tx from the set when they're removed from the mempool? would be re",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 2,
            "nack_count": 0,
            "ack_count": 2
          }
        },
        {
          "start_time": "2025-09-22 06:01:00",
          "end_time": "2025-09-22 06:05:00",
          "message_count": 4,
          "participants": [
            "sipa"
          ],
          "participant_count": 1,
          "first_message": "_aj_: i'm wondering if post-cluster-mempool, we could do something like keep per-peer an iterator into the chunk index (\"mostly relayed every chunk with feerates higher than this\") + a set of unrelaye",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "nack_count": 0,
            "ack_count": 0
          }
        },
        {
          "start_time": "2025-09-22 07:39:00",
          "end_time": "2025-09-22 07:51:00",
          "message_count": 13,
          "participants": [
            "sipa",
            "_aj_"
          ],
          "participant_count": 2,
          "first_message": "sipa: i don't see how that would work at all? we'll get new transactions both above and below the iterator, and need to keep track of anything not relayed in either case?",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 5,
            "nack_count": 0,
            "ack_count": 5
          }
        }
      ]
    }
  ],
  "summary": {
    "days_fetched": 1,
    "total_messages": 75,
    "total_threads": 4,
    "unique_participants": 4
  }
}