{
  "source": "github",
  "repository": "bitcoin/bitcoin",
  "fetched_at": "2026-01-15T20:14:18.309076+00:00",
  "date": "2025-09-14",
  "pull_requests": [],
  "issues": [
    {
      "number": 33387,
      "title": "Assertion hit on shutdown of bitcoin-node with connected mining interface client",
      "body": "### Is there an existing issue for this?\n\n- [x] I have searched the existing issues\n\n### Current behaviour\n\nRunning c0894a0a2be032cd9a5d5945643689230ab10255 `bitcoin-node` with Sjor's [sv2-tp] connected resulted in an unclean shutdown:\n```\n2025-09-14T10:24:34Z CreateNewBlock(): block weight: 460455 txs: 371 fees: 230840 sigops 1336 \n2025-09-14T10:25:05Z CreateNewBlock(): block weight: 513153 txs: 441 fees: 290027 sigops 1489 \n2025-09-14T10:25:36Z CreateNewBlock(): block weight: 635048 txs: 541 fees: 370980 sigops 1872 \n^C2025-09-14T10:25:53Z tor: Thread interrupt                                                 \n2025-09-14T10:25:53Z torcontrol thread exit\n2025-09-14T10:25:53Z addcon thread exit                                                                                                                                                    \n2025-09-14T10:25:53Z opencon thread exit\n2025-09-14T10:25:53Z Shutdown: In progress...                                                \n2025-09-14T10:25:53Z net thread exit \n2025-09-14T10:25:54Z msghand thread exit\n2025-09-14T10:25:54Z DumpAnchors: Flush 2 outbound block-relay-only peer addresses to anchors.dat started\n2025-09-14T10:25:54Z DumpAnchors: Flush 2 outbound block-relay-only peer addresses to anchors.dat completed (0.00s)\n2025-09-14T10:25:54Z scheduler thread exit\n2025-09-14T10:25:54Z Writing 591 mempool transactions to file...\n2025-09-14T10:25:54Z Writing 0 unbroadcast transactions to file.                   \n2025-09-14T10:25:54Z Dumped mempool: 0.005s to copy, 0.004s to dump, 348937 bytes dumped to file\n2025-09-14T10:25:54Z Flushed fee estimates to fee_estimates.dat.\n./node/interfaces.cpp:947 chainman: Assertion `m_node.chainman' failed.\nAborted (core dumped) \n```\n\n### Expected behaviour\n\nNo assertions triggered.\n\n### Steps to reproduce\n\nSeems to happen reliably.\n\n### Relevant log output\n\n_No response_\n\n### How did you obtain Bitcoin Core\n\nCompiled from source\n\n### What version of Bitcoin Core are you using?\n\nmaster@c0894a0a2be032cd9a5d5945643689230ab10255\n\n### Operating system and version\n\nUbuntu 24.04\n\n### Machine specifications\n\n_No response_",
      "state": "open",
      "user": "sedited",
      "created_at": "2025-09-14T10:36:17Z",
      "updated_at": "2025-09-15T12:28:52Z",
      "comments": 4,
      "url": "https://github.com/bitcoin/bitcoin/issues/33387",
      "labels": [
        "interfaces"
      ],
      "comment_list": [
        {
          "user": "sedited",
          "body": "Tried reproducing it again from scratch and could not. I think this was just a version mismatch. There is a crash on the sv2 side, but that is for a different issue tracker. Closing.",
          "created_at": "2025-09-14T11:29:54Z"
        },
        {
          "user": "ryanofsky",
          "body": "Nice find, I think this probably is a real bug. I wouldn't expect it to happen normally because there is an `Ipc::disconnectIncoming()` call in `Shutdown()` to close all IPC connections before the chainman object is freed. However, the `disconnectIncoming()` implementation is only closing the IPC connections without waiting for the objects and threads associated with them to be freed. So if an IPC client makes a request and the node receives it, and the node starts to shut down before the request is executed, asserts like this could be triggered.\n\nA change like the following could fix the issue:\n\n```diff\n--- a/src/ipc/capnp/protocol.cpp\n+++ b/src/ipc/capnp/protocol.cpp\n@@ -75,9 +75,23 @@ public:\n         // Delete incoming connections, except the connection to a parent\n         // process (if there is one), since a parent process should be able to\n         // monitor and control this process, even during shutdown.\n+        int num_disconnected = 0;\n+        std::promise<void> promise;\n         m_loop->sync([&] {\n-            m_loop->m_incoming_connections.remove_if([this](mp::Connection& c) { return &c != m_parent_connection; });\n+            m_loop->m_incoming_connections.remove_if([&](mp::Connection& c) {\n+                if (&c != m_parent_connection) {\n+                    ++num_disconnected;\n+                    return true;\n+                }\n+                return false;\n+            });\n+            if (num_disconnected) m_loop->addAsyncCleanup([&] { promise.set_value(); } );\n         });\n+        if (num_disconnected) {\n+            LogDebug(BCLog::IPC, \"Disconnected %i IPC connections, starting cleanup...\", num_disconnected);\n+            promise.get_future().wait();\n+            LogDebug(BCLog::IPC, \"IPC connection cleanup complete.\");\n+        }\n     }\n     void addCleanup(std::type_index type, void* iface, std::function<void()> cleanup) override\n     {\n\n```\n\n\n",
          "created_at": "2025-09-14T19:13:24Z"
        },
        {
          "user": "fanquake",
          "body": "@Sjors ",
          "created_at": "2025-09-15T09:19:59Z"
        },
        {
          "user": "ryanofsky",
          "body": "Will reopen since I think this is probably worth following up on, even if it may not be an urgent priority.\n\nIf I'm understanding the problem correctly, it might not be to hard to reproduce with a functional test calling IPC methods during shutdown, though the test would be inherently racy. It might be easier to reproduce reliably with a unit test.",
          "created_at": "2025-09-15T12:28:52Z"
        }
      ]
    }
  ],
  "summary": {
    "pull_requests": 0,
    "issues": 1
  }
}