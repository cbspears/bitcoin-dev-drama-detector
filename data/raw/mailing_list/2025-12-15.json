{
  "source": "mailing_list",
  "list": "bitcoin-dev",
  "fetched_at": "2026-01-16T00:37:30.011673+00:00",
  "date": "2025-12-15",
  "threads": [
    {
      "title": "Re: [bitcoindev] [Discussion] Year 2106 Timestamp Overflow - Proposal for uint64 Migration",
      "message_count": 1,
      "participants": [
        "Henry Romp"
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/CAPnXYtPdbFwc1mOsP2eHQBwuJhn2XxLCWC-pt6+N-bu3BRRbiA@mail.gmail.com",
          "title": "Re: [bitcoindev] [Discussion] Year 2106 Timestamp Overflow - Proposal for uint64 Migration",
          "author": "Henry Romp <151henry151@gmail@com>",
          "date": "Mon, 15 Dec 2025 14:09:50 -0500",
          "body": "[-- Attachment #1: Type: text/plain, Size: 3513 bytes --]\n\nAh, I see you're right—once MTP reaches 2^32 - 1, no valid timestamp can\nexceed it, making the next block mathematically impossible.\nI was wrong about the halt. I still maintain my other points about timeline\nand opportunity costs.\n\n\nHenry\n\n\n\n*Henry Romp802-458-7299 <8024587299>*\n*151henry151@gmail•com <151henry151@gmail•com>*\n\n\n\nOn Mon, Dec 15, 2025, 04:59 Garlo Nicon <garlonicon@gmail•com> wrote:\n\n> > The blockchain won't \"halt\" at overflow, it will have validation\n> problems.\n>\n> These \"validation problems\" will be quite serious. For example: it will be\n> possible to produce a chain with a bigger chainwork, and pass it to the old\n> nodes.\n>\n> Which means, that the chain can go forward for the new nodes, while being\n> perceived as a constantly reorged, by the old implementation.\n>\n> And then, the question is: do we want to design a new soft-fork in a way,\n> where it would be seen as constantly-reorged chain by the old nodes?\n>\n> > The overflow doesn't automatically stop the chain.\n>\n> It will, because overflowed timestamps from 1970 will be rejected by all\n> old nodes.\n>\n> > At that point there are no more valid blocks that can be appended to the\n> chain.\n>\n> As long as the chainwork won't overflow, you can always reorg the old\n> blocks. If that reorg will be deterministic, and accepted by hashrate\n> majority, then it will be seen only by old nodes. New nodes can see a\n> stable chain, always going forward, beyond 0xffffffff.\n>\n> Anyway, it will be just one-bit increment per 136 years.\n>\n> niedz., 14 gru 2025 o 15:09 'Russell O'Connor' via Bitcoin Development\n> Mailing List <bitcoindev@googlegroups.com> napisał(a):\n>\n>> On Sat, Dec 13, 2025 at 5:05 AM Henry Romp <151henry151@gmail•com> wrote:\n>>\n>>> The blockchain won't \"halt\" at overflow, it will have validation\n>>> problems. The overflow doesn't automatically stop the chain. Nodes would\n>>> continue with wrapped-around timestamps (though this would cause ",
          "drama_signals": {
            "drama_keywords": 2,
            "positive_keywords": 0,
            "text_length": 2091,
            "has_nack": false,
            "has_ack": false
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 2
      }
    },
    {
      "title": "Re: [bitcoindev] Does GCC preclude a soft fork to handle timestamp overflow?",
      "message_count": 1,
      "participants": [
        "Josh Doman"
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/e7a70843-a304-4d04-9365-08b8b4259caen@googlegroups.com",
          "title": "Re: [bitcoindev] Does GCC preclude a soft fork to handle timestamp overflow?",
          "author": "Josh Doman <joshsdoman@gmail@com>",
          "date": "Mon, 15 Dec 2025 09:27:58 -0800 (PST)",
          "body": "[-- Attachment #1.1: Type: text/plain, Size: 5453 bytes --]\n\n> your idea is to have the header nTime used for difficulty adjustment \nenforced in the coinbase tx.\n\nCorrect. As written, BIP54 makes that soft fork impossible, leaving a hard \nfork as the only option to resolve nTime overflow.\n\n> I was about to write this email myself, but then I realized that since \nBIP 113, timelocks are based on MTP time, and any soft-fork mechanism that \nmesses with MTP time will destroy existing transaction's timelock semantics.\n\nYes, it's unfortunate. There is certainly a tradeoff. On the one hand, \nthere is a risk of coin confiscation, if the soft fork isn't signaled early \nenough (a few decades in advance is probably sufficient). On the other \nhand, there are material benefits to avoiding a hard fork (i.e. you get a \nsmooth and secure upgrade path, developers can write immutable programs \nthat verify the chain, etc).\n\nI think it's presumptive to assume which option a future generation would \nprefer, in the year 2070, 2080, 2090, 2100, etc, given the tradeoffs \ninvolved. I'm not suggesting we decide today, but I am suggesting that \nBIP54 may be unnecessarily restrictive.\n\nThe following modification to BIP54 would resolve the timewarp attack while \nleaving open the possibility of an nTime soft fork:\n1) Add a u64 timestamp to the coinbase and enforce BIP54 there (in addition \nto other timestamp rules)\n2) Given a block of height N, where N % 2016 = 2015, the difference between \nthe nTime and the nTime at height (N - 2015) must be the same as in the \ncoinbase.\n\nOn Monday, December 15, 2025 at 11:36:31 AM UTC-5 Russell O'Connor wrote:\n\nI was about to write this email myself, but then I realized that since BIP \n113, timelocks are based on MTP time, and any soft-fork mechanism that \nmesses with MTP time will destroy existing transaction's timelock \nsemantics.  Now I think the best is to have a hardfork.\n\nOn Sun, Dec 14, 2025 at 3:33 PM Josh Doman <joshs...@gmail•com> wrote:\n\n*TLDR:* The ",
          "drama_signals": {
            "drama_keywords": 3,
            "positive_keywords": 1,
            "text_length": 2077,
            "has_nack": false,
            "has_ack": true
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 3
      }
    },
    {
      "title": "Re: [bitcoindev] The Cat, BIP draft discussion.",
      "message_count": 1,
      "participants": [
        "Greg Maxwell"
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/CAAS2fgQLKD1-DomvniTHLTvw52yeaKmOG-K__TnZeYJ+c2hYFA@mail.gmail.com",
          "title": "Re: [bitcoindev] The Cat, BIP draft discussion.",
          "author": "Greg Maxwell <gmaxwell@gmail@com>",
          "date": "Mon, 15 Dec 2025 16:04:27 +0000",
          "body": "[-- Attachment #1: Type: text/plain, Size: 8747 bytes --]\n\nOn Mon, Dec 15, 2025 at 10:35 AM Nona YoBidnes <pepehodler@gmail•com> wrote:\n\n> Here, your argument runs completely opposite of the claim that \"the miner\n> fees are the filter\". You appear to be claiming that dping nothing about\n> spam, making it easier for spammers and being more accommodating to them is\n> the way to go. Unfortunately, that's the approach we have taken for the\n> last 3 years while spam only gets worst.\n>\n\nOn what basis do you claim that spam has 'only gotten worse'--  the\nexisting setup is incredibly effective against spam, essentially blocking\nall forms of pointless data storage that aren't made more valuable by the\nlimitations.  What does go in, goes in at extremely high costs to the\nspammer.\n\nOf what concern is this residual traffic to Bitcoin use?  It doesn't\nincrease node resource use as that's governed by the capacity limits, in\nfact because it's generally much easier to process it speeds up block\nprocessing.  It's substantially a non-issue.\n\nThe proposed gain is some negligible one time reduction in utxo disk space.\n>\n>\n> Between 40 and 50% of the UTXO set is comprised of spam UTXOs with dust\n> amounts. Even more conservative estimates put it at 30%. The Cat would\n> remove those spam NMUs from the UTXO set. I hardly view that as negligible.\n>\n\nIt is negligible-- it's just a one time constant fraction.  It will not\nincrease the set of devices that can run a node in any meaningful sense.\nWhat improvement it provides could also be alternatively achieved through\nlocal only technical changes like changing how the data is stored.\n\nFurthermore, The Cat would send a strong signal to spammers: you are not\n> welcomed on Bitcoin, we are rugging you, and we might do it again. This\n> likely would reduce future spam activity on Bitcoin, further protecting the\n> UTXO set.\n>\n\nAs I've pointed out, it won't stop their NFTs.  They'll simply make a new\nrule in their NFT indexers that says that deleted N",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 2048,
            "has_nack": false,
            "has_ack": false
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 0
      }
    },
    {
      "title": "Re: [bitcoindev] Re: The Cat, BIP draft discussion.",
      "message_count": 1,
      "participants": [
        "Ataraxia 009"
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/CAADX_vKftpgPfr7FgQGsQQ+6EtEdsZu-fUOWVq-ZQPoLoP8zkQ@mail.gmail.com",
          "title": "Re: [bitcoindev] Re: The Cat, BIP draft discussion.",
          "author": "Ataraxia 009 <ataraxia.009.work@gmail@com>",
          "date": "Sat, 13 Dec 2025 12:37:31 +0530",
          "body": "[-- Attachment #1: Type: text/plain, Size: 2001 bytes --]\n\nYes there is no need to add consensus changes to filter out unspendable\noutputs from the UTXO set, thats just a protocol upgrade.\n\nThe second point with the Ordinals:\nWhether it's a transaction validation rule or not, what you're planning to\ndo is differentiate between utxos on a consensus level. Which is the worst\npath that you could go down. This will break bitcoin more than any spam\never will.\n\nOn Sat, Dec 13, 2025 at 5:19 AM Greg Maxwell <gmaxwell@gmail•com> wrote:\n\n> On Fri, Dec 12, 2025 at 9:26 PM Jonathan Voss <k98kurz@gmail•com> wrote:\n>\n>> Since the Bitcoin Stamps outputs are already unspendable, it makes\n>> perfect sense to mark and drop them from the UTXO set.\n>\n>\n> There is no consensus change involved in not storing a provably\n> unspendable output, it's just an implementation detail with no\n> interoperability implications and doesn't need a BIP.  Bitcoin core has\n> long done so for several types of unspendable outputs, e.g. outputs over\n> 10kb and ones starting with OP_RETURN.\n>\n> --\n> You received this message because you are subscribed to the Google Groups\n> \"Bitcoin Development Mailing List\" group.\n> To unsubscribe from this group and stop receiving emails from it, send an\n> email to bitcoindev+unsubscribe@googlegroups•com.\n> To view this discussion visit\n> https://groups.google.com/d/msgid/bitcoindev/CAAS2fgTjF_e5vLEzp672_jmF82jDUre0wcZKHB5my8kTdFOGgw%40mail.gmail.com\n> <https://groups.google.com/d/msgid/bitcoindev/CAAS2fgTjF_e5vLEzp672_jmF82jDUre0wcZKHB5my8kTdFOGgw%40mail.gmail.com?utm_medium=email&utm_source=footer>\n> .\n>\n\n-- \nYou received this message because you are subscribed to the Google Groups \"Bitcoin Development Mailing List\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to bitcoindev+unsubscribe@googlegroups•com.\nTo view this discussion visit https://groups.google.com/d/msgid/bitcoindev/CAADX_vKftpgPfr7FgQGsQQ%2B6EtEdsZu-fUOWVq-ZQPoLoP8zkQ%",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 2052,
            "has_nack": false,
            "has_ack": false
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 0
      }
    },
    {
      "title": "[bitcoindev] [BIP Proposal] Add PSBT_IN_SP_TWEAK field",
      "message_count": 1,
      "participants": [
        "\"'nymius' via Bitcoin Development Mailing List\""
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/R53cG3TeXgXDUUS4kH_q226GlaFCjI0DZVT6mdTQzSQdj3RnNqWA-bFT7uGgGQFJG6938kDGvDJVoFQj8ItEMsJ6NyOjCTvpVEarYiyW6-8=@proton.me",
          "title": "[bitcoindev] [BIP Proposal] Add PSBT_IN_SP_TWEAK field",
          "author": "\"'nymius' via Bitcoin Development Mailing List\" <bitcoindev@googlegroups.com>",
          "date": "Mon, 15 Dec 2025 08:00:01 +0000",
          "body": "[-- Attachment #1.1.1: Type: text/plain, Size: 5112 bytes --]\n\nHi all,\nI'm working on the implementation of silent payments in BDK.\n\nBDK's transaction creation process is structured around PSBTs. Because of this, a stable implementation of silent payments in the project depends of the specifications of BIP 352 for this format. BIP 375 and BIP 374 are core component for this.\n\nHowever, there is a need for the inclusion of silent payments tweaks in PSBTs to spend silent payment outputs, which was considered before [1][2] but never specified.\n\nI would like to propose the following as a base for a new BIP proposal addressing this gap:\n\n### Abstract\n\nThis document proposes additional fields for BIP 370 PSBTv2 that allow for BIP 352 silent payment tweaks to be included in a PSBT of version 2. These will be fields for scripts that are relevant to the spending of silent payment outputs, but may be also useful to other protocols using taproot tweaks not following BIP 340 spec.\n\n### Motivation\n\nBIPs 352 specify silent payments protocol, which provides a new way to create P2TR outputs and spend them. The existing PSBT fields are unable to support silent payments without changes, due to the new method by which outputs are created. BIP 375 and complementary BIP 374 specify how to create outputs locked with silent payment keys using PSBTs. But they don't specify how to unlock these outputs in a transaction. Therefore new fields must be defined to allow PSBTs to carry the information necessary for tweaking taproot keys without following the BIP 340 tagging scheme.\n\n### Specification\n\nThe new per-input types are defined as follows:\n\n| Name              | \\<keytype\\>               | \\<keydata\\> | <keydata\\><br>Description | \\<valuedata\\>    | \\<valuedata\\><br>Description                                                                                 | \\<Versions Requiring Inlusion\\> | \\<Versions Requiring Exclusion\\> | \\<Versions Allowing Inclusion\\> |\n| ----------------- | --------",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 2055,
            "has_nack": false,
            "has_ack": false
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 0
      }
    },
    {
      "title": "[bitcoindev] Splitting more block, addr and tx classes of network traffic",
      "message_count": 1,
      "participants": [
        "Antoine Riard"
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/CALZpt+Hx9vFwNQd6qGSFMWXU=A6j82m6ZjJg3JaHK26WW0UQZw@mail.gmail.com",
          "title": "[bitcoindev] Splitting more block, addr and tx classes of network traffic",
          "author": "Antoine Riard <antoine.riard@gmail@com>",
          "date": "Thu, 4 Dec 2025 22:33:43 +0000",
          "body": "[-- Attachment #1: Type: text/plain, Size: 3714 bytes --]\n\nHi list,\n\nSurfacing an old idea concerning the network-level and the current meddling\nof block,\ntx and addr messages traffic generally all over one network link.\nHistorically, for\nexample, if you consider bitcoin core by default connections are going to\nbe FULL_RELAY.\nOver the last years, there has been few improvements to separate network\nlinks by types\ne.g with the introduction of dedicated outbound BLOCK-RELAY connections\n[1], without the\nsegregation at the network-level between the class of traffic really being\npursued, or at\nleast more flexibility in network mechanisms to signal to a node's peers\nwhat categories\nof messages will be processed on a given link.\n\nPreviously it has been shown that leveraging tx-relay's orphan mechanism\ncan allow to map\na peer's network-topology [2] (sadly, one trick among others). Being able\nto infer a peer's\n\"likely\" network topology from tx traffic, one can guess the peers used to\ncarry block-relay\ntraffic. From the PoV of an economical node, dissimulating the block-relay\ntraffic is a very\nvaluable to minimize the risks of escalation attacks based on\nnetwork-topology (e.g for\nlightning nodes [3]).\n\nSegregating more network traffic by class of messages sounds to suppose 1)\nbeing able to signal\namong the {ADDR, ADDRV2} service bits if block, addr or tx relay is\nsupported on a link to be\nopened for a pair of a (net_addr, port) or alternatively 2) if network link\nare open blindly\nwith peers, being to signal in the VERSION message or with a dedicated\nmessage what class of\nmessage is supported. There is already a signaling mechanism in the VERSION\nmessage to\ndisable tx-relay (i.e `fRelay`), however there is no signaling to disable\nblock-relay over a link.\nAlternatively, it has been proposed in the past to add a new early message\namong all the other\nhandshake messages between the VERSION / VERACK flow, but it has never been\nimplemented [4].\n\nFor bitcoin backbone, started to nativ",
          "drama_signals": {
            "drama_keywords": 1,
            "positive_keywords": 1,
            "text_length": 2074,
            "has_nack": false,
            "has_ack": true
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 1
      }
    },
    {
      "title": "[bitcoindev] Re: Splitting more block, addr and tx classes of network traffic",
      "message_count": 1,
      "participants": [
        "Antoine Riard"
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/7cceae55-0885-4a66-9e1f-55e1537e2e17n@googlegroups.com",
          "title": "[bitcoindev] Re: Splitting more block, addr and tx classes of network traffic",
          "author": "Antoine Riard <antoine.riard@gmail@com>",
          "date": "Sun, 14 Dec 2025 18:10:14 -0800 (PST)",
          "body": "[-- Attachment #1.1: Type: text/plain, Size: 8730 bytes --]\n\n \n\nHi Defenwycke,\n\nI'm already working on a native multi-process architecture where the traffic\nclasses are isolated on different runtimes, and the \"old\" block store is \nshared.\nAll the points, you made about explicit signaling and the drawbacks are \nvalid,\nand one of the latest time the idea to add a signaling bit for full-rbf \npeers\ncame up, privacy concerns were raised.\n\nThe drawback for the multi-process, multi-socket design approach is to \nmultiply\nthe number of inbound sockets consumed by a peer, though in the case of a \n\"cold\nblock\" archive process it's the inbound peer initiating the connection.\n\nBandwidth-consumption wise, getting messages like BIP 0338 this is still an\noutbound bandwidth win for your full-node peers adopting it, and more \ngenerally\nfor any ingress filtering at the network-level.\n\nBest,\nAntoine\nOTS hash: e1b51b6a80bc77a1cd9e65b1fb74e9b5f52b93473d9e1f1390015eae70674b4c\nLe mercredi 10 décembre 2025 à 18:12:32 UTC, defenwycke a écrit :\n\n> Hello Antoine,\n>\n> This is an interesting problem, and introducing finer-grained traffic \n> classes certainly makes sense. The three areas that stand out to me are \n> peer declaration, topology inference and system bottlenecks.\n>\n> Peer declaration: \n>\n> Explicit signalling of specialised roles (Example - I only relay \n> hot-blocks) to peers increases the fingerprint/profile. We already see \n> topology inference attacks via relay behaviour; adding public role \n> declarations may expand that surface. Nodes can already drop or \n> deprioritise whatever they wish locally, so explicit signalling may not be \n> necessary.\n>\n> Topology inference:\n>\n> Since topology inference can be drawn from tx-relay timing and relay \n> behaviour, an internal class-based model also allows the node to randomise \n> acceptance, forwarding, and scheduling behaviour per class. Even small \n> amounts of deliberate jitter or probabilistic message handling make it far \n> harder for",
          "drama_signals": {
            "drama_keywords": 1,
            "positive_keywords": 2,
            "text_length": 2078,
            "has_nack": false,
            "has_ack": true
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 1
      }
    },
    {
      "title": "Re: [bitcoindev] [BIP Proposal] Add sp() output descriptor format for BIP352",
      "message_count": 1,
      "participants": [
        "Oghenovo Usiwoma"
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/CAOCjZ9QxuFK2-6pEKC9tgL=bThtF2RqxJV_T3MpKz-k084hHAA@mail.gmail.com",
          "title": "Re: [bitcoindev] [BIP Proposal] Add sp() output descriptor format for BIP352",
          "author": "Oghenovo Usiwoma <eunovo9@gmail@com>",
          "date": "Sun, 14 Dec 2025 21:30:17 +0100",
          "body": "[-- Attachment #1: Type: text/plain, Size: 14986 bytes --]\n\nHi Craig,\n\n> > IIUC, this creates a descriptor with a variable length. What if we\nencoded multiple labels in one number? For example, labels 1, 5, 10 are\nencoded into a 64-bit number by setting the corresponding bit positions to\n'1' so that the final number is '1058'. Using one number to encode the\nlabels is very appealing to me.\n\n> Descriptors are generally of variable length, so I'm not sure why this is\nso appealing. Not only does this limit the range from 1 to 63, it has the\nadded disadvantage of making this part of the descriptor unreadable to most\nhumans.\n\nI was thinking about a descriptor with thousands of labels and I thought I\ncould encode a large list of numbers into 64 bit number, but that’s\nimpossible.\n\nConsider this idea, if we strictly increment the label by one, then just\nwriting the max label, '10', in this example should be sufficient to tell\nthe wallet to check for labels 1.. 10. The previous example used \"1, 5, 10\"\nas labels, but I'm not sure why we would we use label '1' and skip to '5',\nso writing just the max label should work?\n\n\nI'm still not in support of using new key formats for the descriptor, but\nI'll see if they receive popular support.\n\nNovo.\n\nOn Fri, 12 Dec 2025, 10:18 am Craig Raw, <craigraw@gmail•com> wrote:\n\n> Hi Novo,\n>\n> Responses inline:\n>\n> > However, without the birthday, the descriptor will still be able to\n> describe its outputs. The birthday can be collected through some other\n> means, as we do with other descriptors today.\n>\n> Indeed, that is why it is an optional argument. Again however, other\n> descriptors do not have to bear the very significant computational overhead\n> that sp() descriptors do. For many deployment contexts, this will\n> effectively make the birthday a requirement to retrieve all silent payment\n> outputs in a wallet within a usable time frame. Other descriptors do not\n> share such a stark usability challenge.\n>\n> > With existing key formats, users",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 2077,
            "has_nack": false,
            "has_ack": false
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 0
      }
    },
    {
      "title": "Re: [bitcoindev] Feedback on a simple 2-path vault design (2-of-2 + CLTV recovery) and use of pruned nodes for UTXO retrieval",
      "message_count": 1,
      "participants": [
        "victor perez"
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/CAHyc38=a=7fCgPg10e=jkpiUA8qiJf+DAaoOaNcP1JajjsZTtw@mail.gmail.com",
          "title": "Re: [bitcoindev] Feedback on a simple 2-path vault design (2-of-2 + CLTV recovery) and use of pruned nodes for UTXO retrieval",
          "author": "victor perez <svcrobotics@gmail@com>",
          "date": "Sun, 14 Dec 2025 11:40:27 +0100",
          "body": "[-- Attachment #1: Type: text/plain, Size: 12246 bytes --]\n\nHi Antoine,\n\nThank you very much for your detailed reply—it's extremely helpful.\n\nFollowing your advice, I'm going to simplify my work and first focus on a\nsingle spending path: a clean 2-of-2 (A + B) multisig, signed with two\nLedgers. My goal is to get this flow stable end-to-end (descriptor\ncreation, UTXO handling, PSBT flow, hardware signing, broadcast) before\nadding the recovery path with a timelock.\n\nJust to clarify the scope: my application is not meant to be a second\nLiana. It's an educational and experimental Ruby on Rails environment that\nI use to better understand Bitcoin in practice.\n\nThe app combines several independent modules:\n\n  - A vault module, where I experiment with descriptors, Miniscript/Taproot\nconstructions, PSBTs, and hardware signing.\n  - A BRC-20 and on-chain analytics module, which helps me explore data\nintelligence by extracting and analysing blockchain data.\n  - A donation module in sats connected to my BTCPay Server.\n  - Various dashboards for visualizing Bitcoin data.\n\nI should mention that I am currently not using regtest. All my experiments\nare done directly on mainnet with real BTC, because it helps me stay fully\naware of real-world constraints and forces me to design things carefully.\nThat said, based on your recommendations, I will start integrating regtest\ninto my workflow so I can iterate faster and test edge-cases more safely.\n\nYour pointers toward Bitcoin Core's wallet API, BDK, Miniscript, and\nhardware-wallet policies give me a very clear roadmap for progressing in a\nstructured way.\n\nThanks again for your time and guidance—it truly helps.\n\nBest regards,\nVictor\n\n\nLe sam. 13 déc. 2025, 18:03, Antoine Poinsot <darosior@protonmail•com> a\nécrit :\n\n> Hi Victor,\n>\n> > If you have any recommendations on what pitfalls to avoid or reading\n> material on robust recovery designs, I’d be glad to hear them.\n>\n> Since you mentioned that your purpose was educational, i would recommen",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 1,
            "text_length": 2126,
            "has_nack": false,
            "has_ack": true
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 0
      }
    },
    {
      "title": "Re: [bitcoindev] Draft BIP: DustSweep – policy-only UTXO dust compaction",
      "message_count": 1,
      "participants": [
        "Murch"
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/02d7368e-95d3-4185-b70f-3aa9b5df1e1d@murch.one",
          "title": "Re: [bitcoindev] Draft BIP: DustSweep – policy-only UTXO dust compaction",
          "author": "Murch <murch@murch@one>",
          "date": "Fri, 12 Dec 2025 14:49:42 -0800",
          "body": "[-- Attachment #1.1.1: Type: text/plain, Size: 5847 bytes --]\n\nHey Defenwycke,\n\n > all inputs are “dust-class” UTXOs\n\nWhat does “dust-class” mean? Are you using the Bitcoin Core dust limit \nor talking about small amounts in general? I don’t have figures off the \ntop of my head, but I would assume that there are relatively few UTXOs \nsmaller than Bitcoin Core’s dust limit.\n\n > only standard scripts (P2PKH / P2WPKH / P2TR)\n\nYou might want to clarify that you mean only P2TR KP inputs. Or would \nP2TR SP be permitted?\n\n > Nodes place these in a small, separate sub-mempool. They’re only\n > accepted when the normal mempool is <50% full, and they’re\n > automatically evicted if normal mempool usage hits 95%.\n\nIt would be a lot of work to have a separate pool for this, and I don’t \nsee a reason why they couldn’t just go in the regular mempool. If the \nmempool fills up, they’d have the lowest feerates and they’d get kicked \nout first anyway. That said, at 50% full, there are still around ~30 \nblocks worth of transactions waiting in the mempool that pay fees, …\n\n > Miners can include them up to a small weight fraction (I suggest ~5%) \nbut only after filling the block with regular fee-paying transactions.\n\n… so if they are only considered in blocks that aren’t full, the only \nones I have seen lately are miners using a minimum feerate of 1 s/vB for \ntheir block templates. Looking at some popular mempool statistic sites, \nin the past 32 months, there would have only been organically non-full \nblocks between April and August this year.\n\nI assume the intention is to only relay these transactions when there \nare blocks that aren’t full, to limit the bandwidth-wasting vector this \nfeature introduces, but overall it seems to me that it would be most \nlikely for such transactions to sit in nodes’ memory until they expire.\n\nAll that said, at the new minimum feerate of 0.1 s/vB, a 148 vB P2PKH \ninput costs 15 sats, a 68 vB P2WPKH input costs 7 sats, and a 57.5 vB \nP2TR input costs 6 sats.",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 2073,
            "has_nack": false,
            "has_ack": false
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 0
      }
    },
    {
      "title": "Re: [bitcoindev] Re: Draft BIP: DustSweep – policy-only UTXO dust compaction",
      "message_count": 1,
      "participants": [
        "Defenwycke"
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/CAOj3_X8GtJtAgcvvPZW2Ovxt31CzGtn9tssqTSZ4BeQ_bL6pxg@mail.gmail.com",
          "title": "Re: [bitcoindev] Re: Draft BIP: DustSweep – policy-only UTXO dust compaction",
          "author": "Defenwycke <cal.defenwycke@gmail@com>",
          "date": "Fri, 12 Dec 2025 20:17:28 +0000",
          "body": "[-- Attachment #1: Type: text/plain, Size: 5466 bytes --]\n\nHello Jonathan,\n\nThanks for the thoughtful feedback — that makes sense.\n\nI started with a very narrow definition mostly to make the invariant\nobvious and easy to reason about. Every DustSweep tx should monotonically\nreduce the UTXO set and never meaningfully compete with the fee market. As\nlong as that holds, I’m not particularly attached to any one parameter.\n\nI agree that requiring 100% dust inputs and exactly one output is probably\noverly strict in practice. A majority dust requirement and an output/input\nratio cap seem like reasonable ways to preserve the incentive (net UTXO\nreduction) while making it more usable for real wallets.\n\nMy main goal here is to give operators something that’s safe to run and\npredictable in behaviour — cheap, bounded, and only active when blockspace\nwould otherwise go unused. I’m happy to adjust thresholds or relax\nconstraints as long as those properties remain intact.\n\nAppreciate you taking the time to look at it.\n\nKind regards,\n\nDefenwycke\n\nOn Fri, Dec 12, 2025 at 6:16 PM Jonathan Voss <k98kurz@gmail•com> wrote:\n\n> Interesting proposal. Something like that would be helpful, but perhaps it\n> would be more useful if it was not quite so narrowly defined. For example,\n> instead of requiring all inputs be dust-class UTXO, it could require a\n> minimum of 80% dust-class inputs; instead of exactly one output, it could\n> be max_outputs = floor(n_inputs / 5) to keep a maximum output/input ratio\n> of 1/5. This could allow for better aggregation of dust outputs into\n> economically meaningful, monetary outputs than the narrower definition\n> while maintaining the incentives for meaningfully reducing UTXO set size.\n>\n> I would run this policy on my node. Hashers should ultimately be okay with\n> this policy since someone among them also has to run full nodes, and it\n> would provide an additional (albeit very small) fee source when block space\n> demand is low.\n>\n> On Thursday, December 11, 20",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 3,
            "text_length": 2077,
            "has_nack": false,
            "has_ack": true
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 0
      }
    },
    {
      "title": "[bitcoindev] Reducing RAM requirements with dynamic dust",
      "message_count": 1,
      "participants": [
        "\"'uuowwpevskfcordh' via Bitcoin Development Mailing List\""
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/Q4RLVZW6OK88aVcalvUK7KJJIKOXckKhB7a5zTN7LTxA-jzal3587k4yUiIMjcIBoqLI0eK4uQLZtjJGsbj1R8zsfaMDM-RGSw2V9KI6AAw=@proton.me",
          "title": "[bitcoindev] Reducing RAM requirements with dynamic dust",
          "author": "\"'uuowwpevskfcordh' via Bitcoin Development Mailing List\" <bitcoindev@googlegroups.com>",
          "date": "Sat, 06 Dec 2025 16:08:45 +0000",
          "body": "[-- Attachment #1: Type: text/plain, Size: 10005 bytes --]\n\nGiven the increasing RAM requirements, due to the increasing UTXO set, I suggest seeing the UTXO set size as a controlled variable. A feedback mechanism sets a dynamic dust level, below from which UTXOs are removed/discarded and thus freeing RAM.\n\nBelow is an overview essay better expressed by grok, which can also be seen in here:\nhttps://hackmd.io/P-2lzGb8TiC86IOE3OGiYA?view\n\n# Enhancing Bitcoin's Scalability: A PID-Inspired Approach to Managing UTXO Set Growth\n\n## Abstract\n\nBitcoin’s UTXO set is currently an unbounded accumulator that risks long-term centralization as node RAM requirements grow without limit. Existing fee incentives have proven insufficient against sustained low-value output creation (e.g., inscriptions, tokenized assets, dust-heavy protocols). This article proposes a soft-fork mechanism that treats UTXO set size as a controlled variable: a slowly rising target size is defined, and a PID-style feedback controller, updated every difficulty epoch, dynamically raises a minimum-value floor beneath which old UTXOs become unspendable. The result is bounded, predictable growth of the UTXO set with ample warning periods, no hard caps on monetary use, and strong resistance to bloat attacks—all while remaining fully compatible with a soft-fork deployment.\n\n## Introduction\n\nBitcoin, the pioneering decentralized digital currency, operates as a complex dynamic system governed by consensus rules that ensure security, immutability, and permissionless participation. At its core, Bitcoin maintains a distributed ledger known as the blockchain, which records all transactions in a sequence of blocks. Each transaction involves inputs and outputs: inputs reference previously unspent outputs from prior transactions, while outputs create new spendable units called Unspent Transaction Outputs (UTXOs). The UTXO set represents the aggregate state of all currently spendable coins in the network, serving as a critica",
          "drama_signals": {
            "drama_keywords": 1,
            "positive_keywords": 1,
            "text_length": 2057,
            "has_nack": false,
            "has_ack": true
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 1
      }
    },
    {
      "title": "Re: [bitcoindev] Re: Reducing RAM requirements with dynamic dust",
      "message_count": 1,
      "participants": [
        "\"'uuowwpevskfcordh' via Bitcoin Development Mailing List\""
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/ASHweqfTSVKBPOY2P4UCyK7k7C28OWcQx2SfP9Ovqv2xifZo6SJYpwYLQTjvXTuf7lCa6fqeP92n47L6EokhlA7gHguLLgzHBQMY87rQDQk=@proton.me",
          "title": "Re: [bitcoindev] Re: Reducing RAM requirements with dynamic dust",
          "author": "\"'uuowwpevskfcordh' via Bitcoin Development Mailing List\" <bitcoindev@googlegroups.com>",
          "date": "Fri, 12 Dec 2025 22:22:51 +0000",
          "body": "Thanks both Eric and Erik for the replies. I can't answer whether RAM requirements due to UTXO would indeed be a problem or not. I think a worse-case performance could be considered assuming a \"no-RAM\" environment.\n\nThis being said, I wanted to share some insights still related to the dynamic dust mechanism (the essay).\n\n1. This proposal could be named \"dynamic dust\" plus \"dust sweeping\". Dynamic dust refers to the dust level being defined by the controller at every epoch, and dust sweeping refers to deprecation of utxos below that dust threshold.\n\n2. The grace period for the sweep, individual for each transaction, allows for a new perspective. This is, again, better portrayed by Grok:\n\n## Annex: Addressing Concerns of Confiscation in UTXO Deprecation\n\nWhile the proposed PID-inspired mechanism for UTXO set management offers a pathway to enhanced scalability, it has elicited valid criticisms, particularly regarding its confiscatory nature. Deprecating low-value UTXOs could be perceived as an involuntary seizure of assets, undermining Bitcoin's principles of ownership and immutability. This concern merits careful consideration, as any protocol change must preserve user trust and avoid arbitrary interventions.\n\nThe proposal maintains a degree of neutrality by evaluating UTXOs solely based on their satoshi (SAT) value, without interpreting their content or purpose. This objective criterion minimizes subjective judgments, applying uniformly across all outputs regardless of their origin or use case.\n\nA key mitigating factor lies in the incorporation of a grace period—typically spanning 6 to 12 months—during which owners of affected UTXOs can consolidate or spend them without direct penalty. This window transforms the deprecation process from outright confiscation into a dynamic incentive structure, favoring resource allocation in a constrained system. In Bitcoin's ecosystem, users already incur ongoing costs for participation, such as transaction fees, which can render lo",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 2065,
            "has_nack": false,
            "has_ack": false
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 0
      }
    },
    {
      "title": "Re: [bitcoindev] Hash-Based Signatures for Bitcoin's Post-Quantum Future",
      "message_count": 1,
      "participants": [
        "Olaoluwa Osuntokun"
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/CAO3Pvs_cetTGP54zDzJJGRPeN7gXrRYGZZYk4mRYnhJQnwotdA@mail.gmail.com",
          "title": "Re: [bitcoindev] Hash-Based Signatures for Bitcoin's Post-Quantum Future",
          "author": "Olaoluwa Osuntokun <laolu32@gmail@com>",
          "date": "Tue, 9 Dec 2025 16:41:48 -0800",
          "body": "[-- Attachment #1: Type: text/plain, Size: 13469 bytes --]\n\nHi y'all,\n\nconduition wrote:\n> I'm personally hoping that we'll find a way to derive child pubkeys using\n> lattices (ML-DSA) and/or isogenies (SQIsign), but I haven't heard of any\n> solid proposals yet.\n\nThis paper [1] proposes a variant of Dilithium (dubbed DilithiumRK, RK for\n'randomized keys' presumably) that enables BIP-32-like functionality. It\nachieves this by getting rid of a public key compression step in the OG\nalgorithm that results in a loss of homomorphic properties. There're\nalgorithmic changes required (eg: a new public network param is needed\nwhich is used for seed/key generation), so it isn't vanilla FIP 204.\n\nAside from the deviation from the standard, the scheme introduces some\nadditional trade offs:\n\n  * Signatures arger as signatures carry a new error hint\n\n  * Signing is 2.7x slower\n\n  * Verification is 1.75x slower\n\nThere's also a published BIP-32-like like scheme for Falcon signatures [2].\nI'm\nless familiar with the details here, but the signature size blows up to\n~24KB compared to ~666 bytes for normal Falcon signatures.\n\n-- Laolu\n\n[1]: https://cic.iacr.org/p/2/3/3\n\n[2]: https://link.springer.com/article/10.1186/s42400-024-00216-w\n\n\nOn Mon, Dec 8, 2025 at 10:49 PM 'conduition' via Bitcoin Development\nMailing List <bitcoindev@googlegroups.com> wrote:\n\n> Great work Jonas and Mikhail, glad to see more eyes and ears surveying\n> these schemes and their potential. Also shameless plug for some of my\n> prior work <https://conduition.io/code/fast-slh-dsa/> on related topics\n> <https://conduition.io/cryptography/quantum-hbs/>.\n>\n> The post-quantum HD wallet derivation problem is one i've been thinking\n> about a lot lately. Due to the lack of algebraic structure in SLH-DSA it's\n> gonna be impossible to fully emulate BIP32 with that scheme alone. I'm\n> personally hoping that we'll find a way to derive child pubkeys using\n> lattices (ML-DSA) and/or isogenies (SQIsign), but I haven't heard of any\n",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 1,
            "text_length": 2073,
            "has_nack": false,
            "has_ack": true
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 0
      }
    },
    {
      "title": "Re: [bitcoindev] Re: Hash-Based Signatures for Bitcoin's Post-Quantum Future",
      "message_count": 1,
      "participants": [
        "Olaoluwa Osuntokun"
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/CAO3Pvs8A0GLW-xdSHCKosjPqo7WSf-=YJ7F7s7t65RvtArcBEQ@mail.gmail.com",
          "title": "Re: [bitcoindev] Re: Hash-Based Signatures for Bitcoin's Post-Quantum Future",
          "author": "Olaoluwa Osuntokun <laolu32@gmail@com>",
          "date": "Tue, 9 Dec 2025 16:53:00 -0800",
          "body": "[-- Attachment #1: Type: text/plain, Size: 12933 bytes --]\n\nHi y'all,\n\nMike wrote:\n> But if we use different parameters sets don’t we already loose the\n> compatiability with the standardized schemes?\n\nIIUC, these smaller SPHINCS+ parameters are under active consideration\nby NIST [1].\n\nOn slide 3 of a recent talk [2] at the 6th PQC Standardization Conference\n[3], Quynh Dang states:\n\n> We plan to standardize 2^24-signature limit rls128cs1, rls192cs1,\nrls256cs1\n\nBut then later in the same slide that:\n> We don’t plan to standardize them now:\n> Lower limits come with higher security risks when misuse happens\n> Minimize the number of parameter sets\n\nIn the linked forum post the OP advocates for a swifter standardization\nprocess for the new params:\n\n> We believe all options should be standardized as soon as possible. To meet\n> the 2030–2035 targets for post-quantum–only deployments, development must\n> be finalized very soon. Roots of trust typically have lifetimes exceeding\n> a decade, and any further delay could make it impossible to adopt these\n> new options.\n\nSo it appears they do plan to standardize these additional parameter sets,\nbut it won't be done \"soon\"?\n\n-- Laolu\n\n[1]: https://groups.google.com/a/list.nist.gov/g/pqc-forum/c/x-eaz9be6_U\n\n[2]:\nhttps://csrc.nist.gov/csrc/media/presentations/2025/sphincs-smaller-parameter-sets/sphincs-dang_2.2.pdf\n\n[3]: https://csrc.nist.gov/Events/2025/6th-pqc-standardization-conference\n\nOn Tue, Dec 9, 2025 at 3:17 PM 'Mikhail Kudinov' via Bitcoin Development\nMailing List <bitcoindev@googlegroups.com> wrote:\n\n> Dear Conduition,\n>\n> You did a really nice job, I was wondering if it will be hard to add the\n> different modifications to your implementations?\n>\n> As for lattice-based schemes and other assumptions, we also thought about\n> investigations the possibilities there.\n>\n> With this derivation technique you propose, am I understanding correctly\n> that if the user signs with the hash-based scheme, then the user would\n> reveal that",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 2077,
            "has_nack": false,
            "has_ack": false
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 0
      }
    },
    {
      "title": "Re: [bitcoindev] A safe way to remove objectionable content from the blockchain",
      "message_count": 1,
      "participants": [
        "Peter Todd"
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/aTl8Y7p4qtYAsHbP@petertodd.org",
          "title": "Re: [bitcoindev] A safe way to remove objectionable content from the blockchain",
          "author": "Peter Todd <pete@petertodd@org>",
          "date": "Wed, 10 Dec 2025 13:57:55 +0000",
          "body": "[-- Attachment #1: Type: text/plain, Size: 2902 bytes --]\n\nOn Tue, Dec 09, 2025 at 11:32:48AM -0800, Boris Nagaev wrote:\n> Hi waxwing/AdamISZ,\n> \n> On incentives: agreed that \"good\" only matters if it's an equilibrium. The \n> aim is to shape early design choices so the incentive-compatible \n> equilibrium includes DA and forced publication, rather than slipping into a \n> DA-weak equilibrium where only a few parties hold full data.\n\nExactly.\n\nFurthermore I want to be clear that in this context, the existence of strong ZK\nmath is an *exploit* on the Bitcoin protocol, in much the same way that a\nmathematical advancement that could be used to break SHA256 preimage security\nis also an exploit on the Bitcoin protocol.\n\nIt may be the case that the power of ZK techniques is sufficiently strong that\nBitcoin needs to be redesigned to mitigate them; there is even a small chance\nthat this is not possible and Lightning/HTLCs eventually become insecure due to\nit. No different than how there is a small chance that quantum computing\nrelevant to cryptography turns out to be real and numerous protocols become\ninsecure due to it.\n\n> > what if mining was done just on an accumulator over the utxo set, instead \n> of the utxo set itself?\n> \n> If miners and nodes only see an UTXO accumulator, how do HTLCs survive? The \n> HTLC success spend path needs the preimage to be revealed and readable. How \n> does this fit in an accumulator-only mining model, and what forces \n> publication so the payer can claim its incoming HTLC?\n\nMore generally, if mining is just an accumulator, how do we preserve censorship\nresistence? It's unlikely that the underlying math of the accumulator allows\nanyone to mine a new block with exactly as much data as is required to verify\nthe accumulator. \n\nRecently I met someone who told me that his company needed a full archival node\nof the Solana (IIRC) blockchain. That is, *all* Solana transactions going back\nin time, sufficient to verify everything. They had a very large b",
          "drama_signals": {
            "drama_keywords": 1,
            "positive_keywords": 2,
            "text_length": 2080,
            "has_nack": false,
            "has_ack": true
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 1
      }
    },
    {
      "title": "Re: [bitcoindev] Motion to Activate BIP 3",
      "message_count": 1,
      "participants": [
        "Mat Balez"
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/CABd6=MPtx9rN2ZtTz7CbT-zb-3qVUecZZrmb56aFyCSVeLxsEQ@mail.gmail.com",
          "title": "Re: [bitcoindev] Motion to Activate BIP 3",
          "author": "Mat Balez <matbalez@gmail@com>",
          "date": "Thu, 20 Nov 2025 12:14:09 -0800",
          "body": "[-- Attachment #1: Type: text/plain, Size: 14565 bytes --]\n\nMore and more of writing by all humans, including BIP proposers, will\ninevitably involve AI in some more or less significant way. I don't expect\npeople to reliably express the degree to which AI was used to inform the\nthinking behind the BIP, or the writing itself. I'm not aware of any common\nstandard we would use to express those things. Adversarially, we have to\nassume people won't do it if it's not in their interests.\n\nRather, I think the expectation should be that BIP proposers are entirely\nresponsible for submitting high quality BIPs and they take ownership for\nwhat they are submitting (submitting garbage burns your rep, always has and\nalways will). BIP reviewers should simply assume for all BIPs that AI was\nlikely used significantly to create them, and judge BIPs only on the merit\nof the ideas and content.\n\nBecause of the advent of LLMs (and their inevitable continued improvement)\nthis will almost certainly result in an increased number of BIPs being\nadvanced, many of low (slop-filled) quality but also, hopefully, more high\nquality ones as well—proposals that might not otherwise have seen the light\nof day and/or proposals themselves being strengthened with better\narguments, ideas and language.\n\nThe solution to such a rise in volume IMO is that BIP reviewers should also\nequip themselves with LLMs and other AI-powered tools to help\nfilter/triage/assess BIPs to get a handle on the rise in noise level. Yet,\njust like BIP proposers, the onus should be on BIP reviewers to take\nownership for the quality of the decision-making around BIP quality and\nthat it not ever be entirely automated but retain \"human in the loop\"\njudgment—at least for the foreseeable future—just made more efficient and\neffective through the use of AI.\n\nOn Thu, Nov 20, 2025 at 1:47 AM Oghenovo Usiwoma <eunovo9@gmail•com> wrote:\n\n> > I think it makes sense to request that submissions should state if - and\n> to what degree - AI has been use",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 1,
            "text_length": 2042,
            "has_nack": false,
            "has_ack": false
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 0
      }
    },
    {
      "title": "Re: [bitcoindev] Re: Motion to Activate BIP 3",
      "message_count": 1,
      "participants": [
        "Greg Sanders"
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/899eb548-3e3b-4b85-8ae0-1e64d15f1b86n@googlegroups.com",
          "title": "Re: [bitcoindev] Re: Motion to Activate BIP 3",
          "author": "Greg Sanders <gsanders87@gmail@com>",
          "date": "Thu, 13 Nov 2025 10:54:20 -0800 (PST)",
          "body": "[-- Attachment #1.1: Type: text/plain, Size: 7475 bytes --]\n\nMakes sense to me, and this e-mail can be a reference point if there's \nfuture discussion.\n\nWith what little review I've done, I think this makes sense to activate!\n\nGreg\n\nOn Wednesday, November 12, 2025 at 7:30:59 PM UTC-5 Murch wrote:\n\n> Hey Greg,\n>\n> Two sections from BIP 3 stand out as relevant here, “BIP Ownership“ and \n> “Deployed Process BIPs”.\n>\n> From “Fundamentals > BIP Ownership”:\n> > “[…] As a BIP progresses through the workflow, it becomes \n> increasingly co-owned by the Bitcoin community.”\n>\n> While Deployed BIPs are considered final and changes should be avoided, \n> the section has a subsection that specifically addresses Process BIPs.\n>\n> From “Workflow > Progression through BIP Statuses > Deployed > Process \n> BIPs”:\n> > “A Process BIP may change status from Complete to Deployed when it \n> achieves rough consensus on the Bitcoin Development Mailing List. A \n> proposal is said to have rough consensus if its advancement has been \n> open to discussion on the mailing list for at least one month, the \n> discussion achieved meaningful engagement, and no person maintains any \n> unaddressed substantiated objections to it. Addressed or obstructive \n> objections may be ignored/overruled by general agreement that they have \n> been sufficiently addressed, but clear reasoning must be given in such \n> circumstances. Deployed Process BIPs may be modified indefinitely as \n> long as a proposed modification has rough consensus per the same criteria.”\n>\n> More specific rules supersede general rules, so this subsection on \n> Process BIPs should hopefully clearly override the general description \n> in “Deployed”. It follows from these two sections that the BIP Authors’ \n> right to decide about changes to their BIP is moderated by the community \n> interests. I would consider especially Process BIPs to be dominantly \n> owned by the community rather than the Authors once they are Deployed. \n> The quoted section s",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 2,
            "text_length": 2046,
            "has_nack": false,
            "has_ack": false
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 0
      }
    },
    {
      "title": "[bitcoindev] CTV activation meeting on IRC - Thursday 18 December 17:00 UTC",
      "message_count": 1,
      "participants": [
        "\"/dev /fd0\""
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/CALiT-Zqo-gRCeMDd2VtmXL7Ox31OT3pFmsfXQ5mq6SzeuFnMzg@mail.gmail.com",
          "title": "[bitcoindev] CTV activation meeting on IRC - Thursday 18 December 17:00 UTC",
          "author": "\"/dev /fd0\" <alicexbtong@gmail@com>",
          "date": "Wed, 10 Dec 2025 03:38:14 +0530",
          "body": "[-- Attachment #1: Type: text/plain, Size: 775 bytes --]\n\nHi everyone,\n\nWe will organize a meeting next week to discuss the activation parameters\nfor BIP 119. You can review the related pull requests, different activation\nmethods, past meeting logs etc. and participate in the discussion.\n\nIRC Channel: #ctv-csfs-activation on libera.chat\nAgenda: Discuss activation parameters and build activation client for BIP\n119\n\n/dev/fd0\nfloppy disk guy\n\n-- \nYou received this message because you are subscribed to the Google Groups \"Bitcoin Development Mailing List\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to bitcoindev+unsubscribe@googlegroups•com.\nTo view this discussion visit https://groups.google.com/d/msgid/bitcoindev/CALiT-Zqo-gRCeMDd2VtmXL7Ox31OT3pFmsfXQ5mq6SzeuFnMzg%40mail.gmail.com.\n\n[-- Attachment #2: Type: text/html, Size: 1197 bytes --]",
          "drama_signals": {
            "drama_keywords": 2,
            "positive_keywords": 0,
            "text_length": 966,
            "has_nack": false,
            "has_ack": false
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 2
      }
    },
    {
      "title": "[bitcoindev] Request for early peer review of two BIP drafts (BUDS and segOP)",
      "message_count": 1,
      "participants": [
        "Callum"
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/CAOj3_X-WjPYaiTj5NQnAFp+df+4qWCT8oG_c9DsS-TZ47SjZow@mail.gmail.com",
          "title": "[bitcoindev] Request for early peer review of two BIP drafts (BUDS and segOP)",
          "author": "Callum <cal.defenwycke@gmail@com>",
          "date": "Mon, 8 Dec 2025 21:52:26 +0000",
          "body": "[-- Attachment #1: Type: text/plain, Size: 1167 bytes --]\n\nHello all,\n\nI would appreciate early peer review on two BIP drafts I have published:\n\n   1.\n\n   BUDS (Bitcoin Unified Data Standard) — an informational BIP defining a\n   neutral, non-consensus taxonomy for describing transaction data.\n\n   Draft and reference materials:\n   https://github.com/defenwycke/bip-buds\n   2.\n\n   segOP (Segregated OP_RETURN) — a consensus proposal describing a\n   structured TLV data section and corresponding commitment output.\n\n   Draft and reference materials:\n   https://github.com/defenwycke/bip-segop\n\nBoth drafts are small and self-contained. Feedback on clarity, correctness,\nstructure, or missing considerations would be very welcome.\n\nThank you for your time.\n\nKind regards,\n\nDefenwycke\n\n08.12.2025\n\n-- \nYou received this message because you are subscribed to the Google Groups \"Bitcoin Development Mailing List\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to bitcoindev+unsubscribe@googlegroups•com.\nTo view this discussion visit https://groups.google.com/d/msgid/bitcoindev/CAOj3_X-WjPYaiTj5NQnAFp%2Bdf%2B4qWCT8oG_c9DsS-TZ47SjZow%40mail.gmail.com.\n\n[-- Attachment #2: Type: text/html, Size: 2528 bytes --]",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 1,
            "text_length": 1323,
            "has_nack": false,
            "has_ack": true
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 0
      }
    },
    {
      "title": "[bitcoindev] A safe way to remove objectionable content from the blockchain (now on GitHub)",
      "message_count": 1,
      "participants": [
        "Lazy Fair"
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/CABHzxrjfvyBRD7sG9rngvDhr9cfzLEQibn4bup_J8pz7UHQpqA@mail.gmail.com",
          "title": "[bitcoindev] A safe way to remove objectionable content from the blockchain (now on GitHub)",
          "author": "Lazy Fair <laissez.faire.btc@gmail@com>",
          "date": "Sat, 6 Dec 2025 17:41:10 +1100",
          "body": "[-- Attachment #1: Type: text/plain, Size: 4166 bytes --]\n\nI've started putting together some ideas around how to remove objectionable\ncontent from the blockchain. The very early work-in-progress description is\non GitHub: https://github.com/laissez-faire-btc/safe-remove\n\nI won't include all the details here, because there's necessarily a lot to\ncover, but the basic design goals I've aimed to address are something like\nthis:\n\n* optional - each node gets to decide what to remove, if anything\n* safe - provably no harm is done to those not choosing to use it, and any\ncost or risk to those using it is well understood, minimal, and mitigated\n* full node functionality - a node that does remove content can still do\neverything it could have done otherwise, without relying on anyone else\n* retrospective - content that exists on the blockchain today\n(pre-implementation) can be removed later (post-implementation)\n* trustless, verifiable, permissionless - control messages enabling data to\nbe removed are simple verifiable statements of fact that can be written by\nanybody\n* lightweight - minimal changes and impact to policy, consensus,\nimplementation, usage, the economy\n* granularity, associativity, commutability, idempotence - the least\npossible data is removed, and ordering is inconsequential\n* transferable - nodes that choose to remove objectionable content can\nshare those blocks (with content removed) with others who hold the same\nobjection, so that the receiver may never even momentarily hold the\nobjectionable content\n\nBeing design goals, these are probably not all achievable. I'll need your\nhelp to work through all the details.\n\nI have some more notes I just haven't written up yet, so I'm keen for your\ninput please, on what direction I should take, questions I should answer,\naspects I should consider or detail further, etc.\n\nIn the absence of any feedback, I'll be proceeding with either documenting\nthe threat model, or a bit of a literature review - starting with the\nfollowi",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 1,
            "text_length": 2092,
            "has_nack": false,
            "has_ack": true
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 0
      }
    },
    {
      "title": "Re: [bitcoindev] op_ctv still has no technical objections",
      "message_count": 1,
      "participants": [
        "\"/dev /fd0\""
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/CALiT-ZrgNdMVs7mAWRhrXEqkyxXAdjE-gwjZfpv=2arzcSFopA@mail.gmail.com",
          "title": "Re: [bitcoindev] op_ctv still has no technical objections",
          "author": "\"/dev /fd0\" <alicexbtong@gmail@com>",
          "date": "Sun, 30 Nov 2025 04:38:27 +0530",
          "body": "[-- Attachment #1: Type: text/plain, Size: 2986 bytes --]\n\nHi Erik,\n\nWe can organize a meeting to discuss the activation parameters and build an\nactivation client. If enough economic nodes and miners run the activation\nclient it will be activated with no further politics or drama.\n\n#ctv-csfs-activation IRC channel can be used for the meeting.\n\n/dev/fd0\nfloppy disk guy\n\nOn Thu, Nov 27, 2025 at 2:47 PM Erik Aronesty <erik@q32•com> wrote:\n\n> It's been many years and there's been a lot of discussion about various\n> covenants\n>\n> I think one of the biggest problems is everyone has to insist on their\n> baby is the best baby.\n>\n> op_ctv is quite literally not the best at anything.  That's the whole\n> point.  It's non-recursive, can't be used for strange or dangerous things,\n> and can be used to emulate a lot of other opcodes.\n>\n> It's adequate.  And I don't think we want anything \"better\" than adequate\n> the first time around. lnhance is more comprehensive.  but also it's so\n> much harder to reason about three separate op codes and what the attack\n> surface could be.\n>\n> I don't think it's possible to optimize a series of covenants for all\n> possible scenarios.  Easy to make them too powerful and now nodes are doing\n> too much work and we're attracting the kind of network activity that nobody\n> wants.\n>\n> Fortunately the risk of CTV is fairly low.  It's always possible to turn\n> it off (no new tx)... if there's a game theory issue.\n>\n> I don't think there's any particular rush, but we could lose a lot of fees\n> and support for miners if Bitcoin continues to do what it is doing now...\n> scaling almost entirely in custodial systems.  That's also just not the\n> Bitcoin that anyone loves.\n>\n> At this point it feels like it's \"perfect is the enemy of the good\".\n>\n> We have an old and rather well tested pull request that is only a handful\n> of lines of code that everyone has scrutinized a million ways.\n>\n> I don't think we're getting that for any other covenant opcode.\n>\n>\n>\n>\n>",
          "drama_signals": {
            "drama_keywords": 3,
            "positive_keywords": 2,
            "text_length": 2058,
            "has_nack": false,
            "has_ack": true
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 3
      }
    },
    {
      "title": "[bitcoindev] Re: op_ctv still has no technical objections",
      "message_count": 1,
      "participants": [
        "\"'conduition' via Bitcoin Development Mailing List\""
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/2914ad6f-7e1a-42b6-9c25-87ac48c63228n@googlegroups.com",
          "title": "[bitcoindev] Re: op_ctv still has no technical objections",
          "author": "\"'conduition' via Bitcoin Development Mailing List\" <bitcoindev@googlegroups.com>",
          "date": "Thu, 27 Nov 2025 18:04:28 -0800 (PST)",
          "body": "[-- Attachment #1.1: Type: text/plain, Size: 2607 bytes --]\n\nAs someone who has had a merely passive interest in covenants tech, I can \nconfidently say that OP_CTV is probably the only covenants proposal whose \neffects I can confidently say I fully grasp. It's also easy to explain to \nothers. Not saying i'm not in favor of more complex multi-pronged upgrades \nlike LNHANCE, just saying I don't fully understand their opcode interplay \nenough to say yay/nay. Which is maybe an under-represented argument in \nfavor of plain OP_CTV.\n\nregards,\nconduition\n\nOn Thursday, November 27, 2025 at 1:18:03 AM UTC-8 Erik Aronesty wrote:\n\n> It's been many years and there's been a lot of discussion about various \n> covenants \n>\n> I think one of the biggest problems is everyone has to insist on their \n> baby is the best baby. \n>\n> op_ctv is quite literally not the best at anything.  That's the whole \n> point.  It's non-recursive, can't be used for strange or dangerous things, \n> and can be used to emulate a lot of other opcodes. \n>\n> It's adequate.  And I don't think we want anything \"better\" than adequate \n> the first time around. lnhance is more comprehensive.  but also it's so \n> much harder to reason about three separate op codes and what the attack \n> surface could be.\n>\n> I don't think it's possible to optimize a series of covenants for all \n> possible scenarios.  Easy to make them too powerful and now nodes are doing \n> too much work and we're attracting the kind of network activity that nobody \n> wants.  \n>\n> Fortunately the risk of CTV is fairly low.  It's always possible to turn \n> it off (no new tx)... if there's a game theory issue. \n>\n> I don't think there's any particular rush, but we could lose a lot of fees \n> and support for miners if Bitcoin continues to do what it is doing now... \n> scaling almost entirely in custodial systems.  That's also just not the \n> Bitcoin that anyone loves.\n>\n> At this point it feels like it's \"perfect is the enemy of the good\".  \n>\n> We have ",
          "drama_signals": {
            "drama_keywords": 2,
            "positive_keywords": 1,
            "text_length": 2058,
            "has_nack": false,
            "has_ack": true
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 2
      }
    },
    {
      "title": "Re: [bitcoindev] OP_CIV - Post-Quantum Signature Aggregation",
      "message_count": 1,
      "participants": [
        "\"'conduition' via Bitcoin Development Mailing List\""
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/02e3cb64-50e8-4814-bf53-72db87deafc8n@googlegroups.com",
          "title": "Re: [bitcoindev] OP_CIV - Post-Quantum Signature Aggregation",
          "author": "\"'conduition' via Bitcoin Development Mailing List\" <bitcoindev@googlegroups.com>",
          "date": "Fri, 28 Nov 2025 10:52:27 -0800 (PST)",
          "body": "[-- Attachment #1.1: Type: text/plain, Size: 17217 bytes --]\n\nHey Tadge,\n\nYou're right that OP_CIV would discourage address reuse, but it'd also make \nlife difficult for wallet developers who want to adopt it. Some wallet devs \n*today* don't even bother with multi-address support, so imagine if to do \nso effeciently, they needed to statefully track prior UTXOs and generate \naddresses based on a changing UTXO set over time.\n\nAlso I want to mention, there's a big privacy difference between these CISA \ntechniques, and CISA via address reuse. \n\nIf I receive two payments to the same address, i immediately reveal that \nthose UTXOs are owned by the same entity: me.\n\nIf I receive two payments to *distinct* addresses which are linked via your \nOP_CIV (or via my idea by committing to pubkeys) then I can choose when and \nwhether to reveal the fact that those UTXOs are commonly owned. Most user \nwallets have more than just two UTXOs, so in a setting where I have \npossibly dozens of UTXOs, this offers me more flexibility with respect to \nmy on-chain privacy, allowing me to choose when and how to reveal common \nUTXO ownership. This is kind of already the status quo, because chainalysis \nuses common-input ownership heuristics even if they are flawed/incorrect, \njust for the sake of having a \"working\" tool they can sell.\n\nRegarding the extra cost, we can quantify that! Let's say we have a taptree \nof height `h` with `2^h` leaves. We use one leaf for a unique pubkey, and \nthe other `2^h - 1` tap leaves store commitments to other pubkeys or to \npre-existing UTXOs. To spend a TX with `n` inputs using this CISA paradigm, \nwe need one signature, plus `n - 1` taproot control blocks and tapscripts. \nEach control block has size `h * 32`, plus ~32 bytes to reference the other \npubkey or UTXO in the locking tapscript. So in total, the witness size \nscales as: `(n - 1)((h + 1) * 32)`. In other words, for every additional \ninput covered by the CISA scheme, we must pay for roughly `(h + 1) * 32",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 1,
            "text_length": 2061,
            "has_nack": false,
            "has_ack": true
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 0
      }
    },
    {
      "title": "Re: [bitcoindev] SLH-DSA (SPHINCS) Performance Optimization Techniques",
      "message_count": 1,
      "participants": [
        "Tim Ruffing"
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/6ad6c7418b6b845d6e2dd0ccdb2b508de0c3c10c.camel@real-or-random.org",
          "title": "Re: [bitcoindev] SLH-DSA (SPHINCS) Performance Optimization Techniques",
          "author": "Tim Ruffing <me@real-or-random@org>",
          "date": "Fri, 28 Nov 2025 16:39:12 +0100",
          "body": "Let me just say that leave the note here that this is awesome work!\n\nI didn't expect that so much can be gained using SIMD, and that it\nbeats SHA-NI by such a large margin (even taking into account the\ncaveats you've mentioned).\n\nTim\n\nOn Sun, 2025-11-23 at 18:46 -0800, 'conduition' via Bitcoin Development\nMailing List wrote:\n> Hi devs,\n> \n> I've spent the last several months implementing and benchmarking\n> optimization techniques for the post-quantum hash-based signature\n> scheme SLH-DSA (formerly SPHINCS+), which is being considered as a\n> candidate for a quantum-resistant soft-fork upgrade to Bitcoin, re:\n> BIP360.\n> \n> Survey article: https://conduition.io/code/fast-slh-dsa/\n> \n> char1.png\n> \n> As a material result of my findings, I believe I now possess what may\n> be the fastest publicly available implementation of SLH-DSA (at least\n> on my hardware), and possibly also one of the fastest GPU\n> implementations, though I've had difficulty finding comparable\n> alternatives on that front. Its speed is owed to the Vulkan graphics\n> programming API, often used by video game devs to squeeze performance\n> out of gaming PCs and mobile phones.\n> \n> The code: \n> - https://github.com/conduition/slhvk\n> - https://github.com/conduition/slh-experiments\n> \n> Using my CPU, this code can sign a message with SLH-DSA-SHA2-128s in\n> just 11 milliseconds, and can generate keys in only 2 milliseconds\n> (1ms if batched). Verification throughput approaches that of ECDSA,\n> at around 15000 nanoseconds per verification if properly batched. If\n> you have a GPU with drivers, everything runs even faster.\n> \n> For perspective, the fastest open source SLH-DSA library I could\n> find, PQClean, requires 94 milliseconds for SLH-DSA-SHA2-128s signing\n> and 12ms for keygen on my CPU. PQClean can only achieve this speed on\n> x86 CPUs, whereas Vulkan works on ARM devices, including Apple\n> silicon.\n> \n> There are caveats. This technique is memory-hungry, requiring several\n> megabytes of RAM for signin",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 2071,
            "has_nack": false,
            "has_ack": false
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 0
      }
    },
    {
      "title": "[bitcoindev] Q-Lock: Quantum-Resistant Spending via ECDSA + Hash-Based Secrets",
      "message_count": 1,
      "participants": [
        "Amarildo"
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/1c6c4afc-0fe4-4b72-b70f-3f6ba4c19315n@googlegroups.com",
          "title": "[bitcoindev] Q-Lock: Quantum-Resistant Spending via ECDSA + Hash-Based Secrets",
          "author": "Amarildo <amarildocaka01@gmail@com>",
          "date": "Fri, 28 Nov 2025 07:00:04 -0800 (PST)",
          "body": "[-- Attachment #1.1: Type: text/plain, Size: 14499 bytes --]\n\nHi everyone,\n\nI'd like to propose an alternative approach to quantum resistance \nfor Bitcoin that I believe is simpler than BIP-360 P2QRH.\n\n**Q-Lock: Quantum-Resistant Spending Protocol**\n\nSUMMARY:\n- Keeps ECDSA unchanged (no new signature algorithms!)\n- Adds hash-based secret layer on top\n- Uses only SHA256 + Merkle trees (proven crypto)\n- ~3 KB transactions (comparable to FALCON)\n- Two-phase commit-reveal scheme\n- Soft fork compatible\n- BIP-32 HD wallets work normally\n\nKEY INSIGHT:\nInstead of replacing ECDSA with new post-quantum algorithms \n(FALCON, SPHINCS+, Dilithium), Q-Lock adds a quantum-safe \nsecret layer. Attacker must break BOTH ECDSA AND know the \nhash preimages - quantum computers can't reverse SHA256.\n\nCOMPARISON TO BIP-360:\n- BIP-360: New lattice-based crypto, 1.3-50 KB sigs, breaks BIP-32\n- Q-Lock: Proven SHA256 crypto, ~3 KB sigs, BIP-32 works\n\nHOW IT WORKS:\n1. Setup: Generate 64 random secrets, commit via Merkle root\n2. Commit phase: Lock outputs WITHOUT exposing pubkey\n3. Reveal phase: Expose pubkey + secrets at block-hash-determined positions\n4. Quantum attacker sees pubkey too late - outputs already locked!\n\n```\nQ-Lock is a quantum-resistant spending protocol for Bitcoin \nthat adds a hash-based secret layer on top of existing ECDSA \nsignatures. It uses a two-phase commit-reveal scheme where \nspending positions are determined by the block hash, making \nit secure against quantum attackers who can break ECDSA.\n\nQ-Lock does NOT replace ECDSA. It adds quantum protection \nwhile preserving Bitcoin's existing cryptographic foundation.\n\nTransaction size: ~3 KB\nRequires: Soft fork (1-2 new opcodes)\n```\n\n-----\n\n## MOTIVATION\n\n```\nTHE QUANTUM THREAT:\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\nShor's algorithm can break ECDSA by extracting private keys \nfrom public keys. When a Bitcoin transaction is broadcast, \nthe public key is exposed in the mempool. A quantum attacker \ncould:\n\n1. See public ke",
          "drama_signals": {
            "drama_keywords": 2,
            "positive_keywords": 1,
            "text_length": 2079,
            "has_nack": false,
            "has_ack": true
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 2
      }
    },
    {
      "title": "Re: [bitcoindev] Benchmarking Bitcoin Script Evaluation for the Varops Budget (GSR)",
      "message_count": 1,
      "participants": [
        "\"'Julian' via Bitcoin Development Mailing List\""
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/5906e2bb-c215-44b0-bb61-0bb91d55717dn@googlegroups.com",
          "title": "Re: [bitcoindev] Benchmarking Bitcoin Script Evaluation for the Varops Budget (GSR)",
          "author": "\"'Julian' via Bitcoin Development Mailing List\" <bitcoindev@googlegroups.com>",
          "date": "Fri, 28 Nov 2025 05:09:25 -0800 (PST)",
          "body": "[-- Attachment #1.1: Type: text/plain, Size: 6894 bytes --]\n\nHi Russell,\n\nthanks for taking a look at the code.\n\nIn interpreter.cpp the static function EvalChecksigTapscript(...) is \nresponsible for subtracting from execdata.m_validation_weight_left, for the \noriginal SigVersion::TAPSCRIPT this is still the case, but Tapscript v2 is \nimplemented as a new SigVersion::TAPSCRIPT_V2 and therefore it will not \ntake the original sigops constraint into account (there is an if condition \nright above checking for the SigVersion).\n\nThe new varops budget replaces this sigops constraint and is contained in \nthe new EvalScript(...) overload. Currently it will only subtract from the \nbudget if the checksig succeeds, but I think this should be moved up a \nstatement, such that it will always subtract the varops cost, making the \ncost calculation more static.\n\nThe changes have not been reviewed in depth and I am looking for someone \ninterested in helping me with that.\n\n\n\nOn Monday, 10 November 2025 at 15:48:27 UTC+1 Russell O'Connor wrote:\n\nMy understanding is that in order to avoid block assembly becoming an \nNP-hard packing problem, there must be only one dimension of constraint \nsolving.  However, AFAICT, in your tarscript V2 code you have both the new \nvarops constraint and the original sigops constraint.\n\nFWIW, in Simplicity we reuse the same budget mechanism introduced in \ntapscript (V1) with our cost calculations (though our costs are computed \nstatically instead of dynamically at runtime for better or for worse).\n\nOn Fri, Nov 7, 2025 at 11:06 AM 'Julian' via Bitcoin Development Mailing \nList <bitco...@googlegroups•com> wrote:\n\nHello everyone interested in Great Script Restoration and the Varops Budget,\n\nThe main concerns that led to the disabling of many opcodes in v0.3.1 were \ndenial-of-service attacks through excessive computational time and memory \nusage in Bitcoin script execution. To mitigate these risks, we propose to \ngeneralize the sigops budget in a new Tapscript le",
          "drama_signals": {
            "drama_keywords": 1,
            "positive_keywords": 1,
            "text_length": 2084,
            "has_nack": false,
            "has_ack": true
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 1
      }
    },
    {
      "title": "[bitcoindev] Re: AI-assisted drafts and disclosure",
      "message_count": 1,
      "participants": [
        "Oghenovo Usiwoma"
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/CAOCjZ9RmtKWALV033EENs=wzt43GUG9k4mOeJkAzvyp2e2xj9w@mail.gmail.com",
          "title": "[bitcoindev] Re: AI-assisted drafts and disclosure",
          "author": "Oghenovo Usiwoma <eunovo9@gmail@com>",
          "date": "Thu, 20 Nov 2025 18:48:02 +0100",
          "body": "[-- Attachment #1: Type: text/plain, Size: 2166 bytes --]\n\n> Clear disclosure of AI assistance as a process note, not a stigma.\n\nI agree with you, but I think disclosure of AI assistance will be treated\nas \"stigma\", even if that was not its intention. This is my issue with the\n\"AI-label\". If I use AI for research, do I have to add \"AI label\" to my\nBIP? at what point do I have to add the label?\n\n- Novo\n\nOn Thu, Nov 20, 2025 at 1:16 PM nt yl <wrapperband@googlemail•com> wrote:\n\n> Hi Oghenovo Usiwoma and Bitcoin Mechanic,\n>\n> You wrote:\n>\n> In my humble opinion, I believe that humans will continue to use the\n> easiest method available to them to achieve their goals. If we agree that\n> humans will do this, then there will be a lot of AI-assisted content. If I\n> did write an AI-assisted BIP draft, why would I add this \"AI-label\" to my\n> BIP when I know that it will cause reviewers to ignore it?\n>\n> As a disabled person who uses AI tools, my view is that AI will soon be\n> part of most serious workflows, much like reading the manuals and prior\n> discussions is today. Used well, it can summarise long threads, prioritise\n> issues, deduplicate proposals, and help check code for obvious bugs.\n> Refusing to use any such tools can be a step backward in productivity.\n>\n> The key is how we use them. I would support:\n>\n>    -\n>\n>    Clear disclosure of AI assistance as a process note, not a stigma.\n>    -\n>\n>    Strong norms that final authorship, technical accuracy, and\n>    accountability rest with the human proposer.\n>    -\n>\n>    Encouraging A.I. for review support, not for replacing understanding.\n>\n> This balances transparency with practical benefits and keeps the bar on\n> rigour where it belongs.\n>\n> Best,\n> Wrapper\n>\n> https://www.zerogpt.com/   0% A.I.\n>\n\n-- \nYou received this message because you are subscribed to the Google Groups \"Bitcoin Development Mailing List\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to bitcoindev+unsubsc",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 2,
            "text_length": 2051,
            "has_nack": false,
            "has_ack": true
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 0
      }
    },
    {
      "title": "[bitcoindev] Announcing Penlock v1: Paper-Based Secret Splitting for BIP39 Seed Phrases",
      "message_count": 1,
      "participants": [
        "\"'Rama Gan' via Bitcoin Development Mailing List\""
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/GbNAPQX2Q4TzZvS6aWLPf3iy7z1yTXVrgnPpXazBjdcWH-zROBEBieE02r6GX128LM7mml6oTzAmlboV97EWpG1ujLcVZ6fx6uihUMXxCEo=@proton.me",
          "title": "[bitcoindev] Announcing Penlock v1: Paper-Based Secret Splitting for BIP39 Seed Phrases",
          "author": "\"'Rama Gan' via Bitcoin Development Mailing List\" <bitcoindev@googlegroups.com>",
          "date": "Thu, 20 Nov 2025 09:04:24 +0000",
          "body": "Hello everyone,\n\nI am thrilled to announce the public release of Penlock!\n\nThe goal is achieved! If you have a printer, scissors, a craft knife,\nand a pin, you can mechanically secret-split a 12-word seed phrase\nin under two hours. This includes the entire process—learning,\nprinting, assembling, executing, and storing the shares.\n\nPenlock is a printable paper calculator that guides you through\nsplitting a seed phrase into a 2-of-3 backup. It is open-source,\nuses straightforward and robust cryptography, and includes various\nfail-safes that protect against errors. A beta was announced on\nthis list last year, and the public release is now available at:\n<https://v1.penlock.io/en/>\n\nThis release breaks backward-compatibility with the beta, allowing for\nenhancements that make Penlock significantly easier to operate. Here\nare the main improvements in v1:\n\n- Faster Secret-Splitting: Penlock now focuses exclusively on producing\n2-of-3 backups using its own paper-optimized splitting algorithm. The\nprevious iteration supported K-of-M splitting with Shamir Secret\nSharing, but at the cost of more complexity and a clunkier 2-of-3\nprocess. Since 2-of-3 covers nearly all use cases, optimizing for it\nseemed like the right approach.\n\n- Backup Strategy Template: Penlock now suggests a generic, adaptable\nbackup strategy that helps set up offsite recovery and trust-minimized\ninheritance. In short, each share is tied to a different type of\nstorage: Digital, Social, and (optionally) Legal. This ensures an\nattacker would have to run two different types of attacks, and makes\nit hard for a party holding one share to obtain a second one. You\ncan find more details at <https://v1.penlock.io/en/split#strategy>.\n\n- On-Paper Error Correction: Penlock v1 introduces what I believe to\nbe the first on-paper error correction algorithm. Each BIP39 word is\nextended with two pre-computed parity symbols, guaranteeing per-word\nunambiguous correction of 1 error and detection of 2. In practice,\nit's also poss",
          "drama_signals": {
            "drama_keywords": 1,
            "positive_keywords": 1,
            "text_length": 2088,
            "has_nack": false,
            "has_ack": true
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 1
      }
    },
    {
      "title": "Re: [bitcoindev] Improve Bitcoin’s resilience to large-scale power grid failures and Carrington-type solar storms",
      "message_count": 1,
      "participants": [
        "\"Edil Guimarães de Medeiros\""
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/CANJiN3LnCFJxJpGxScLTqT2JF4iQNsX3hipiNiBK_OLMkkXb1g@mail.gmail.com",
          "title": "Re: [bitcoindev] Improve Bitcoin’s resilience to large-scale power grid failures and Carrington-type solar storms",
          "author": "\"Edil Guimarães de Medeiros\" <jose.edil@gmail@com>",
          "date": "Wed, 19 Nov 2025 14:04:36 -0300",
          "body": "[-- Attachment #1: Type: text/plain, Size: 4863 bytes --]\n\nI don't see any specific measure that would require specific support from\nBitcoin Core, maybe you can point to more specific requirements.\nThe canonical approach is to maintain specific projects that solve specific\nproblems using one of the node interfaces (e.g. RPC).\nBut of course, anyone is free to contribute patches that might help handle\nthis kind of situation.\n\nAs you said, reorgs are expected to be gracefully handled already by the\nnode implementations.\nMost software that is tested in testnet probably also was exposed to harsh\nconditions like deep reorgs and long periods without any block being mined.\nHaving said that, the potential problem you describe is not specific to\nBitcoin and having alternative critical communication mechanisms is\ndesirable.\nBut they probably fall under the economically not viable kind of\ninfrastructure that humans have relied on governments to implement and\nmaintain, which is far from an ideal approach.\n\nAnd by the way, this is the mailing list.\n\nRegards.\n\nEm dom., 16 de nov. de 2025 às 20:00, Alexandre <alexandre.lg99@gmail•com>\nescreveu:\n\n> Hi,\n> I’m submitting this feature request to explore how Bitcoin could better\n> withstand extreme, long-lasting infrastructure failures caused by major\n> solar events. Before explaining the request itself, I want to provide a\n> brief overview of what these events are, because their scale matters.\n>\n> A large solar storm occurs when the Sun emits an intense burst of charged\n> particles and electromagnetic energy. When this material reaches Earth, it\n> can disturb the magnetic field and induce strong electric currents in long\n> conductors such as power lines. In extreme cases, this can damage\n> transformers, overload electrical grids, interrupt satellite operations,\n> and disrupt long-distance communication systems. The most famous historical\n> example is the Carrington Event of 1859, the largest geomagnetic storm ever\n> recorded. It trigge",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 1,
            "text_length": 2114,
            "has_nack": false,
            "has_ack": false
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 0
      }
    },
    {
      "title": "[bitcoindev] [BIP Proposal] Standardization of On-Chain Identity Publication",
      "message_count": 1,
      "participants": [
        "\"'Edyth Kylak Johnson' via Bitcoin Development Mailing List\""
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/2acc9c28-6413-4147-8d11-e1ae0a677b75n@googlegroups.com",
          "title": "[bitcoindev] [BIP Proposal] Standardization of On-Chain Identity Publication",
          "author": "\"'Edyth Kylak Johnson' via Bitcoin Development Mailing List\" <bitcoindev@googlegroups.com>",
          "date": "Wed, 19 Nov 2025 03:54:25 -0800 (PST)",
          "body": "[-- Attachment #1.1: Type: text/plain, Size: 943 bytes --]\n\nDear bitcoin-dev,\nI am submitting a draft Bitcoin Improvement Proposal titled *“Standardization \nof On-Chain Identity Publication”* for discussion. The draft specifies \ncanonical CBOR payloads, Poseidon-based `nullifier_hash` domain separation \n(`v0iden` / `v0corp`), and an optional Ed25519 signature wrapper. The \nproposal text and implementation notes are available in this \nPR: https://github.com/bitcoin/bips/pull/2038 . I welcome review and \nfeedback on interoperability, canonicalization (deterministic CBOR), and \nsecurity considerations.\n\n-- \nYou received this message because you are subscribed to the Google Groups \"Bitcoin Development Mailing List\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to bitcoindev+unsubscribe@googlegroups•com.\nTo view this discussion visit https://groups.google.com/d/msgid/bitcoindev/2acc9c28-6413-4147-8d11-e1ae0a677b75n%40googlegroups.com.\n\n[-- Attachment #1.2: Type: text/html, Size: 1196 bytes --]",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 1,
            "text_length": 1121,
            "has_nack": false,
            "has_ack": true
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 0
      }
    },
    {
      "title": "[bitcoindev] New bitcoin backbone code release + Tx relay v2 update",
      "message_count": 1,
      "participants": [
        "Antoine Riard"
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/299d4f39-b8cd-4736-b6bb-71def4d85f74n@googlegroups.com",
          "title": "[bitcoindev] New bitcoin backbone code release + Tx relay v2 update",
          "author": "Antoine Riard <antoine.riard@gmail@com>",
          "date": "Tue, 18 Nov 2025 16:01:33 -0800 (PST)",
          "body": "[-- Attachment #1.1: Type: text/plain, Size: 3679 bytes --]\n\nHello devs,\n\nShared new code for bitcoin backbone available on the website\n(bitcoinbackbone.org). Biggest changes from latest release has\nbeen mostly working on BIP324 re-implementation, cleaning bugs\nimplementing a simple tx-relay stack, a little mempoool buffer\nand some groundworks on address management. Tx syncing works with\nvanilla bitcoin core v0.30 software.\n\nI did a layout of the process architecture on the website, but the\nmempool is fully living in its own mempool process, fully separate\nfrom the block pipeline. In case of mempool DoS for whatever reasons,\nthe full-node keeps processing blocks. This also opens the door to\nhave *multiple* mempools with incompatible policies among themselves,\nand just select the highest fees paying graph of consensus-valid\ntransactions, after sanitizing out conflicts.\n\nAs I was writing in my latest email about bitcoin backbone, of course\nthere are some trade-offs with the mempool not living in the same memory\nspace than the validation engine, though I think you have practical\nimprovements on this area.\n\nThe simple tx-relay stack also implements a basic implementation of the\nproposed overhaul of the tx-relay v2 [0]. Currently, the tx flow is\nINV(txid) -> ; <- GETDATA(inv(txid)) ; TX(tx) -> . With the proposed \ntx-relay\nv2 overhaul, if an INV for the txid has not previously received for the\ntransaction, i.e the transaction processing has not been requested, the\ntransaction is strictly rejected, without further processing. This more\nstricter tx processing can be activated with a setting option in bitcoin\nbackbone.\n\nLong-term, I think some form of tx-relay link-level mitigation is a strong\nnecessity to diminish the surface attack of time-sensitive contracting\nprotocol in face of tx-relay throughput overflow, where a malicious peer\nis buying out your full-node tx bandwidth to tamper with the propagation\nof a time-sensitive tx (e.g a lightning's HTLC-preimage) [1].\n\nThe d",
          "drama_signals": {
            "drama_keywords": 3,
            "positive_keywords": 1,
            "text_length": 2068,
            "has_nack": false,
            "has_ack": true
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 3
      }
    },
    {
      "title": "[bitcoindev] OP_CHECKUTXOSETHASH idea",
      "message_count": 1,
      "participants": [
        "Erik Aronesty"
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/CAJowKgLE4kb7qT1NxXrmEssr8+fQGd-=7=m-BAsjePoti8TRRg@mail.gmail.com",
          "title": "[bitcoindev] OP_CHECKUTXOSETHASH idea",
          "author": "Erik Aronesty <erik@q32@com>",
          "date": "Mon, 29 Sep 2025 17:09:15 -0700",
          "body": "[-- Attachment #1: Type: text/plain, Size: 1671 bytes --]\n\nA soft fork could introduce a new opcode, `OP_CHECKUTXOSETHASH`, allowing\nminers to optionally commit a deterministic hash of the current UTXO set\ninto a block. If present, all nodes must verify its correctness or reject\nthe block; if absent, the block is still valid. Old nodes treat the opcode\nas unspendable, so backward compatibility is preserved.\n\nBecause computing the full UTXO root is costly, this makes each checkpoint\nintentionally expensive to produce, ensuring that miners will only include\nthem when compensated with sufficient fees. Additionally, it could be\nlimited to one per block.\n\nThe result is a voluntary, self-limiting, incentive-aligned, fee-driven\nsystem where checkpoints are cheaply consensus-enforced when included but\nnever mandatory.\n\nMost nodes could operate on a rolling history validated by occasional,\nhigh-value commitments, while archival nodes remain free to preserve the\nfull chain. This reduces the burden of initial sync and resource use\nwithout sacrificing Bitcoin’s security model, since any invalid checkpoint\nwould invalidate its block.\n\nIn practice, the chain becomes more efficient for everyday use while the\nhistorical record remains intact for those willing to bear the expense of\nmaintaining it.\n\n-- \nYou received this message because you are subscribed to the Google Groups \"Bitcoin Development Mailing List\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to bitcoindev+unsubscribe@googlegroups•com.\nTo view this discussion visit https://groups.google.com/d/msgid/bitcoindev/CAJowKgLE4kb7qT1NxXrmEssr8%2BfQGd-%3D7%3Dm-BAsjePoti8TRRg%40mail.gmail.com.\n\n[-- Attachment #2: Type: text/html, Size: 2187 bytes --]",
          "drama_signals": {
            "drama_keywords": 2,
            "positive_keywords": 1,
            "text_length": 1794,
            "has_nack": false,
            "has_ack": true
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 2
      }
    },
    {
      "title": "[bitcoindev] Re: OP_CHECKUTXOSETHASH idea",
      "message_count": 1,
      "participants": [
        "Eric Voskuil"
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/76c1b5a1-5b78-4576-981f-4df69aefc9a6n@googlegroups.com",
          "title": "[bitcoindev] Re: OP_CHECKUTXOSETHASH idea",
          "author": "Eric Voskuil <eric@voskuil@org>",
          "date": "Sun, 16 Nov 2025 11:11:16 -0800 (PST)",
          "body": "[-- Attachment #1.1: Type: text/plain, Size: 2923 bytes --]\n\nHi Erik,\n\n>  Most nodes could operate on a rolling history validated by occasional, \nhigh-value commitments, while archival nodes remain free to preserve the \nfull chain.\n\nThis is an old big-blocker idea, and still a terrible one. It effectively \nreduces what we now call validation to majority hash power control. IOW \nfunctionally equivalent to SPV. A few actual full nodes (maybe) validating \ndoes not have the implied effect. For a node's validation to matter, the \nnode has to be accepting coin in trade. SPV entirely relies on the \npresumption that a very large portion of economic activity is actually \nvalidated. Very large means enough that majority hash power has a true \ndisincentive to intentionally mine invalid blocks, despite the reward for \ndoing so (e.g. unlimited inflation). What you are calling \"archival nodes\" \ndon't actually \"preserve the full chain\" for everyone else, because their \neffect is limited to their own transactions. Otherwise we are talking about \nfraud proofs, which is a conversation that doesn't end well.\n\n>  Because computing the full UTXO root is costly...\n\nIt is not, it's getting cheaper every year.\n\ne\n\nOn Monday, September 29, 2025 at 8:11:51 PM UTC-4 Erik Aronesty wrote:\n\nA soft fork could introduce a new opcode, `OP_CHECKUTXOSETHASH`, allowing \nminers to optionally commit a deterministic hash of the current UTXO set \ninto a block. If present, all nodes must verify its correctness or reject \nthe block; if absent, the block is still valid. Old nodes treat the opcode \nas unspendable, so backward compatibility is preserved. \n\nBecause computing the full UTXO root is costly, this makes each checkpoint \nintentionally expensive to produce, ensuring that miners will only include \nthem when compensated with sufficient fees. Additionally, it could be \nlimited to one per block.\n\nThe result is a voluntary, self-limiting, incentive-aligned, fee-driven \nsystem where checkpoints are cheaply ",
          "drama_signals": {
            "drama_keywords": 2,
            "positive_keywords": 1,
            "text_length": 2042,
            "has_nack": false,
            "has_ack": true
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 2
      }
    },
    {
      "title": "Re: [bitcoindev] [BIP Proposal] Reduced Data Temporary Softfork",
      "message_count": 1,
      "participants": [
        "\"/dev /fd0\""
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/CALiT-ZrT+R7ApsQJOtKM5h5xTThGL-WMyCRDwt29sXmt4AA+Rg@mail.gmail.com",
          "title": "Re: [bitcoindev] [BIP Proposal] Reduced Data Temporary Softfork",
          "author": "\"/dev /fd0\" <alicexbtong@gmail@com>",
          "date": "Tue, 28 Oct 2025 14:46:26 +0530",
          "body": "[-- Attachment #1: Type: text/plain, Size: 9718 bytes --]\n\nHi Greg,\n\n> There are also proposals such as BIP 337:\nhttps://github.com/bitcoin/bips/blob/master/bip-0337.mediawiki  none of\nthese things are consensus changes-- many aren't even bippable because they\ndon't have interoperability considerations (e.g. representations on\ndisk/memory).\n\n[Compression][0] of raw transactions is interesting although it won't be\nhelpful in this case. I had reviewed the pull request that implemented BIP\n337 in bitcoin core and was closed. Maybe we can add it in knots.\n\n> Forget for a moment the (un)likelyhood that the concerns being discussed\nare meaningfully modulated by exactly how data is represented in p2p,\nmemory, rpc, disk, etc.. for assumption just assume they are.\n\n> If so, the correct move would be to change those encodings rather than\nany consensus rule change--- particularly because any consensus rule method\nwill simply be evaded, and can't provide the level of swizzling that\nchanging the encoding can accomplish.  Encoding changes are also\nsubstantially non-coercive: people who think they're valuable can adopt\nthem and leave other people alone.\n\nI am not sure if encoding or encryption would be the right approach but\nthis is worth trying. I have opened an [issue][1] in knots repository to\ndiscuss these ideas and also created a web [page][2] that shows the binary\nfor OP_RETURN in a transaction.\n\n[0[: https://github.com/bitcoin/bitcoin/pull/29134\n[1]: https://github.com/bitcoinknots/bitcoin/issues/229\n[2]: https://opreturn01.github.io/\n\n/dev/fd0\nfloppy disk guy\n\nOn Tue, Oct 28, 2025 at 1:28 AM Greg Maxwell <gmaxwell@gmail•com> wrote:\n\n> The only consensus normative data encoding in Bitcoin is the order data\n> goes into hashes.  The representations in memory, rpc, in the p2p network,\n> etc. are already different and could be made arbitrarily different without\n> any consensus change.  Case in point:  the data is now normally encrypted\n> on disk and in P2P.  There are also prop",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 2064,
            "has_nack": false,
            "has_ack": false
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 0
      }
    },
    {
      "title": "[bitcoindev] BIP54 implementation and test vectors",
      "message_count": 1,
      "participants": [
        "\"'Antoine Poinsot' via Bitcoin Development Mailing List\""
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/V0qeILOW1CuH3NS2O8IUdQBK8i3o8LwzLNGf7xh1UO0S_Gzui1CpdP5NhdT3EtrW6NgqxJ538egeag6bVZoBX8C8E46ZYTCyPg1qBxkwCXs=@protonmail.com",
          "title": "[bitcoindev] BIP54 implementation and test vectors",
          "author": "\"'Antoine Poinsot' via Bitcoin Development Mailing List\" <bitcoindev@googlegroups.com>",
          "date": "Tue, 21 Oct 2025 15:46:04 +0000",
          "body": "Hi everyone,\n\nI'd like to give an update on my Consensus Cleanup work, now BIP54.\n\nI opened an implementation against Bitcoin Inquisition v29.1 at [0]. It contains extensive testing\nof each of the four proposed mitigations, and was used as a basis to generate test vectors for\nBIP54. I opened a PR against the BIPs repository to add them to BIP54 [1].\n\nThe test vectors for the transaction-level sigops limit contain a wide variety of usage combinations\nas well as ways of running into the limit. They also include some historical violations as well as\npathological transactions demonstrating the implementation details of the sigop accounting logic\n(which was itself borrowed from that of BIP16, which all Bitcoin implementations presumably already\nhave).\n\nThe test vectors for the new witness-stripped transaction size restriction similarly exercise the\nbounds of the check under various conditions (e.g. transactions with/without a witness). All\nhistorical violations were also added to the test vectors, thanks to Chris Stewart for digging those\nup.\n\nBecause the new timestamp restrictions are tailor-made to the mainnet difficulty adjustment\nparameters, the test vectors for those contain a number of chains of mainnet headers (from genesis).\nEach test case contains a full header chain and whether it is valid according to BIP54. These chains\nwere generated using a custom miner available in [2] and added to the implementation as a JSON data\nfile.\n\nThe test vectors for the coinbase restriction similarly include a chain of mainnet blocks, because\nthe timelock check is context-dependent. These were generated using a similar miner also available\nat [2].\n\nI'm seeking feedback on these test vectors from everybody but in particular developers of\nalternative Bitcoin clients, as compatibility with other Bitcoin implementations than Bitcoin Core\nwas a design goal.\n\nBest,\nAntoine Poinsot\n\n[0]: https://github.com/bitcoin-inquisition/bitcoin/pull/99\n[1]: https://github.com/bitcoin/bips/pull/201",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 1,
            "text_length": 2051,
            "has_nack": false,
            "has_ack": true
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 0
      }
    },
    {
      "title": "Re: [bitcoindev] Re: BIP54 implementation and test vectors",
      "message_count": 1,
      "participants": [
        "Antoine Riard"
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/5fe13896-913c-4b16-8507-69809b369612n@googlegroups.com",
          "title": "Re: [bitcoindev] Re: BIP54 implementation and test vectors",
          "author": "Antoine Riard <antoine.riard@gmail@com>",
          "date": "Sun, 9 Nov 2025 17:40:58 -0800 (PST)",
          "body": "[-- Attachment #1.1: Type: text/plain, Size: 11336 bytes --]\n\nHi Poinsot,\n\nThanks for the precision. Yes my wonder is more if you put yourself in the \nshoes of an attacker,\nand you have to calculate your cost for an attack, what is the most \ninteresting between playing on\nthe number of prevout lookups and maximizing the quadratic hashing. I do \nbelieve the proposed 2500\nsigops limit is slashing the quadratic hashing worst-case concern, while at \nthe same time not providing\nan advantage to the attacker on the prevout lookup cost. Say differently, I \nbelieve we should ensure that\nany introduced DoS limit in the goal to reduce worst-case for a DoS vector \nA do not downgrade the worst-case\nfor another DoS vector B. \n\nPreviously, as the way the novel limit was proposed in abstracto, I had a \nconcern with given that\nif you take for example bitcoin core multiple input checks where made \n(first all scripts flags and\nthen for consensus mandatory script flags) [0], a DoS attacker could have \ndeliberately make the\nscript failed on a policy flag and then make it hard fails on the novel \n2500 limit, _at a cheaper\nprice_ (less CHECKMULTISIG bytes to pack in the tx). I don't think it's a \nconcern anymore as after\n[1] and others, there is no double validation anymore and \n`CheckSigOpsBIP54` has been implemented\nwith the other policy check limits.\n\nOf course the number of CHECKMULTISIG bytes to pack is only a concern for \nan attacker in the situation\nwhere satoshis have to be provided to pass the `min_relay_feerate` policy \nrule, but it's a realistic\nlimit one has to reason when you're considering the cost of network-wide \nDoS. Somehow, you're maximizing\nthe higher DoS cost per byte per satoshi you might have to commit in a \nsingle tx.\n\nDisagree with you on the prevout lookup cost exploitation, as I think there \nis at least variant to\nattempt to slash the cost for an attacker for some categories of DoS. But \nyes seen the calculations\nfor various DoS blocks, and that can be discussed",
          "drama_signals": {
            "drama_keywords": 2,
            "positive_keywords": 2,
            "text_length": 2059,
            "has_nack": false,
            "has_ack": true
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 2
      }
    },
    {
      "title": "Re: [bitcoindev] LNHANCE a soft-fork package",
      "message_count": 1,
      "participants": [
        "Brandon Black"
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/aQzcGITQE3ckHB7K@console",
          "title": "Re: [bitcoindev] LNHANCE a soft-fork package",
          "author": "Brandon Black <freedom@reardencode@com>",
          "date": "Thu, 6 Nov 2025 09:34:16 -0800",
          "body": "As the original proposer of LNHANCE, let it be known that I still think\nit's a great direction for bitcoin script improvements. This set of\nopcodes is carefully crafted to offer significant utility with minimal\nfootguns or even sharp corners.\n\nI am also a fan of the competing TEMPLATEHASH+CSFS+IKEY proposal which\nremoves the blunted corner of modifying legacy script while adding the\nblunted corner of committing to the annex. It also removes the hacky\nsibling commitment via scriptSig, loses the ability to use simple vaults\nwith legacy ECDSA-only signing infrastructure, and saves just under 8vB\nin a typical lightning symmetry uncontested force close scenario.\n\nBoth are excellent directions to move bitcoin script and I hope to see\none of the two enforced on the network in the (not too distant) future.\n\nBest,\n\n--Brandon\n\n-- \nYou received this message because you are subscribed to the Google Groups \"Bitcoin Development Mailing List\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to bitcoindev+unsubscribe@googlegroups•com.\nTo view this discussion visit https://groups.google.com/d/msgid/bitcoindev/aQzcGITQE3ckHB7K%40console.",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 2,
            "text_length": 1220,
            "has_nack": false,
            "has_ack": true
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 0
      }
    },
    {
      "title": "Re: [bitcoindev] [Pre-BIP Discussion] Bitcoin Node Repository Consensus-Policy Separation",
      "message_count": 1,
      "participants": [
        "Juan Aleman"
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/73a08ea3-b9be-424b-a1cb-beac3206723cn@googlegroups.com",
          "title": "Re: [bitcoindev] [Pre-BIP Discussion] Bitcoin Node Repository Consensus-Policy Separation",
          "author": "Juan Aleman <bitcoindev@juanaleman@com>",
          "date": "Fri, 31 Oct 2025 11:51:18 -0700 (PDT)",
          "body": "[-- Attachment #1.1: Type: text/plain, Size: 7260 bytes --]\n\nHello Matt, thanks for your response.\n\nI searched about libbitcoinkernel and it does look like some effort is \nbeing put into the creation of this library.\n\nBut again, my focus is SPECIFICALLY on the powerful influence of the \nbitcoin/bitcoin repo itself. If you don't this merits a BIP, what would be \nthe appropriate avenue to address and potentially do something about \nreorganizing the repo itself?\n\nOn Friday, October 31, 2025 at 2:41:30 PM UTC-4 Matt Corallo wrote:\n\n> You should probably dig into the libbitcoinkernel project (and the immense \n> amount of work that has gone into it, as well as the immense amount of work \n> that it requires). Also this is not anything that would merit a BIP.\n>\n> On Oct 31, 2025, at 14:20, Juan Aleman <bitco...@juanaleman•com> wrote:\n>\n> ﻿Hello bitcoin developers,\n>\n>\n>\n> My name is Juan Alemán, and this is my first post to the mailing list. But \n> I've been involved with Bitcoin since 2017. First only as a hard money \n> investor, but later also as a developer, specially fascinated by this \n> permanent medium. I hope this proposal can be appreciated by all \n> perspectives as a pragmatic (maybe unorthodox, but timely) solution to move \n> forward in agreement.\n>\n> The changes in v30 defaults got my attention (similar to many of you), as \n> they are completely opposite to what has historically been \"standard\" \n> practice. A highly controversial change that surfaces the influence over \n> default policy in the network, escalating to the point of a fork proposal \n> <https://github.com/bitcoin/bips/pull/2017>.\n>\n> First, it must be reminded that a fork should be unnecessary if defaults \n> are simply reverted <https://github.com/bitcoin/bitcoin/pull/33682>, \n> while still allowing all policy possibilities.\n>\n> After my second PR <https://github.com/bitcoin/bitcoin/pull/33690> \n> attempt was (also) closed (and I was blocked from the repo), I realized \n> that the main issue here is s",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 1,
            "text_length": 2090,
            "has_nack": false,
            "has_ack": false
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 0
      }
    },
    {
      "title": "Re: [bitcoindev] On (in)ability to embed data into Schnorr",
      "message_count": 1,
      "participants": [
        "waxwing/ AdamISZ"
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/31d18bd9-62e0-4035-b04f-f70ff4253257n@googlegroups.com",
          "title": "Re: [bitcoindev] On (in)ability to embed data into Schnorr",
          "author": "waxwing/ AdamISZ <ekaggata@gmail@com>",
          "date": "Sun, 2 Nov 2025 05:30:44 -0800 (PST)",
          "body": "[-- Attachment #1.1: Type: text/plain, Size: 4249 bytes --]\n\n> I already told you, when I said \"known plaintext attack\". If you want to \nput random data into private keys or signatures, then things are hard to \nbreak. However, if it is something useful for the reader, then usually, \nthat kind of data are non-random. For example: some users store \ntransactions inside OP_RETURNs, and they use ASCII hex representation. If \nthey would use binary encoding, then they would save 50% space. But people \nsimply don't care.\n\n> And the similar case is possible here: if you want to store random data, \nthen it is hard to use this method. However, if you want to store ASCII \ntext, where many words can be found in a dictionary, or where the format of \nthe data is known upfront, or can be easily guessed, then the security of \nthe keys, is comparable to the brainwallets.\n\n> Which means, that you can just put your data into the private key of the \nuser, and a \"signature nonce\" (which is nothing else, but yet another \nprivate key, placed on secp256k1). And then, if you know, that your data, \nis for example \"ASCII string\", then it means, that each and every key, that \nyou produce, simply leaks at least 32 bits per 256-bit key, if not more.\n\nAh, right; I had originally written a response to this idea but then \ndiscarded it on the basis that it's kinda \"obvious\" that we shouldn't think \nabout that, and focused on the more in-the-weeds concept of a lattice \nattack instead.\n\nBut it isn't obvious.\n\nSo let's think of the spectrum here. First, the most trivial nonce to \nbreak: one consisting of a single bit (OK technically you can't encode k=0, \nheh, but, whatever, put it in the second bit of the string). Obviously that \nis extractable, getting 32 bytes plus one bit. That one extra bit above the \n33% is achievable because of \"grinding\" except here grinding is the most \ntrivial version possible: trying 2 alternatives. This still fits my \noriginal claim, which is \"33% plus whatever you can get f",
          "drama_signals": {
            "drama_keywords": 1,
            "positive_keywords": 1,
            "text_length": 2059,
            "has_nack": false,
            "has_ack": true
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 1
      }
    },
    {
      "title": "[bitcoindev] segOP potential BIP discussion",
      "message_count": 1,
      "participants": [
        "defenwycke"
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/c38d00f7-a42b-4f7b-899e-11e823a43d7dn@googlegroups.com",
          "title": "[bitcoindev] segOP potential BIP discussion",
          "author": "defenwycke <cal.defenwycke@gmail@com>",
          "date": "Wed, 29 Oct 2025 16:40:50 -0700 (PDT)",
          "body": "[-- Attachment #1.1: Type: text/plain, Size: 13060 bytes --]\n\nHi all,\n\nI attempted to propose a BIP earlier today. I was notified of my incorrect \nactions versus core procedures.\n\nSee attached or below - my proposal for discussion.\n\nRegards\n\nDefenwycke.\n\n---\n\nA proposed discussion of segOP\nAuthor: Defenwycke / defenwycke@icloud•com\n29.10.2025\n\nsegOP (Segregated OP_RETURN)\n\n    A proposed extension to Bitcoin transactions. It introduces a \ndedicated, structured, full-fee data lane for on-chain data, without \ndisrupting existing transaction rules. Think of it like this - segOP is to \narbitrary data what SegWit was to signatures — a clean, isolated, \nforward-compatible path that preserves old-node harmony while restoring fee \nfairness.\n\nAbstract:\n\n    segOP defines a new segregated data section for Bitcoin transactions, \ncryptographically committed via OP_SUCCESS184. It standardizes on-chain \nmetadata storage by enforcing full fees, structured TLV encoding, and a 100 \nKB cap, while remaining backward-compatible with legacy nodes.\n\nIt is not:\n\n    - A replacement for OP_RETURN\n    - An off-chain mechanism\n    - A hard fork\n\nIt is:\n\n    - A soft-fork-safe, future-proof, SegWit-style data section\n    - Full-fee (4 weight units per byte)\n    - Structured (TLV + Merkle-root verified)\n    - Limited to 100 KB per transaction\n\nWhat issues could segOP rectify?:\n\n    1. Ordinals abuse witness discount. segOP will apply full fee rate for \nlarge data.\n    2. No structured metadata lane. segOP introduces TLV-encoded + \nMerkle-verified section.\n    3. Witness discount abused for megabytes. segOP Enforces 100 KB cap.\n    4. OP_RETURN unstructured & limited. segOP = structured + verifiable.\n    5. Spam cheap storage. segOP deters spam with fees.\n\n    In short - segOP restores fairness — data pays its real weight cost, \npreserving block space for financial use.\n\nWhere segOP lives:\n\n    Transaction Layout (Post-segOP)\n    Transaction\n    ├── nVersion\n    ├── vin (inputs)\n    ├── vout (o",
          "drama_signals": {
            "drama_keywords": 2,
            "positive_keywords": 1,
            "text_length": 2044,
            "has_nack": false,
            "has_ack": true
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 2
      }
    },
    {
      "title": "Re: [bitcoindev] Re: segOP potential BIP discussion",
      "message_count": 1,
      "participants": [
        "\"'moonsettler' via Bitcoin Development Mailing List\""
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/3S1IMGSH6z-3Ho81Ugp9o-ltTwpIC-4ow6bn6aEAtK4XrkG5HTkvTw0BeZFpPILfabdp7rz_LDHEBWX_XZk0a7nKR4sJRUp_3B7pAMaJ86I=@protonmail.com",
          "title": "Re: [bitcoindev] Re: segOP potential BIP discussion",
          "author": "\"'moonsettler' via Bitcoin Development Mailing List\" <bitcoindev@googlegroups.com>",
          "date": "Sat, 01 Nov 2025 12:00:26 +0000",
          "body": "Hi Defenwycke,\n\nI think this is not a horrible idea, there might be future demand for a pruneable proof of publication space.\n\nBut, your proposal does not provide an incentive for anyone to adopt it. If it was cheaper than witness space,\nI think it would be a serious consideration especially for rollups.\nThe idea that the bytes incur full cost (4WU) makes it on arrival economically speaking.\n\nAlso it's a bit unclear how consensus and nodes in sync would interact with the \"recent window\".\n\nIt's a reasonable approach to mandate the presence of such an extension block near the chain-tip, but\nnodes by default should not download or verify it during IBD. This would only add a constant burden to nodes,\nwhile allowing bitcoin to scale on higher layers more that require such proof of publication mechanism for\ntheir security.\n\nThinking that this would be used by graffiti type payloads that especially are seeking the permanence, persistence\nand replication of bitcoin transactional data, or that metaprotocols that want to use the block space as an archival\nlayer for their own token ledgers is I'm afraid completely misguided or even delusional.\n\nBR,\nmoonsettler\n\n-- \nYou received this message because you are subscribed to the Google Groups \"Bitcoin Development Mailing List\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to bitcoindev+unsubscribe@googlegroups•com.\nTo view this discussion visit https://groups.google.com/d/msgid/bitcoindev/3S1IMGSH6z-3Ho81Ugp9o-ltTwpIC-4ow6bn6aEAtK4XrkG5HTkvTw0BeZFpPILfabdp7rz_LDHEBWX_XZk0a7nKR4sJRUp_3B7pAMaJ86I%3D%40protonmail.com.",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 1668,
            "has_nack": false,
            "has_ack": false
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 0
      }
    },
    {
      "title": "Re: [bitcoindev] [BIP Proposal] Soft Fork Compromise on op_return to Resolve Current Bitcoin Controversies",
      "message_count": 1,
      "participants": [
        "Melvin Carvalho"
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/CAKaEYhKdhJ1vP5BBYNAMF7557M9dSoQx8_uqjdAzRM3DgA6ajg@mail.gmail.com",
          "title": "Re: [bitcoindev] [BIP Proposal] Soft Fork Compromise on op_return to Resolve Current Bitcoin Controversies",
          "author": "Melvin Carvalho <melvincarvalho@gmail@com>",
          "date": "Fri, 31 Oct 2025 07:31:03 +0100",
          "body": "[-- Attachment #1: Type: text/plain, Size: 13893 bytes --]\n\nčt 30. 10. 2025 v 20:35 odesílatel Martin Habovštiak <\nmartin.habovstiak@gmail•com> napsal:\n\n> \"Honest\" only refers to miners not trying to reverse the transactions by\n> making an alternative chain. It has nothing to do with your subjective\n> evaluation of the transactions worth trying to censor the \"bad ones\".\n>\n\nHi Martin,\n\nSmall clarification per the white paper: \"honest\" isn’t only about\nreversing transactions. Section 8 of the white paper also discusses honest\nnodes and which transactions are accepted into blocks.\n\nSatoshi further clarified standardness:\n\"The design supports a broad range of transaction types that are possible,\nbut not all are standard. Standard transactions are the ones that are\ndesigned to be used for the common case.\"\n-- June 2010\n\nPer Section 11, the white paper's security model shows honest miners (e.g.\nfollowing standard operation) outpacing alternatives. In practice, miners\nfollowing a standardness agreement or soft fork for OP_RETURN would, on\naffected blocks, earn around $100,000 more, than under a mixed policy,\nmaking prioritizing standard transactions the economically optimal strategy.\n\nThis is because there are a finite number of blocks before each halving, so\nthe opportunity cost of non-standard payloads is half the subsidy, which is\nmore than enough economic upside to offset fees. A standardness agreement\nsoft fork is therefore economically compelling for miners: it aligns with\nlong-standing practice and provides long-term incentives.\n\nBest,\nMelvin\n\n\n>\n> Bitcoin was specifically designed to prevent censorship of transactions\n> that follow the consensus rules. Trying to go against it makes you a fool\n> at best or an attacker at worst.\n>\n> Dňa št 30. 10. 2025, 18:37 Melvin Carvalho <melvincarvalho@gmail•com>\n> napísal(a):\n>\n>>\n>>\n>> čt 30. 10. 2025 v 3:16 odesílatel Greg Maxwell <gmaxwell@gmail•com>\n>> napsal:\n>>\n>>> I searched for the source of your quotation and am unable",
          "drama_signals": {
            "drama_keywords": 3,
            "positive_keywords": 2,
            "text_length": 2107,
            "has_nack": false,
            "has_ack": true
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 3
      }
    },
    {
      "title": "[bitcoindev] [BIP Proposal] Limit ScriptPubkey Size >= 520 Bytes Consensus.",
      "message_count": 1,
      "participants": [
        "PortlandHODL"
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/6f6b570f-7f9d-40c0-a771-378eb2c0c701n@googlegroups.com",
          "title": "[bitcoindev] [BIP Proposal] Limit ScriptPubkey Size >= 520 Bytes Consensus.",
          "author": "PortlandHODL <admin@qrsnap@io>",
          "date": "Thu, 2 Oct 2025 13:42:06 -0700 (PDT)",
          "body": "[-- Attachment #1.1: Type: text/plain, Size: 1881 bytes --]\n\nProposing: Softfork to after (n) block height; the creation of outpoints \nwith greater than 520 bytes in the ScriptPubkey would be consensus invalid. \n\nThis is my gathering of information per BIP 0002\n\nAfter doing some research into the number of outpoints that would have \nviolated the proposed rule there are exactly 169 outpoints. With only 8 \nbeing non OP_RETURN. I think after 15 years and not having discovered use \nfor 'large' ScriptPubkeys; the reward for not invalidating them at the \nconsensus level is lower than the risk of their abuse. \n\n   - \n*Reasons for *\n      - Makes DoS blocks likely impossible to create that would have any \n      sufficient negative impact on the network.\n      - Leaves enough room for hooks long term\n      - Would substantially reduce the divergence between consensus  and \n      relay policy\n      - Incredibly little use onchain as evidenced above.\n      - Could possibly reduce codebase complexity. Legacy Script is largely \n      considered a mess though this isn't a complete disablement it should reduce \n      the total surface that is problematic.\n      - Would make it harder to use the ScriptPubkey as a 'large' \n      datacarrier.\n      - Possible UTXO set size bloat reduction.\n      \n      - *Reasons Against *\n      - Bitcoin could need it in the future? Quantum?\n      - Users could just create more outpoints.\n   \nThoughts?\n\nsource of onchain data  \n<https://github.com/portlandhodl/portlandhodl/blob/main/greater_520_pubkeys.csv>\n\nPortlandHODL\n\n-- \nYou received this message because you are subscribed to the Google Groups \"Bitcoin Development Mailing List\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to bitcoindev+unsubscribe@googlegroups•com.\nTo view this discussion visit https://groups.google.com/d/msgid/bitcoindev/6f6b570f-7f9d-40c0-a771-378eb2c0c701n%40googlegroups.com.\n\n[-- Attachment #1.2: Type: text/html, Size: 2232 bytes --",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 2076,
            "has_nack": false,
            "has_ack": false
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 0
      }
    },
    {
      "title": "Re: [bitcoindev] Re: [BIP Proposal] Limit ScriptPubkey Size >= 520 Bytes Consensus.",
      "message_count": 1,
      "participants": [
        "Doctor Buzz"
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/73793fc0-29d8-4b79-b7bf-048e459c928bn@googlegroups.com",
          "title": "Re: [bitcoindev] Re: [BIP Proposal] Limit ScriptPubkey Size >= 520 Bytes Consensus.",
          "author": "Doctor Buzz <buzzheavy@gmail@com>",
          "date": "Thu, 30 Oct 2025 15:15:06 -0700 (PDT)",
          "body": "[-- Attachment #1.1: Type: text/plain, Size: 27313 bytes --]\n\n> We should reflect on the goal of minimizing UTXO set size.  Would we as \neasily say we should minimize the number of people/entities who hold L1 \ncoins, or the number of ways each person/entity can hold them?\n> The dire concern with UTXO set size was born with the optimization of the \ncore bitcoin software for mining, rather than for holding and transfers, in \n2012.  Some geniuses were involved with that change.  Satoshi was not one \nof them.\n\nThe goal isn’t to minimize UTXOs themselves or discourage people from \nself-custodying coins on L1. We should minimize *non-monetary UTXOs* (those \ncreated only for file storage, dusting-type tracking, etc.), not those used \nfor transferring or holding value, as intended.\n\nMinimizing *unnecessary resource usage* that doesn’t serve the \n“peer-to-peer money” purpose helps everyone that has that purpose as a goal:\n– Everyday users who want to store their savings securely.\n– Active users who want to spend and settle efficiently.\n– Node runners who keep the network decentralized and verifiable.\n\nBitcoin should take all steps to avoid unnecessary bloat (which has led to \nan extra ~30 GB/yr of immutable data storage since Feb 2023 and completely \nsidetracked any REAL improvements / security upgrades that should've been \nleading discussions).  Then perhaps in just a few years, every new phone \ncould comfortably run its own fully-verifying node with no trusted servers \nor light-clients... enabling efficient, permissionless monetary use on both \nL1 or L2.  Self-hosted LN nodes cannot compete with those who want file \nstorage that lasts longer than the user itself.\n\nSince you brought it up... I have a proposal to limit \"bulk dust\" (defined \nas a Tx with >= ~20 \"Tiny\" outputs &&  >= ~70% of the outputs are \"Tiny\", \nwhich starts at 4096 sats but halves every epoch, beginning at block \n1,260,000).\n\nIt should be difficult to use Bitcoin for data storage and sending tiny Txs \nfor",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 1,
            "text_length": 2084,
            "has_nack": false,
            "has_ack": true
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 0
      }
    },
    {
      "title": "[bitcoindev] Policy restrictions Was: [BIP Proposal] Limit ScriptPubkey Size >= 520 Bytes Consensus.",
      "message_count": 1,
      "participants": [
        "\"'Russell O'Connor' via Bitcoin Development Mailing List\""
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/CAMZUoKnyYpfJ1fZRan7BzyGvMitxznoSyCXkjxc2Qy5Z9pNvMA@mail.gmail.com",
          "title": "[bitcoindev] Policy restrictions Was: [BIP Proposal] Limit ScriptPubkey Size >= 520 Bytes Consensus.",
          "author": "\"'Russell O'Connor' via Bitcoin Development Mailing List\" <bitcoindev@googlegroups.com>",
          "date": "Thu, 30 Oct 2025 16:27:34 -0400",
          "body": "[-- Attachment #1: Type: text/plain, Size: 1423 bytes --]\n\nOn Thu, Oct 30, 2025 at 2:40 AM Greg Maxwell <gmaxwell@gmail•com> wrote:\n\n> I don't even think bitcoin has ever policy restricted something that was\n> in active use, much less softforked out something like that.\n>\n\nI invite the Bitcore lore experts to correct me here, but I recall someone\nmany years ago finding that their bare multisig funds (likely related to\nthe Counterparty nonsense) were stuck by policy due to some new policy\nbeing enacted to mandate that pubkeys in bare multisigs must now all be\non-curve points ... or something like that.  I do hope that they managed to\nget their funds recovered by now with direct miner intervention.\n\nI really ought to vet my claim above by going through my IRC logs and\nBitcoin development history ... but a quicker way is to post a claim\npublicly on the internet and wait for someone else to call it out as being\nwrong.\n\nAlso, I think this type of policy change quite harmful and shouldn't be\nreplicated, and ideally reverted, assuming my story is correct.\n\n-- \nYou received this message because you are subscribed to the Google Groups \"Bitcoin Development Mailing List\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to bitcoindev+unsubscribe@googlegroups•com.\nTo view this discussion visit https://groups.google.com/d/msgid/bitcoindev/CAMZUoKnyYpfJ1fZRan7BzyGvMitxznoSyCXkjxc2Qy5Z9pNvMA%40mail.gmail.com.\n\n[-- Attachment #2: Type: text/html, Size: 2077 bytes --]",
          "drama_signals": {
            "drama_keywords": 1,
            "positive_keywords": 0,
            "text_length": 1613,
            "has_nack": false,
            "has_ack": false
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 1
      }
    },
    {
      "title": "[bitcoindev] Re: Policy restrictions Was: [BIP Proposal] Limit ScriptPubkey Size >= 520 Bytes Consensus.",
      "message_count": 1,
      "participants": [
        "\"'Russell O'Connor' via Bitcoin Development Mailing List\""
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/CAMZUoKmPfbwJApAeYkXs6U9Syuj4KjbcsH3aFJ7desFnHxyyTw@mail.gmail.com",
          "title": "[bitcoindev] Re: Policy restrictions Was: [BIP Proposal] Limit ScriptPubkey Size >= 520 Bytes Consensus.",
          "author": "\"'Russell O'Connor' via Bitcoin Development Mailing List\" <bitcoindev@googlegroups.com>",
          "date": "Thu, 30 Oct 2025 18:23:20 -0400",
          "body": "[-- Attachment #1: Type: text/plain, Size: 2548 bytes --]\n\nFine, I ended up looking into it.\n\nPR 5247 <http://github.com/bitcoin/bitcoin/pull/5247> changed the semantics\nof STRICTENC policy in late 2014 so that during a CHECKMULTISIGVERIFY, if a\npubkey with an incorrect prefix is encountered, the script fails.  The\nprevious behaviour was that if a pubkey was invalid, the check failed for\nonly that pubkey and processing continued on to the next pubkey.\n\nI still have to go through my IRC logs, but my recollection is there is at\nleast one person who had their funds \"soft confiscated\" in the sense that\nthey were now, by policy only, unable to spend their UTXOs and would\nrequire bypassing policy to retrieve their funds.\n\nPeople who have better databases than me are welcome to search through bare\nmultisig UTXOs to see if there are any having a strict subset of malformed\npubkeys in them.\n\nSo one minor correction to my story: it wasn't a matter of the pubkey being\noff-curve, but rather having an invalid prefix / invalid encoding.\n\nOn Thu, Oct 30, 2025 at 4:27 PM Russell O'Connor <roconnor@blockstream•com>\nwrote:\n\n> On Thu, Oct 30, 2025 at 2:40 AM Greg Maxwell <gmaxwell@gmail•com> wrote:\n>\n>> I don't even think bitcoin has ever policy restricted something that was\n>> in active use, much less softforked out something like that.\n>>\n>\n> I invite the Bitcore lore experts to correct me here, but I recall someone\n> many years ago finding that their bare multisig funds (likely related to\n> the Counterparty nonsense) were stuck by policy due to some new policy\n> being enacted to mandate that pubkeys in bare multisigs must now all be\n> on-curve points ... or something like that.  I do hope that they managed to\n> get their funds recovered by now with direct miner intervention.\n>\n> I really ought to vet my claim above by going through my IRC logs and\n> Bitcoin development history ... but a quicker way is to post a claim\n> publicly on the internet and wait for someone else to call it ou",
          "drama_signals": {
            "drama_keywords": 1,
            "positive_keywords": 0,
            "text_length": 2105,
            "has_nack": false,
            "has_ack": false
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 1
      }
    },
    {
      "title": "[bitcoindev] By: Doctorbuzz1 {GitHub} Limit \"Bulk Dust\" with a default filter or consensus.",
      "message_count": 1,
      "participants": [
        "Frenchanfry"
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/b0dee827-c2fe-4a68-9b41-f1447ba3c1d3n@googlegroups.com",
          "title": "[bitcoindev] By: Doctorbuzz1 {GitHub} Limit \"Bulk Dust\" with a default filter or consensus.",
          "author": "Frenchanfry <andujon18@gmail@com>",
          "date": "Wed, 29 Oct 2025 17:03:45 -0700 (PDT)",
          "body": "[-- Attachment #1.1: Type: text/plain, Size: 10004 bytes --]\n\nA proposal on GitHub I found Highly interesting and a better improvement, \ndealing with spammers/congestion.\n\nI’m exploring a potential default filter or consensus-level rule (since a \nlarge number of people believe that default filters don't work) to \ndiscourage UTXO-bloat patterns without touching Script, witness data, or \nthe block size limit.\n\nThe idea is to target “bulk dust” transactions — those that create large \nnumbers of extremely small outputs — which are the main cause of long-term \nUTXO set growth.\n\nThese types of \"bulk dust\" transactions have been the No. 1 reason cited \nfor wanting to expand the default OP_RETURN limit... and removing that \nlimit obviously influenced BIP 444. So it appears to me that there is \noverwhelming majority support for limiting these types of \"bulk dust\" \ntransactions, as they do present a legitimate concern for node runners.\n\nConcept\n\nFlag a transaction as “bulk dust” if:\n\n   - It has >=100 outputs each below a dynamically defined TinyTx \n   threshold, and\n   - Those tiny outputs make up >=60% of all outputs in the transaction.\n\nWhen flagged, it would be considered nonstandard (relay policy) or invalid \n(if soft-forked into consensus).\n\nTinyTx threshold (dynamic halving schedule)\n\nI originally considered a constant definition of what was a \"tiny\" Tx to be \n1,000 sats... but some might still just use 1,001 sats, right? Plus there \nvery likely will be a time where there is a valid use-case of >100 outputs \nunder 1,000 sats.\n\nRather than fixing the “tiny” threshold to a constant like 1,000 sats, the \nrule defines it as a decreasing function of block height, starting high and \ngradually tightening over time.\n\n   - Starts at 4096 sats when activated (target ~2028).\n   - Halves every 210,000 blocks (~4 years).\n   - Never falls below 1 sat (hard floor).\n\nYear ---- Block Height -- TinyTx Threshold\n2028 --- ~activation ---- 4096 sats\n2032 --- ~1,260,000 ---- 2048 sats\n2036 ",
          "drama_signals": {
            "drama_keywords": 2,
            "positive_keywords": 0,
            "text_length": 2092,
            "has_nack": false,
            "has_ack": false
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 2
      }
    },
    {
      "title": "[bitcoindev] Re: By: Doctorbuzz1 {GitHub} Limit \"Bulk Dust\" with a default filter or consensus.",
      "message_count": 1,
      "participants": [
        "Doctor Buzz"
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/15849db6-67af-4a03-86a4-bb5a288b9ad1n@googlegroups.com",
          "title": "[bitcoindev] Re: By: Doctorbuzz1 {GitHub} Limit \"Bulk Dust\" with a default filter or consensus.",
          "author": "Doctor Buzz <buzzheavy@gmail@com>",
          "date": "Thu, 30 Oct 2025 07:14:32 -0700 (PDT)",
          "body": "[-- Attachment #1.1: Type: text/plain, Size: 12555 bytes --]\n\nA preemptive response to those who might say that a conservative \n\"tiny_count\" of 100 \"wouldn't do anything\":\n\nThe point is to add friction without inhibiting any non-data Txs. The image \nof Pepe pumping iron with \"UTXO\" on top was stored in 1,859 fake pubkeys / \nUTXOs. The proposed tiny_count of 100 would split that particular image \nacross at least 19 Txs (likely a lot more if on-chain indexing were used), \nwhich only adds at least +6% to fees, but it does ruin \"atomicity\" (images \nall-in-one Tx) by adding complexity of needing some type of index to link \nthem, causes confirmation risk, & pushes data abusers toward OP_RETURN or \nwitness space.\n\nChanging the 100 tiny_count to 50 ≈ +11% fees; to 30 ≈ +16% fees; & to 20 ≈ \n+24% fees (this only takes into account an extra 200 bytes per additional \ninput Tx and does not consider any additional indexing needs) . Perhaps a \ntiny_count could be 20 with a higher ratio of 70%?? ~24% extra fees + added \ncomplexity could definitely prevent a lot of UTXO abuse.  I was obviously \njust trying to avoid ALL false positives, but there definitely seems like \nthere's room to move the tiny_count lower.\n\nOn Wednesday, October 29, 2025 at 8:15:08 PM UTC-5 Doctor Buzz wrote:\n\n> Thanks!  I came here to post it myself.  I just want to point out that \n> it's awfully discouraging for a GitHub mod to \"close\" my 90% developed \n> code, asking me to post it elsewhere... but anyway!\n>\n> Original GitHub post here:\n> https://github.com/bitcoin/bitcoin/issues/33737#issuecomment-3465288829\n>\n> The first concept of this with static definition of a \"tiny\" Tx was posted \n> here (with no responses):  \n> https://bitcoin.stackexchange.com/questions/129139/would-a-bulk-dust-relay-consensus-rule-limiting-100-sub-1-000-sat-outputs-p\n>\n> Pastebin code probably looks better here than what I can see in the OP of \n> this thread:  https://pastebin.com/9qdQCH83\n> On Wednesday, October 29, 2025 at 7:47:08",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 1,
            "text_length": 2096,
            "has_nack": false,
            "has_ack": true
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 0
      }
    },
    {
      "title": "[bitcoindev] [Concept] Anticipation Pool - Off-chain scaling using miner-validated transaction forwarding",
      "message_count": 1,
      "participants": [
        "Jakob Widmann"
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/55e36b59-76c2-4ffc-8f36-9a9a0c2fc02bn@googlegroups.com",
          "title": "[bitcoindev] [Concept] Anticipation Pool - Off-chain scaling using miner-validated transaction forwarding",
          "author": "Jakob Widmann <jweb.git@gmail@com>",
          "date": "Wed, 29 Oct 2025 03:22:41 -0700 (PDT)",
          "body": "[-- Attachment #1.1: Type: text/plain, Size: 5302 bytes --]\n\n\n\nHello everyone,\n\nI've been thinking extensively about how we could scale Bitcoin, \nparticularly looking for alternatives to Lightning's limitations. My main \nfrustrations with Lightning are the need for watchtowers, and routing \ncomplexities that arise from channel liquidity limitations. I wondered if \nwe could leverage the existing mining infrastructure instead of building \nseparate systems.\n\nMy goal was to find a scaling solution that:\n\n\n   - Eliminates watchtower requirements by using miners (who are already \n   always online)\n   - Avoids channel-based routing complexities\n   - Guarantees eventual settlement\n   - Creates natural economic incentives for all participants\n\nI'm not a Bitcoin developer, but I wanted to share this concept with the \ncommunity to see if others think it could work and how it might be \nimplemented. I'm particularly interested in feedback on the technical \nfeasibility.\n\nHere's the concept:\n\n*Anticipation Pool*\n\nAnticipation transactions (ATX) are pre-signed Bitcoin transactions that \nare forwardable to anyone without immediate blockchain settlement.\n\nHere's how it works:\n\nInstead of publishing transactions to the mempool to add them to the \nblockchain, you create a pre-signed ATX that goes into a special pool \ncalled the anticipation pool, managed by miners. Once enough miners have \nvalidated your ATX and added it to the pool (validation threshold to be \ndetermined, e.g., 10-30% of hashpower depending on amount), the recipient \ncan consider it \"confirmed\". Double-spending is prevented because miners \ntimestamp and validate each forward, with first-seen winning, and check \nagainst the anticipation pool, mempool and blockchain to ensure the outputs \nhaven't been spent elsewhere.\n\nThe ATX has a timelock (e.g. 30 days), during this time only the recipient \ncan publish it to the mempool, forward it to someone else (including to \nthemselves), or split it to multiple people. Each forwa",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 1,
            "text_length": 2106,
            "has_nack": false,
            "has_ack": true
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 0
      }
    }
  ],
  "summary": {
    "total_threads": 50,
    "total_messages": 50,
    "unique_participants": 39
  }
}