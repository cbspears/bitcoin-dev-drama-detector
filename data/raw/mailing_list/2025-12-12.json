{
  "source": "mailing_list",
  "list": "bitcoin-dev",
  "fetched_at": "2026-01-16T00:34:57.001005+00:00",
  "date": "2025-12-12",
  "threads": [
    {
      "title": "Re: [bitcoindev] Re: The Cat, BIP draft discussion.",
      "message_count": 1,
      "participants": [
        "Greg Maxwell"
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/CAAS2fgTjF_e5vLEzp672_jmF82jDUre0wcZKHB5my8kTdFOGgw@mail.gmail.com",
          "title": "Re: [bitcoindev] Re: The Cat, BIP draft discussion.",
          "author": "Greg Maxwell <gmaxwell@gmail@com>",
          "date": "Fri, 12 Dec 2025 23:40:53 +0000",
          "body": "[-- Attachment #1: Type: text/plain, Size: 921 bytes --]\n\nOn Fri, Dec 12, 2025 at 9:26 PM Jonathan Voss <k98kurz@gmail•com> wrote:\n\n> Since the Bitcoin Stamps outputs are already unspendable, it makes perfect\n> sense to mark and drop them from the UTXO set.\n\n\nThere is no consensus change involved in not storing a provably unspendable\noutput, it's just an implementation detail with no interoperability\nimplications and doesn't need a BIP.  Bitcoin core has long done so for\nseveral types of unspendable outputs, e.g. outputs over 10kb and ones\nstarting with OP_RETURN.\n\n-- \nYou received this message because you are subscribed to the Google Groups \"Bitcoin Development Mailing List\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to bitcoindev+unsubscribe@googlegroups•com.\nTo view this discussion visit https://groups.google.com/d/msgid/bitcoindev/CAAS2fgTjF_e5vLEzp672_jmF82jDUre0wcZKHB5my8kTdFOGgw%40mail.gmail.com.\n\n[-- Attachment #2: Type: text/html, Size: 1495 bytes --]",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 1070,
            "has_nack": false,
            "has_ack": false
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 0
      }
    },
    {
      "title": "Re: [bitcoindev] Draft BIP: DustSweep – policy-only UTXO dust compaction",
      "message_count": 1,
      "participants": [
        "Murch"
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/02d7368e-95d3-4185-b70f-3aa9b5df1e1d@murch.one",
          "title": "Re: [bitcoindev] Draft BIP: DustSweep – policy-only UTXO dust compaction",
          "author": "Murch <murch@murch@one>",
          "date": "Fri, 12 Dec 2025 14:49:42 -0800",
          "body": "[-- Attachment #1.1.1: Type: text/plain, Size: 5847 bytes --]\n\nHey Defenwycke,\n\n > all inputs are “dust-class” UTXOs\n\nWhat does “dust-class” mean? Are you using the Bitcoin Core dust limit \nor talking about small amounts in general? I don’t have figures off the \ntop of my head, but I would assume that there are relatively few UTXOs \nsmaller than Bitcoin Core’s dust limit.\n\n > only standard scripts (P2PKH / P2WPKH / P2TR)\n\nYou might want to clarify that you mean only P2TR KP inputs. Or would \nP2TR SP be permitted?\n\n > Nodes place these in a small, separate sub-mempool. They’re only\n > accepted when the normal mempool is <50% full, and they’re\n > automatically evicted if normal mempool usage hits 95%.\n\nIt would be a lot of work to have a separate pool for this, and I don’t \nsee a reason why they couldn’t just go in the regular mempool. If the \nmempool fills up, they’d have the lowest feerates and they’d get kicked \nout first anyway. That said, at 50% full, there are still around ~30 \nblocks worth of transactions waiting in the mempool that pay fees, …\n\n > Miners can include them up to a small weight fraction (I suggest ~5%) \nbut only after filling the block with regular fee-paying transactions.\n\n… so if they are only considered in blocks that aren’t full, the only \nones I have seen lately are miners using a minimum feerate of 1 s/vB for \ntheir block templates. Looking at some popular mempool statistic sites, \nin the past 32 months, there would have only been organically non-full \nblocks between April and August this year.\n\nI assume the intention is to only relay these transactions when there \nare blocks that aren’t full, to limit the bandwidth-wasting vector this \nfeature introduces, but overall it seems to me that it would be most \nlikely for such transactions to sit in nodes’ memory until they expire.\n\nAll that said, at the new minimum feerate of 0.1 s/vB, a 148 vB P2PKH \ninput costs 15 sats, a 68 vB P2WPKH input costs 7 sats, and a 57.5 vB \nP2TR input costs 6 sats.",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 2073,
            "has_nack": false,
            "has_ack": false
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 0
      }
    },
    {
      "title": "Re: [bitcoindev] Re: Draft BIP: DustSweep – policy-only UTXO dust compaction",
      "message_count": 1,
      "participants": [
        "Defenwycke"
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/CAOj3_X8GtJtAgcvvPZW2Ovxt31CzGtn9tssqTSZ4BeQ_bL6pxg@mail.gmail.com",
          "title": "Re: [bitcoindev] Re: Draft BIP: DustSweep – policy-only UTXO dust compaction",
          "author": "Defenwycke <cal.defenwycke@gmail@com>",
          "date": "Fri, 12 Dec 2025 20:17:28 +0000",
          "body": "[-- Attachment #1: Type: text/plain, Size: 5466 bytes --]\n\nHello Jonathan,\n\nThanks for the thoughtful feedback — that makes sense.\n\nI started with a very narrow definition mostly to make the invariant\nobvious and easy to reason about. Every DustSweep tx should monotonically\nreduce the UTXO set and never meaningfully compete with the fee market. As\nlong as that holds, I’m not particularly attached to any one parameter.\n\nI agree that requiring 100% dust inputs and exactly one output is probably\noverly strict in practice. A majority dust requirement and an output/input\nratio cap seem like reasonable ways to preserve the incentive (net UTXO\nreduction) while making it more usable for real wallets.\n\nMy main goal here is to give operators something that’s safe to run and\npredictable in behaviour — cheap, bounded, and only active when blockspace\nwould otherwise go unused. I’m happy to adjust thresholds or relax\nconstraints as long as those properties remain intact.\n\nAppreciate you taking the time to look at it.\n\nKind regards,\n\nDefenwycke\n\nOn Fri, Dec 12, 2025 at 6:16 PM Jonathan Voss <k98kurz@gmail•com> wrote:\n\n> Interesting proposal. Something like that would be helpful, but perhaps it\n> would be more useful if it was not quite so narrowly defined. For example,\n> instead of requiring all inputs be dust-class UTXO, it could require a\n> minimum of 80% dust-class inputs; instead of exactly one output, it could\n> be max_outputs = floor(n_inputs / 5) to keep a maximum output/input ratio\n> of 1/5. This could allow for better aggregation of dust outputs into\n> economically meaningful, monetary outputs than the narrower definition\n> while maintaining the incentives for meaningfully reducing UTXO set size.\n>\n> I would run this policy on my node. Hashers should ultimately be okay with\n> this policy since someone among them also has to run full nodes, and it\n> would provide an additional (albeit very small) fee source when block space\n> demand is low.\n>\n> On Thursday, December 11, 20",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 3,
            "text_length": 2077,
            "has_nack": false,
            "has_ack": true
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 0
      }
    },
    {
      "title": "Re: [bitcoindev] [Discussion] Year 2106 Timestamp Overflow - Proposal for uint64 Migration",
      "message_count": 1,
      "participants": [
        "Ethan Heilman"
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/CAEM=y+W8c=CocE-T-uKRmLPH6fzL_dr38iW2VU-3Z9OaWA9Z5A@mail.gmail.com",
          "title": "Re: [bitcoindev] [Discussion] Year 2106 Timestamp Overflow - Proposal for uint64 Migration",
          "author": "Ethan Heilman <eth3rs@gmail@com>",
          "date": "Fri, 12 Dec 2025 08:40:55 -0500",
          "body": "[-- Attachment #1: Type: text/plain, Size: 9234 bytes --]\n\nYou can soft fork in a hash function. There is no need for a hard fork to\nswitch from SHA256 to say MD6. Just commit to the new hash outputs in a\nblock. Nodes enforcing old consensus won't see them, nodes enforcing new\nconsensus will require the new hash outputs match. This tightens consensus\nso is a soft fork.\n\nOn Fri, Dec 12, 2025, 07:41 Possibly <zelatyna123@gmail•com> wrote:\n\n> By then we will have to make a hard fork to move to a new hash algo\n> anyway. When we will, we'll likely change the whole structure of the block\n> header, including likely switching to u64 timestamps.\n>\n>\n> On December 8, 2025 7:43:35 PM GMT+01:00, \"אושר חיים גליק\" <\n> osher.gluck@gmail•com> wrote:\n>\n>> Subject: [Discussion] Year 2106 Timestamp Overflow - Proposal for uint64\n>> Migration\n>>\n>> Hello Bitcoin Developers,\n>>\n>> I would like to open a discussion about a long-term but critical issue:\n>> Bitcoin's timestamp overflow in 2106.\n>>\n>> # Bitcoin Year 2106 Problem: A Call for Proactive Action\n>>\n>> ## The Problem Explained\n>>\n>> Bitcoin's block timestamp field is stored as a **32-bit unsigned integer\n>> (uint32)**, representing Unix time in seconds since January 1, 1970. This\n>> design choice creates a critical limitation:\n>>\n>> **Maximum value: 2^32 - 1 = 4,294,967,295 seconds**\n>> **Overflow date: February 7, 2106, at 06:28:15 UTC**\n>>\n>> ### Technical Impact\n>>\n>> When the timestamp overflows, several critical failures will occur:\n>>\n>> 1. **Block Validation Failure**\n>>    - Nodes will reject blocks with timestamps >= 2^32\n>>    - The blockchain will effectively halt\n>>    - No new transactions can be confirmed\n>>\n>> 2. **Difficulty Adjustment Breakdown**\n>>    - Difficulty calculation relies on accurate timestamps\n>>    - Overflow will corrupt the difficulty adjustment algorithm\n>>    - Mining becomes unpredictable or impossible\n>>\n>> 3. **Time-Locked Transactions**\n>>    - nLockTime and CheckLockTimeVerify (CLTV) will m",
          "drama_signals": {
            "drama_keywords": 3,
            "positive_keywords": 0,
            "text_length": 2091,
            "has_nack": false,
            "has_ack": false
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 3
      }
    },
    {
      "title": "Re: [bitcoindev] [BIP Proposal] Add sp() output descriptor format for BIP352",
      "message_count": 1,
      "participants": [
        "Craig Raw"
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/CAPR5oBNX5oyzVJcWH9FDMOXE7q2MaZgSHX_LtCnOu=dcW9D70w@mail.gmail.com",
          "title": "Re: [bitcoindev] [BIP Proposal] Add sp() output descriptor format for BIP352",
          "author": "Craig Raw <craigraw@gmail@com>",
          "date": "Fri, 12 Dec 2025 11:18:24 +0200",
          "body": "[-- Attachment #1: Type: text/plain, Size: 13351 bytes --]\n\nHi Novo,\n\nResponses inline:\n\n> However, without the birthday, the descriptor will still be able to\ndescribe its outputs. The birthday can be collected through some other\nmeans, as we do with other descriptors today.\n\nIndeed, that is why it is an optional argument. Again however, other\ndescriptors do not have to bear the very significant computational overhead\nthat sp() descriptors do. For many deployment contexts, this will\neffectively make the birthday a requirement to retrieve all silent payment\noutputs in a wallet within a usable time frame. Other descriptors do not\nshare such a stark usability challenge.\n\n> With existing key formats, users and wallets will be able use their\nexisting master key to generate silent payment outputs using a descriptor\nlike: sp(<master_key>/352h/1h/0h/1h/0,<master_key>/352h/1h/0h/0h/0).\n\nYou are requiring the user to specify their master xprv in an output\ndescriptor, even for a watch-only wallet. This is a non-starter. Today,\noutput descriptors are often stored in clear text alongside a hardware\nwallet or similar as privacy-sensitive but not directly security-sensitive\ninformation.\n\n> > The advantages of Bech32m encoding, including strong error detection\nand unambiguous characters\n> I'm not sure these are strong enough to warrant new key formats.\n\nTo the contrary, the use of output descriptors today means they are often\nwritten down (sometimes into durable media). In this context, the\nadvantages of strong error detection and unambiguous characters are\nsignificant.\n\n> > Safety from accidentally mixing different unrelated scan and spend keys\n> I'm not sure what the chance of this happening is.... If this is done\nproperly, then the keys should not be mixed up.\n\nIt might also be done intentionally, with unexpected results. Regardless of\nthe cause, having one key expression discourages this.\n\n> We should use the descriptor prefix to indicate the silent payments\nversion instead of ",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 2077,
            "has_nack": false,
            "has_ack": false
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 0
      }
    },
    {
      "title": "Re: [bitcoindev] Feedback on a simple 2-path vault design (2-of-2 + CLTV recovery) and use of pruned nodes for UTXO retrieval",
      "message_count": 1,
      "participants": [
        "\"'Antoine Poinsot' via Bitcoin Development Mailing List\""
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/ywe-dbEMw2fKIXvUV7gTVyZmJENCN20PSAbB-pZYGe-0Skd9OKGFxAOfOT17UUHcFOkw1j0BeNL-G5aEMe3HQlnh1pNuxpxGL4Z09oUkdTo=@protonmail.com",
          "title": "Re: [bitcoindev] Feedback on a simple 2-path vault design (2-of-2 + CLTV recovery) and use of pruned nodes for UTXO retrieval",
          "author": "\"'Antoine Poinsot' via Bitcoin Development Mailing List\" <bitcoindev@googlegroups.com>",
          "date": "Thu, 11 Dec 2025 14:14:42 +0000",
          "body": "[-- Attachment #1: Type: text/plain, Size: 3950 bytes --]\n\nHi Victor,\n\nCan we as a community stop calling everything vaults? We've seen vanilla multisig wallets being called vaults, then CTV transaction chains, and now a decaying multisig. Not trying to blame you in particular, but a vault in Bitcoin has (or used to have?) a [specific meaning](https://bitcoinops.org/en/topics/vaults/), and regardless grouping constructions with very different properties under a single label only creates confusion.\n\nWhat you present is essentially Liana: [https://github.com/wizardsardine/liana](https://github.com/wizardsardine/liana/). Except that Liana uses relative timelocks (CSV) because absolute timelocks (CLTV) put an expiration date on your descriptor, which adds lots of friction and can be quite confusing to less technical users.\n\nBy default Liana comes bundled with a pruned Bitcoin Core node and uses its watchonly wallet functionality to track your coins. For a remote node, Bitcoin Core's RPC interface is not adapted and Liana lets you configure an Electrum server instead.\n\nRegards,\nAntoine\nOn Thursday, December 11th, 2025 at 6:31 AM, victor perez <svcrobotics@gmail•com> wrote:\n\n> Hello everyone,\n>\n> I’m working on a small non-custodial vault system and would like to collect feedback on the safety and correctness of a simple script design, as well as on a question regarding pruned nodes and PSBT workflows.\n>\n> Vault design\n>\n> The vault uses two spending paths:\n>\n> -\n>\n> Normal spending path (immediate):\n> 2-of-2 multisig (key A + key B required)\n>\n> -\n>\n> Recovery path (delayed):\n> After a predefined block height (CLTV), key B alone can spend:\n>\n> <CLTV_height> OP_CHECKLOCKTIMEVERIFY OP_DROP <pubkey_B> OP_CHECKSIG\n>\n> Both paths behave as expected on regtest, including enforcement of the CLTV height.\n>\n> The goal is a simple inheritance/emergency mechanism:\n> – before the delay expires → strict 2-of-2\n> – after the delay → key B alone can recover funds\n> No custodial compone",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 1,
            "text_length": 2126,
            "has_nack": false,
            "has_ack": true
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 0
      }
    },
    {
      "title": "[bitcoindev] Reducing RAM requirements with dynamic dust",
      "message_count": 1,
      "participants": [
        "\"'uuowwpevskfcordh' via Bitcoin Development Mailing List\""
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/Q4RLVZW6OK88aVcalvUK7KJJIKOXckKhB7a5zTN7LTxA-jzal3587k4yUiIMjcIBoqLI0eK4uQLZtjJGsbj1R8zsfaMDM-RGSw2V9KI6AAw=@proton.me",
          "title": "[bitcoindev] Reducing RAM requirements with dynamic dust",
          "author": "\"'uuowwpevskfcordh' via Bitcoin Development Mailing List\" <bitcoindev@googlegroups.com>",
          "date": "Sat, 06 Dec 2025 16:08:45 +0000",
          "body": "[-- Attachment #1: Type: text/plain, Size: 10005 bytes --]\n\nGiven the increasing RAM requirements, due to the increasing UTXO set, I suggest seeing the UTXO set size as a controlled variable. A feedback mechanism sets a dynamic dust level, below from which UTXOs are removed/discarded and thus freeing RAM.\n\nBelow is an overview essay better expressed by grok, which can also be seen in here:\nhttps://hackmd.io/P-2lzGb8TiC86IOE3OGiYA?view\n\n# Enhancing Bitcoin's Scalability: A PID-Inspired Approach to Managing UTXO Set Growth\n\n## Abstract\n\nBitcoin’s UTXO set is currently an unbounded accumulator that risks long-term centralization as node RAM requirements grow without limit. Existing fee incentives have proven insufficient against sustained low-value output creation (e.g., inscriptions, tokenized assets, dust-heavy protocols). This article proposes a soft-fork mechanism that treats UTXO set size as a controlled variable: a slowly rising target size is defined, and a PID-style feedback controller, updated every difficulty epoch, dynamically raises a minimum-value floor beneath which old UTXOs become unspendable. The result is bounded, predictable growth of the UTXO set with ample warning periods, no hard caps on monetary use, and strong resistance to bloat attacks—all while remaining fully compatible with a soft-fork deployment.\n\n## Introduction\n\nBitcoin, the pioneering decentralized digital currency, operates as a complex dynamic system governed by consensus rules that ensure security, immutability, and permissionless participation. At its core, Bitcoin maintains a distributed ledger known as the blockchain, which records all transactions in a sequence of blocks. Each transaction involves inputs and outputs: inputs reference previously unspent outputs from prior transactions, while outputs create new spendable units called Unspent Transaction Outputs (UTXOs). The UTXO set represents the aggregate state of all currently spendable coins in the network, serving as a critica",
          "drama_signals": {
            "drama_keywords": 1,
            "positive_keywords": 1,
            "text_length": 2057,
            "has_nack": false,
            "has_ack": true
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 1
      }
    },
    {
      "title": "Re: [bitcoindev] Re: Reducing RAM requirements with dynamic dust",
      "message_count": 1,
      "participants": [
        "Eric Voskuil"
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/97065ca0-662b-4a5d-90c5-7607db41ba2fn@googlegroups.com",
          "title": "Re: [bitcoindev] Re: Reducing RAM requirements with dynamic dust",
          "author": "Eric Voskuil <eric@voskuil@org>",
          "date": "Wed, 10 Dec 2025 12:44:33 -0800 (PST)",
          "body": "[-- Attachment #1.1: Type: text/plain, Size: 16411 bytes --]\n\nHi Erik,\n\nIt's a good idea, that's why we did it.\n\nLibbitcoin is a set of libraries, libbitcoin-database being one of 4 that \nmake up node (system, network, database, and node).  (libbitcoin-server is \nan additional library that adds comprehensive set of client-server \ninterfaces, making the node useful.) libbitcoin-database is an \nimplementation of a simple query interface (defined as a c++ class) over a \nbacking store. The store is a templated collection of tables. The tables \nare mmap-based head and body files used to construct multimaps, hashmaps, \narraymaps, arrays, and blobs. We mock the tables using \nstd::vector<uint8_t>, mock the store using the the mocked tables, and test \nmost of the query interface using the mock store. The structure is highly \nrelational, and surrogate keys are exposed to the caller for optimized \nnavigation.\n\nThis is an isolated and clear storage abstraction layer. All validation is \nperformed within the chain:: classes (e.g. block, header, tx, input, \nscript, etc.) defined in the base level libbitcoin-system. There is no \nvalidator coupling to the store. The store retains chain objects, indexes, \nand validation state for headers/blocks and txs as they progress through a \nstate machine. There is no utxo table, just natural relations between \nobjects, indexed and related. Validation correctness of course requires \nstore fidelity, but is totally decoupled from it. We validate blocks \nconcurrently, queueing up 50,000 blocks at a time by default (e.g. with 50k \nthreads we would validate all at the same time).\n\nThe store could be replaced with no impact to the query interface (as we \nalready do in testing). So it's not really accurate to imply that \nlibbitcoin's validation is tied to mmap or even append-only. Pruning could \nbe implemented in the existing model for example. The existing store could \nbe replaced with something simple and light like SQLite, or a full RDBMS. \nWe had s",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 1,
            "text_length": 2065,
            "has_nack": false,
            "has_ack": true
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 0
      }
    },
    {
      "title": "Re: [bitcoindev] Hash-Based Signatures for Bitcoin's Post-Quantum Future",
      "message_count": 1,
      "participants": [
        "Olaoluwa Osuntokun"
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/CAO3Pvs_cetTGP54zDzJJGRPeN7gXrRYGZZYk4mRYnhJQnwotdA@mail.gmail.com",
          "title": "Re: [bitcoindev] Hash-Based Signatures for Bitcoin's Post-Quantum Future",
          "author": "Olaoluwa Osuntokun <laolu32@gmail@com>",
          "date": "Tue, 9 Dec 2025 16:41:48 -0800",
          "body": "[-- Attachment #1: Type: text/plain, Size: 13469 bytes --]\n\nHi y'all,\n\nconduition wrote:\n> I'm personally hoping that we'll find a way to derive child pubkeys using\n> lattices (ML-DSA) and/or isogenies (SQIsign), but I haven't heard of any\n> solid proposals yet.\n\nThis paper [1] proposes a variant of Dilithium (dubbed DilithiumRK, RK for\n'randomized keys' presumably) that enables BIP-32-like functionality. It\nachieves this by getting rid of a public key compression step in the OG\nalgorithm that results in a loss of homomorphic properties. There're\nalgorithmic changes required (eg: a new public network param is needed\nwhich is used for seed/key generation), so it isn't vanilla FIP 204.\n\nAside from the deviation from the standard, the scheme introduces some\nadditional trade offs:\n\n  * Signatures arger as signatures carry a new error hint\n\n  * Signing is 2.7x slower\n\n  * Verification is 1.75x slower\n\nThere's also a published BIP-32-like like scheme for Falcon signatures [2].\nI'm\nless familiar with the details here, but the signature size blows up to\n~24KB compared to ~666 bytes for normal Falcon signatures.\n\n-- Laolu\n\n[1]: https://cic.iacr.org/p/2/3/3\n\n[2]: https://link.springer.com/article/10.1186/s42400-024-00216-w\n\n\nOn Mon, Dec 8, 2025 at 10:49 PM 'conduition' via Bitcoin Development\nMailing List <bitcoindev@googlegroups.com> wrote:\n\n> Great work Jonas and Mikhail, glad to see more eyes and ears surveying\n> these schemes and their potential. Also shameless plug for some of my\n> prior work <https://conduition.io/code/fast-slh-dsa/> on related topics\n> <https://conduition.io/cryptography/quantum-hbs/>.\n>\n> The post-quantum HD wallet derivation problem is one i've been thinking\n> about a lot lately. Due to the lack of algebraic structure in SLH-DSA it's\n> gonna be impossible to fully emulate BIP32 with that scheme alone. I'm\n> personally hoping that we'll find a way to derive child pubkeys using\n> lattices (ML-DSA) and/or isogenies (SQIsign), but I haven't heard of any\n",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 1,
            "text_length": 2073,
            "has_nack": false,
            "has_ack": true
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 0
      }
    },
    {
      "title": "Re: [bitcoindev] Re: Hash-Based Signatures for Bitcoin's Post-Quantum Future",
      "message_count": 1,
      "participants": [
        "Olaoluwa Osuntokun"
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/CAO3Pvs8A0GLW-xdSHCKosjPqo7WSf-=YJ7F7s7t65RvtArcBEQ@mail.gmail.com",
          "title": "Re: [bitcoindev] Re: Hash-Based Signatures for Bitcoin's Post-Quantum Future",
          "author": "Olaoluwa Osuntokun <laolu32@gmail@com>",
          "date": "Tue, 9 Dec 2025 16:53:00 -0800",
          "body": "[-- Attachment #1: Type: text/plain, Size: 12933 bytes --]\n\nHi y'all,\n\nMike wrote:\n> But if we use different parameters sets don’t we already loose the\n> compatiability with the standardized schemes?\n\nIIUC, these smaller SPHINCS+ parameters are under active consideration\nby NIST [1].\n\nOn slide 3 of a recent talk [2] at the 6th PQC Standardization Conference\n[3], Quynh Dang states:\n\n> We plan to standardize 2^24-signature limit rls128cs1, rls192cs1,\nrls256cs1\n\nBut then later in the same slide that:\n> We don’t plan to standardize them now:\n> Lower limits come with higher security risks when misuse happens\n> Minimize the number of parameter sets\n\nIn the linked forum post the OP advocates for a swifter standardization\nprocess for the new params:\n\n> We believe all options should be standardized as soon as possible. To meet\n> the 2030–2035 targets for post-quantum–only deployments, development must\n> be finalized very soon. Roots of trust typically have lifetimes exceeding\n> a decade, and any further delay could make it impossible to adopt these\n> new options.\n\nSo it appears they do plan to standardize these additional parameter sets,\nbut it won't be done \"soon\"?\n\n-- Laolu\n\n[1]: https://groups.google.com/a/list.nist.gov/g/pqc-forum/c/x-eaz9be6_U\n\n[2]:\nhttps://csrc.nist.gov/csrc/media/presentations/2025/sphincs-smaller-parameter-sets/sphincs-dang_2.2.pdf\n\n[3]: https://csrc.nist.gov/Events/2025/6th-pqc-standardization-conference\n\nOn Tue, Dec 9, 2025 at 3:17 PM 'Mikhail Kudinov' via Bitcoin Development\nMailing List <bitcoindev@googlegroups.com> wrote:\n\n> Dear Conduition,\n>\n> You did a really nice job, I was wondering if it will be hard to add the\n> different modifications to your implementations?\n>\n> As for lattice-based schemes and other assumptions, we also thought about\n> investigations the possibilities there.\n>\n> With this derivation technique you propose, am I understanding correctly\n> that if the user signs with the hash-based scheme, then the user would\n> reveal that",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 2077,
            "has_nack": false,
            "has_ack": false
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 0
      }
    },
    {
      "title": "Re: [bitcoindev] A safe way to remove objectionable content from the blockchain",
      "message_count": 1,
      "participants": [
        "Peter Todd"
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/aTl8Y7p4qtYAsHbP@petertodd.org",
          "title": "Re: [bitcoindev] A safe way to remove objectionable content from the blockchain",
          "author": "Peter Todd <pete@petertodd@org>",
          "date": "Wed, 10 Dec 2025 13:57:55 +0000",
          "body": "[-- Attachment #1: Type: text/plain, Size: 2902 bytes --]\n\nOn Tue, Dec 09, 2025 at 11:32:48AM -0800, Boris Nagaev wrote:\n> Hi waxwing/AdamISZ,\n> \n> On incentives: agreed that \"good\" only matters if it's an equilibrium. The \n> aim is to shape early design choices so the incentive-compatible \n> equilibrium includes DA and forced publication, rather than slipping into a \n> DA-weak equilibrium where only a few parties hold full data.\n\nExactly.\n\nFurthermore I want to be clear that in this context, the existence of strong ZK\nmath is an *exploit* on the Bitcoin protocol, in much the same way that a\nmathematical advancement that could be used to break SHA256 preimage security\nis also an exploit on the Bitcoin protocol.\n\nIt may be the case that the power of ZK techniques is sufficiently strong that\nBitcoin needs to be redesigned to mitigate them; there is even a small chance\nthat this is not possible and Lightning/HTLCs eventually become insecure due to\nit. No different than how there is a small chance that quantum computing\nrelevant to cryptography turns out to be real and numerous protocols become\ninsecure due to it.\n\n> > what if mining was done just on an accumulator over the utxo set, instead \n> of the utxo set itself?\n> \n> If miners and nodes only see an UTXO accumulator, how do HTLCs survive? The \n> HTLC success spend path needs the preimage to be revealed and readable. How \n> does this fit in an accumulator-only mining model, and what forces \n> publication so the payer can claim its incoming HTLC?\n\nMore generally, if mining is just an accumulator, how do we preserve censorship\nresistence? It's unlikely that the underlying math of the accumulator allows\nanyone to mine a new block with exactly as much data as is required to verify\nthe accumulator. \n\nRecently I met someone who told me that his company needed a full archival node\nof the Solana (IIRC) blockchain. That is, *all* Solana transactions going back\nin time, sufficient to verify everything. They had a very large b",
          "drama_signals": {
            "drama_keywords": 1,
            "positive_keywords": 2,
            "text_length": 2080,
            "has_nack": false,
            "has_ack": true
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 1
      }
    },
    {
      "title": "Re: [bitcoindev] Motion to Activate BIP 3",
      "message_count": 1,
      "participants": [
        "Mat Balez"
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/CABd6=MPtx9rN2ZtTz7CbT-zb-3qVUecZZrmb56aFyCSVeLxsEQ@mail.gmail.com",
          "title": "Re: [bitcoindev] Motion to Activate BIP 3",
          "author": "Mat Balez <matbalez@gmail@com>",
          "date": "Thu, 20 Nov 2025 12:14:09 -0800",
          "body": "[-- Attachment #1: Type: text/plain, Size: 14565 bytes --]\n\nMore and more of writing by all humans, including BIP proposers, will\ninevitably involve AI in some more or less significant way. I don't expect\npeople to reliably express the degree to which AI was used to inform the\nthinking behind the BIP, or the writing itself. I'm not aware of any common\nstandard we would use to express those things. Adversarially, we have to\nassume people won't do it if it's not in their interests.\n\nRather, I think the expectation should be that BIP proposers are entirely\nresponsible for submitting high quality BIPs and they take ownership for\nwhat they are submitting (submitting garbage burns your rep, always has and\nalways will). BIP reviewers should simply assume for all BIPs that AI was\nlikely used significantly to create them, and judge BIPs only on the merit\nof the ideas and content.\n\nBecause of the advent of LLMs (and their inevitable continued improvement)\nthis will almost certainly result in an increased number of BIPs being\nadvanced, many of low (slop-filled) quality but also, hopefully, more high\nquality ones as well—proposals that might not otherwise have seen the light\nof day and/or proposals themselves being strengthened with better\narguments, ideas and language.\n\nThe solution to such a rise in volume IMO is that BIP reviewers should also\nequip themselves with LLMs and other AI-powered tools to help\nfilter/triage/assess BIPs to get a handle on the rise in noise level. Yet,\njust like BIP proposers, the onus should be on BIP reviewers to take\nownership for the quality of the decision-making around BIP quality and\nthat it not ever be entirely automated but retain \"human in the loop\"\njudgment—at least for the foreseeable future—just made more efficient and\neffective through the use of AI.\n\nOn Thu, Nov 20, 2025 at 1:47 AM Oghenovo Usiwoma <eunovo9@gmail•com> wrote:\n\n> > I think it makes sense to request that submissions should state if - and\n> to what degree - AI has been use",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 1,
            "text_length": 2042,
            "has_nack": false,
            "has_ack": false
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 0
      }
    },
    {
      "title": "Re: [bitcoindev] Re: Motion to Activate BIP 3",
      "message_count": 1,
      "participants": [
        "Greg Sanders"
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/899eb548-3e3b-4b85-8ae0-1e64d15f1b86n@googlegroups.com",
          "title": "Re: [bitcoindev] Re: Motion to Activate BIP 3",
          "author": "Greg Sanders <gsanders87@gmail@com>",
          "date": "Thu, 13 Nov 2025 10:54:20 -0800 (PST)",
          "body": "[-- Attachment #1.1: Type: text/plain, Size: 7475 bytes --]\n\nMakes sense to me, and this e-mail can be a reference point if there's \nfuture discussion.\n\nWith what little review I've done, I think this makes sense to activate!\n\nGreg\n\nOn Wednesday, November 12, 2025 at 7:30:59 PM UTC-5 Murch wrote:\n\n> Hey Greg,\n>\n> Two sections from BIP 3 stand out as relevant here, “BIP Ownership“ and \n> “Deployed Process BIPs”.\n>\n> From “Fundamentals > BIP Ownership”:\n> > “[…] As a BIP progresses through the workflow, it becomes \n> increasingly co-owned by the Bitcoin community.”\n>\n> While Deployed BIPs are considered final and changes should be avoided, \n> the section has a subsection that specifically addresses Process BIPs.\n>\n> From “Workflow > Progression through BIP Statuses > Deployed > Process \n> BIPs”:\n> > “A Process BIP may change status from Complete to Deployed when it \n> achieves rough consensus on the Bitcoin Development Mailing List. A \n> proposal is said to have rough consensus if its advancement has been \n> open to discussion on the mailing list for at least one month, the \n> discussion achieved meaningful engagement, and no person maintains any \n> unaddressed substantiated objections to it. Addressed or obstructive \n> objections may be ignored/overruled by general agreement that they have \n> been sufficiently addressed, but clear reasoning must be given in such \n> circumstances. Deployed Process BIPs may be modified indefinitely as \n> long as a proposed modification has rough consensus per the same criteria.”\n>\n> More specific rules supersede general rules, so this subsection on \n> Process BIPs should hopefully clearly override the general description \n> in “Deployed”. It follows from these two sections that the BIP Authors’ \n> right to decide about changes to their BIP is moderated by the community \n> interests. I would consider especially Process BIPs to be dominantly \n> owned by the community rather than the Authors once they are Deployed. \n> The quoted section s",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 2,
            "text_length": 2046,
            "has_nack": false,
            "has_ack": false
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 0
      }
    },
    {
      "title": "[bitcoindev] Splitting more block, addr and tx classes of network traffic",
      "message_count": 1,
      "participants": [
        "Antoine Riard"
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/CALZpt+Hx9vFwNQd6qGSFMWXU=A6j82m6ZjJg3JaHK26WW0UQZw@mail.gmail.com",
          "title": "[bitcoindev] Splitting more block, addr and tx classes of network traffic",
          "author": "Antoine Riard <antoine.riard@gmail@com>",
          "date": "Thu, 4 Dec 2025 22:33:43 +0000",
          "body": "[-- Attachment #1: Type: text/plain, Size: 3714 bytes --]\n\nHi list,\n\nSurfacing an old idea concerning the network-level and the current meddling\nof block,\ntx and addr messages traffic generally all over one network link.\nHistorically, for\nexample, if you consider bitcoin core by default connections are going to\nbe FULL_RELAY.\nOver the last years, there has been few improvements to separate network\nlinks by types\ne.g with the introduction of dedicated outbound BLOCK-RELAY connections\n[1], without the\nsegregation at the network-level between the class of traffic really being\npursued, or at\nleast more flexibility in network mechanisms to signal to a node's peers\nwhat categories\nof messages will be processed on a given link.\n\nPreviously it has been shown that leveraging tx-relay's orphan mechanism\ncan allow to map\na peer's network-topology [2] (sadly, one trick among others). Being able\nto infer a peer's\n\"likely\" network topology from tx traffic, one can guess the peers used to\ncarry block-relay\ntraffic. From the PoV of an economical node, dissimulating the block-relay\ntraffic is a very\nvaluable to minimize the risks of escalation attacks based on\nnetwork-topology (e.g for\nlightning nodes [3]).\n\nSegregating more network traffic by class of messages sounds to suppose 1)\nbeing able to signal\namong the {ADDR, ADDRV2} service bits if block, addr or tx relay is\nsupported on a link to be\nopened for a pair of a (net_addr, port) or alternatively 2) if network link\nare open blindly\nwith peers, being to signal in the VERSION message or with a dedicated\nmessage what class of\nmessage is supported. There is already a signaling mechanism in the VERSION\nmessage to\ndisable tx-relay (i.e `fRelay`), however there is no signaling to disable\nblock-relay over a link.\nAlternatively, it has been proposed in the past to add a new early message\namong all the other\nhandshake messages between the VERSION / VERACK flow, but it has never been\nimplemented [4].\n\nFor bitcoin backbone, started to nativ",
          "drama_signals": {
            "drama_keywords": 1,
            "positive_keywords": 1,
            "text_length": 2074,
            "has_nack": false,
            "has_ack": true
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 1
      }
    },
    {
      "title": "[bitcoindev] Re: Splitting more block, addr and tx classes of network traffic",
      "message_count": 1,
      "participants": [
        "defenwycke"
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/de958fe0-5897-4fb0-9b4c-b41f5c63296bn@googlegroups.com",
          "title": "[bitcoindev] Re: Splitting more block, addr and tx classes of network traffic",
          "author": "defenwycke <cal.defenwycke@gmail@com>",
          "date": "Tue, 9 Dec 2025 15:13:05 -0800 (PST)",
          "body": "[-- Attachment #1.1: Type: text/plain, Size: 7494 bytes --]\n\nHello Antoine,\n\nThis is an interesting problem, and introducing finer-grained traffic \nclasses certainly makes sense. The three areas that stand out to me are \npeer declaration, topology inference and system bottlenecks.\n\nPeer declaration: \n\nExplicit signalling of specialised roles (Example - I only relay \nhot-blocks) to peers increases the fingerprint/profile. We already see \ntopology inference attacks via relay behaviour; adding public role \ndeclarations may expand that surface. Nodes can already drop or \ndeprioritise whatever they wish locally, so explicit signalling may not be \nnecessary.\n\nTopology inference:\n\nSince topology inference can be drawn from tx-relay timing and relay \nbehaviour, an internal class-based model also allows the node to randomise \nacceptance, forwarding, and scheduling behaviour per class. Even small \namounts of deliberate jitter or probabilistic message handling make it far \nharder for an observer to infer which peers are responsible for block-relay \nversus tx-relay traffic. This further reduces the value of explicit \ncapability signalling, since role exposure can be disguised at the \nbehavioural level instead.\n\nSystem bottlenecks: \n\nIntroducing multiple traffic classes as separate processes and sockets \nincreases resource consumption, as you’ve noted. A single connection can \nalready multiplex all P2P message types.\n\nA cleaner approach might be to integrate the class separation internally, \nwithout advertising anything to peers. Incoming messages can be classified \n(Examples - hot blocks, cold blocks, tx, address gossip, etc.) and \nper-class policies applied locally. Since a node doesn’t need to receive or \nforward traffic it doesn’t want to handle, it seems unnecessary to declare \ntoggle roles at handshake time.\n\nAn additional benefit of integrating classes internally is that it gives a \nnatural place for per-class bandwidth accounting and load shedding. Under \ncongestion, hot",
          "drama_signals": {
            "drama_keywords": 1,
            "positive_keywords": 2,
            "text_length": 2078,
            "has_nack": false,
            "has_ack": true
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 1
      }
    },
    {
      "title": "[bitcoindev] CTV activation meeting on IRC - Thursday 18 December 17:00 UTC",
      "message_count": 1,
      "participants": [
        "\"/dev /fd0\""
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/CALiT-Zqo-gRCeMDd2VtmXL7Ox31OT3pFmsfXQ5mq6SzeuFnMzg@mail.gmail.com",
          "title": "[bitcoindev] CTV activation meeting on IRC - Thursday 18 December 17:00 UTC",
          "author": "\"/dev /fd0\" <alicexbtong@gmail@com>",
          "date": "Wed, 10 Dec 2025 03:38:14 +0530",
          "body": "[-- Attachment #1: Type: text/plain, Size: 775 bytes --]\n\nHi everyone,\n\nWe will organize a meeting next week to discuss the activation parameters\nfor BIP 119. You can review the related pull requests, different activation\nmethods, past meeting logs etc. and participate in the discussion.\n\nIRC Channel: #ctv-csfs-activation on libera.chat\nAgenda: Discuss activation parameters and build activation client for BIP\n119\n\n/dev/fd0\nfloppy disk guy\n\n-- \nYou received this message because you are subscribed to the Google Groups \"Bitcoin Development Mailing List\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to bitcoindev+unsubscribe@googlegroups•com.\nTo view this discussion visit https://groups.google.com/d/msgid/bitcoindev/CALiT-Zqo-gRCeMDd2VtmXL7Ox31OT3pFmsfXQ5mq6SzeuFnMzg%40mail.gmail.com.\n\n[-- Attachment #2: Type: text/html, Size: 1197 bytes --]",
          "drama_signals": {
            "drama_keywords": 2,
            "positive_keywords": 0,
            "text_length": 966,
            "has_nack": false,
            "has_ack": false
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 2
      }
    },
    {
      "title": "[bitcoindev] Request for early peer review of two BIP drafts (BUDS and segOP)",
      "message_count": 1,
      "participants": [
        "Callum"
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/CAOj3_X-WjPYaiTj5NQnAFp+df+4qWCT8oG_c9DsS-TZ47SjZow@mail.gmail.com",
          "title": "[bitcoindev] Request for early peer review of two BIP drafts (BUDS and segOP)",
          "author": "Callum <cal.defenwycke@gmail@com>",
          "date": "Mon, 8 Dec 2025 21:52:26 +0000",
          "body": "[-- Attachment #1: Type: text/plain, Size: 1167 bytes --]\n\nHello all,\n\nI would appreciate early peer review on two BIP drafts I have published:\n\n   1.\n\n   BUDS (Bitcoin Unified Data Standard) — an informational BIP defining a\n   neutral, non-consensus taxonomy for describing transaction data.\n\n   Draft and reference materials:\n   https://github.com/defenwycke/bip-buds\n   2.\n\n   segOP (Segregated OP_RETURN) — a consensus proposal describing a\n   structured TLV data section and corresponding commitment output.\n\n   Draft and reference materials:\n   https://github.com/defenwycke/bip-segop\n\nBoth drafts are small and self-contained. Feedback on clarity, correctness,\nstructure, or missing considerations would be very welcome.\n\nThank you for your time.\n\nKind regards,\n\nDefenwycke\n\n08.12.2025\n\n-- \nYou received this message because you are subscribed to the Google Groups \"Bitcoin Development Mailing List\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to bitcoindev+unsubscribe@googlegroups•com.\nTo view this discussion visit https://groups.google.com/d/msgid/bitcoindev/CAOj3_X-WjPYaiTj5NQnAFp%2Bdf%2B4qWCT8oG_c9DsS-TZ47SjZow%40mail.gmail.com.\n\n[-- Attachment #2: Type: text/html, Size: 2528 bytes --]",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 1,
            "text_length": 1323,
            "has_nack": false,
            "has_ack": true
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 0
      }
    },
    {
      "title": "[bitcoindev] A safe way to remove objectionable content from the blockchain (now on GitHub)",
      "message_count": 1,
      "participants": [
        "Lazy Fair"
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/CABHzxrjfvyBRD7sG9rngvDhr9cfzLEQibn4bup_J8pz7UHQpqA@mail.gmail.com",
          "title": "[bitcoindev] A safe way to remove objectionable content from the blockchain (now on GitHub)",
          "author": "Lazy Fair <laissez.faire.btc@gmail@com>",
          "date": "Sat, 6 Dec 2025 17:41:10 +1100",
          "body": "[-- Attachment #1: Type: text/plain, Size: 4166 bytes --]\n\nI've started putting together some ideas around how to remove objectionable\ncontent from the blockchain. The very early work-in-progress description is\non GitHub: https://github.com/laissez-faire-btc/safe-remove\n\nI won't include all the details here, because there's necessarily a lot to\ncover, but the basic design goals I've aimed to address are something like\nthis:\n\n* optional - each node gets to decide what to remove, if anything\n* safe - provably no harm is done to those not choosing to use it, and any\ncost or risk to those using it is well understood, minimal, and mitigated\n* full node functionality - a node that does remove content can still do\neverything it could have done otherwise, without relying on anyone else\n* retrospective - content that exists on the blockchain today\n(pre-implementation) can be removed later (post-implementation)\n* trustless, verifiable, permissionless - control messages enabling data to\nbe removed are simple verifiable statements of fact that can be written by\nanybody\n* lightweight - minimal changes and impact to policy, consensus,\nimplementation, usage, the economy\n* granularity, associativity, commutability, idempotence - the least\npossible data is removed, and ordering is inconsequential\n* transferable - nodes that choose to remove objectionable content can\nshare those blocks (with content removed) with others who hold the same\nobjection, so that the receiver may never even momentarily hold the\nobjectionable content\n\nBeing design goals, these are probably not all achievable. I'll need your\nhelp to work through all the details.\n\nI have some more notes I just haven't written up yet, so I'm keen for your\ninput please, on what direction I should take, questions I should answer,\naspects I should consider or detail further, etc.\n\nIn the absence of any feedback, I'll be proceeding with either documenting\nthe threat model, or a bit of a literature review - starting with the\nfollowi",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 1,
            "text_length": 2092,
            "has_nack": false,
            "has_ack": true
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 0
      }
    },
    {
      "title": "Re: [bitcoindev] op_ctv still has no technical objections",
      "message_count": 1,
      "participants": [
        "\"/dev /fd0\""
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/CALiT-ZrgNdMVs7mAWRhrXEqkyxXAdjE-gwjZfpv=2arzcSFopA@mail.gmail.com",
          "title": "Re: [bitcoindev] op_ctv still has no technical objections",
          "author": "\"/dev /fd0\" <alicexbtong@gmail@com>",
          "date": "Sun, 30 Nov 2025 04:38:27 +0530",
          "body": "[-- Attachment #1: Type: text/plain, Size: 2986 bytes --]\n\nHi Erik,\n\nWe can organize a meeting to discuss the activation parameters and build an\nactivation client. If enough economic nodes and miners run the activation\nclient it will be activated with no further politics or drama.\n\n#ctv-csfs-activation IRC channel can be used for the meeting.\n\n/dev/fd0\nfloppy disk guy\n\nOn Thu, Nov 27, 2025 at 2:47 PM Erik Aronesty <erik@q32•com> wrote:\n\n> It's been many years and there's been a lot of discussion about various\n> covenants\n>\n> I think one of the biggest problems is everyone has to insist on their\n> baby is the best baby.\n>\n> op_ctv is quite literally not the best at anything.  That's the whole\n> point.  It's non-recursive, can't be used for strange or dangerous things,\n> and can be used to emulate a lot of other opcodes.\n>\n> It's adequate.  And I don't think we want anything \"better\" than adequate\n> the first time around. lnhance is more comprehensive.  but also it's so\n> much harder to reason about three separate op codes and what the attack\n> surface could be.\n>\n> I don't think it's possible to optimize a series of covenants for all\n> possible scenarios.  Easy to make them too powerful and now nodes are doing\n> too much work and we're attracting the kind of network activity that nobody\n> wants.\n>\n> Fortunately the risk of CTV is fairly low.  It's always possible to turn\n> it off (no new tx)... if there's a game theory issue.\n>\n> I don't think there's any particular rush, but we could lose a lot of fees\n> and support for miners if Bitcoin continues to do what it is doing now...\n> scaling almost entirely in custodial systems.  That's also just not the\n> Bitcoin that anyone loves.\n>\n> At this point it feels like it's \"perfect is the enemy of the good\".\n>\n> We have an old and rather well tested pull request that is only a handful\n> of lines of code that everyone has scrutinized a million ways.\n>\n> I don't think we're getting that for any other covenant opcode.\n>\n>\n>\n>\n>",
          "drama_signals": {
            "drama_keywords": 3,
            "positive_keywords": 2,
            "text_length": 2058,
            "has_nack": false,
            "has_ack": true
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 3
      }
    },
    {
      "title": "[bitcoindev] Re: op_ctv still has no technical objections",
      "message_count": 1,
      "participants": [
        "\"'conduition' via Bitcoin Development Mailing List\""
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/2914ad6f-7e1a-42b6-9c25-87ac48c63228n@googlegroups.com",
          "title": "[bitcoindev] Re: op_ctv still has no technical objections",
          "author": "\"'conduition' via Bitcoin Development Mailing List\" <bitcoindev@googlegroups.com>",
          "date": "Thu, 27 Nov 2025 18:04:28 -0800 (PST)",
          "body": "[-- Attachment #1.1: Type: text/plain, Size: 2607 bytes --]\n\nAs someone who has had a merely passive interest in covenants tech, I can \nconfidently say that OP_CTV is probably the only covenants proposal whose \neffects I can confidently say I fully grasp. It's also easy to explain to \nothers. Not saying i'm not in favor of more complex multi-pronged upgrades \nlike LNHANCE, just saying I don't fully understand their opcode interplay \nenough to say yay/nay. Which is maybe an under-represented argument in \nfavor of plain OP_CTV.\n\nregards,\nconduition\n\nOn Thursday, November 27, 2025 at 1:18:03 AM UTC-8 Erik Aronesty wrote:\n\n> It's been many years and there's been a lot of discussion about various \n> covenants \n>\n> I think one of the biggest problems is everyone has to insist on their \n> baby is the best baby. \n>\n> op_ctv is quite literally not the best at anything.  That's the whole \n> point.  It's non-recursive, can't be used for strange or dangerous things, \n> and can be used to emulate a lot of other opcodes. \n>\n> It's adequate.  And I don't think we want anything \"better\" than adequate \n> the first time around. lnhance is more comprehensive.  but also it's so \n> much harder to reason about three separate op codes and what the attack \n> surface could be.\n>\n> I don't think it's possible to optimize a series of covenants for all \n> possible scenarios.  Easy to make them too powerful and now nodes are doing \n> too much work and we're attracting the kind of network activity that nobody \n> wants.  \n>\n> Fortunately the risk of CTV is fairly low.  It's always possible to turn \n> it off (no new tx)... if there's a game theory issue. \n>\n> I don't think there's any particular rush, but we could lose a lot of fees \n> and support for miners if Bitcoin continues to do what it is doing now... \n> scaling almost entirely in custodial systems.  That's also just not the \n> Bitcoin that anyone loves.\n>\n> At this point it feels like it's \"perfect is the enemy of the good\".  \n>\n> We have ",
          "drama_signals": {
            "drama_keywords": 2,
            "positive_keywords": 1,
            "text_length": 2058,
            "has_nack": false,
            "has_ack": true
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 2
      }
    },
    {
      "title": "Re: [bitcoindev] OP_CIV - Post-Quantum Signature Aggregation",
      "message_count": 1,
      "participants": [
        "\"'conduition' via Bitcoin Development Mailing List\""
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/02e3cb64-50e8-4814-bf53-72db87deafc8n@googlegroups.com",
          "title": "Re: [bitcoindev] OP_CIV - Post-Quantum Signature Aggregation",
          "author": "\"'conduition' via Bitcoin Development Mailing List\" <bitcoindev@googlegroups.com>",
          "date": "Fri, 28 Nov 2025 10:52:27 -0800 (PST)",
          "body": "[-- Attachment #1.1: Type: text/plain, Size: 17217 bytes --]\n\nHey Tadge,\n\nYou're right that OP_CIV would discourage address reuse, but it'd also make \nlife difficult for wallet developers who want to adopt it. Some wallet devs \n*today* don't even bother with multi-address support, so imagine if to do \nso effeciently, they needed to statefully track prior UTXOs and generate \naddresses based on a changing UTXO set over time.\n\nAlso I want to mention, there's a big privacy difference between these CISA \ntechniques, and CISA via address reuse. \n\nIf I receive two payments to the same address, i immediately reveal that \nthose UTXOs are owned by the same entity: me.\n\nIf I receive two payments to *distinct* addresses which are linked via your \nOP_CIV (or via my idea by committing to pubkeys) then I can choose when and \nwhether to reveal the fact that those UTXOs are commonly owned. Most user \nwallets have more than just two UTXOs, so in a setting where I have \npossibly dozens of UTXOs, this offers me more flexibility with respect to \nmy on-chain privacy, allowing me to choose when and how to reveal common \nUTXO ownership. This is kind of already the status quo, because chainalysis \nuses common-input ownership heuristics even if they are flawed/incorrect, \njust for the sake of having a \"working\" tool they can sell.\n\nRegarding the extra cost, we can quantify that! Let's say we have a taptree \nof height `h` with `2^h` leaves. We use one leaf for a unique pubkey, and \nthe other `2^h - 1` tap leaves store commitments to other pubkeys or to \npre-existing UTXOs. To spend a TX with `n` inputs using this CISA paradigm, \nwe need one signature, plus `n - 1` taproot control blocks and tapscripts. \nEach control block has size `h * 32`, plus ~32 bytes to reference the other \npubkey or UTXO in the locking tapscript. So in total, the witness size \nscales as: `(n - 1)((h + 1) * 32)`. In other words, for every additional \ninput covered by the CISA scheme, we must pay for roughly `(h + 1) * 32",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 1,
            "text_length": 2061,
            "has_nack": false,
            "has_ack": true
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 0
      }
    },
    {
      "title": "Re: [bitcoindev] SLH-DSA (SPHINCS) Performance Optimization Techniques",
      "message_count": 1,
      "participants": [
        "Tim Ruffing"
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/6ad6c7418b6b845d6e2dd0ccdb2b508de0c3c10c.camel@real-or-random.org",
          "title": "Re: [bitcoindev] SLH-DSA (SPHINCS) Performance Optimization Techniques",
          "author": "Tim Ruffing <me@real-or-random@org>",
          "date": "Fri, 28 Nov 2025 16:39:12 +0100",
          "body": "Let me just say that leave the note here that this is awesome work!\n\nI didn't expect that so much can be gained using SIMD, and that it\nbeats SHA-NI by such a large margin (even taking into account the\ncaveats you've mentioned).\n\nTim\n\nOn Sun, 2025-11-23 at 18:46 -0800, 'conduition' via Bitcoin Development\nMailing List wrote:\n> Hi devs,\n> \n> I've spent the last several months implementing and benchmarking\n> optimization techniques for the post-quantum hash-based signature\n> scheme SLH-DSA (formerly SPHINCS+), which is being considered as a\n> candidate for a quantum-resistant soft-fork upgrade to Bitcoin, re:\n> BIP360.\n> \n> Survey article: https://conduition.io/code/fast-slh-dsa/\n> \n> char1.png\n> \n> As a material result of my findings, I believe I now possess what may\n> be the fastest publicly available implementation of SLH-DSA (at least\n> on my hardware), and possibly also one of the fastest GPU\n> implementations, though I've had difficulty finding comparable\n> alternatives on that front. Its speed is owed to the Vulkan graphics\n> programming API, often used by video game devs to squeeze performance\n> out of gaming PCs and mobile phones.\n> \n> The code: \n> - https://github.com/conduition/slhvk\n> - https://github.com/conduition/slh-experiments\n> \n> Using my CPU, this code can sign a message with SLH-DSA-SHA2-128s in\n> just 11 milliseconds, and can generate keys in only 2 milliseconds\n> (1ms if batched). Verification throughput approaches that of ECDSA,\n> at around 15000 nanoseconds per verification if properly batched. If\n> you have a GPU with drivers, everything runs even faster.\n> \n> For perspective, the fastest open source SLH-DSA library I could\n> find, PQClean, requires 94 milliseconds for SLH-DSA-SHA2-128s signing\n> and 12ms for keygen on my CPU. PQClean can only achieve this speed on\n> x86 CPUs, whereas Vulkan works on ARM devices, including Apple\n> silicon.\n> \n> There are caveats. This technique is memory-hungry, requiring several\n> megabytes of RAM for signin",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 2071,
            "has_nack": false,
            "has_ack": false
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 0
      }
    },
    {
      "title": "[bitcoindev] Q-Lock: Quantum-Resistant Spending via ECDSA + Hash-Based Secrets",
      "message_count": 1,
      "participants": [
        "Amarildo"
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/1c6c4afc-0fe4-4b72-b70f-3f6ba4c19315n@googlegroups.com",
          "title": "[bitcoindev] Q-Lock: Quantum-Resistant Spending via ECDSA + Hash-Based Secrets",
          "author": "Amarildo <amarildocaka01@gmail@com>",
          "date": "Fri, 28 Nov 2025 07:00:04 -0800 (PST)",
          "body": "[-- Attachment #1.1: Type: text/plain, Size: 14499 bytes --]\n\nHi everyone,\n\nI'd like to propose an alternative approach to quantum resistance \nfor Bitcoin that I believe is simpler than BIP-360 P2QRH.\n\n**Q-Lock: Quantum-Resistant Spending Protocol**\n\nSUMMARY:\n- Keeps ECDSA unchanged (no new signature algorithms!)\n- Adds hash-based secret layer on top\n- Uses only SHA256 + Merkle trees (proven crypto)\n- ~3 KB transactions (comparable to FALCON)\n- Two-phase commit-reveal scheme\n- Soft fork compatible\n- BIP-32 HD wallets work normally\n\nKEY INSIGHT:\nInstead of replacing ECDSA with new post-quantum algorithms \n(FALCON, SPHINCS+, Dilithium), Q-Lock adds a quantum-safe \nsecret layer. Attacker must break BOTH ECDSA AND know the \nhash preimages - quantum computers can't reverse SHA256.\n\nCOMPARISON TO BIP-360:\n- BIP-360: New lattice-based crypto, 1.3-50 KB sigs, breaks BIP-32\n- Q-Lock: Proven SHA256 crypto, ~3 KB sigs, BIP-32 works\n\nHOW IT WORKS:\n1. Setup: Generate 64 random secrets, commit via Merkle root\n2. Commit phase: Lock outputs WITHOUT exposing pubkey\n3. Reveal phase: Expose pubkey + secrets at block-hash-determined positions\n4. Quantum attacker sees pubkey too late - outputs already locked!\n\n```\nQ-Lock is a quantum-resistant spending protocol for Bitcoin \nthat adds a hash-based secret layer on top of existing ECDSA \nsignatures. It uses a two-phase commit-reveal scheme where \nspending positions are determined by the block hash, making \nit secure against quantum attackers who can break ECDSA.\n\nQ-Lock does NOT replace ECDSA. It adds quantum protection \nwhile preserving Bitcoin's existing cryptographic foundation.\n\nTransaction size: ~3 KB\nRequires: Soft fork (1-2 new opcodes)\n```\n\n-----\n\n## MOTIVATION\n\n```\nTHE QUANTUM THREAT:\n━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n\nShor's algorithm can break ECDSA by extracting private keys \nfrom public keys. When a Bitcoin transaction is broadcast, \nthe public key is exposed in the mempool. A quantum attacker \ncould:\n\n1. See public ke",
          "drama_signals": {
            "drama_keywords": 2,
            "positive_keywords": 1,
            "text_length": 2079,
            "has_nack": false,
            "has_ack": true
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 2
      }
    },
    {
      "title": "Re: [bitcoindev] Benchmarking Bitcoin Script Evaluation for the Varops Budget (GSR)",
      "message_count": 1,
      "participants": [
        "\"'Julian' via Bitcoin Development Mailing List\""
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/5906e2bb-c215-44b0-bb61-0bb91d55717dn@googlegroups.com",
          "title": "Re: [bitcoindev] Benchmarking Bitcoin Script Evaluation for the Varops Budget (GSR)",
          "author": "\"'Julian' via Bitcoin Development Mailing List\" <bitcoindev@googlegroups.com>",
          "date": "Fri, 28 Nov 2025 05:09:25 -0800 (PST)",
          "body": "[-- Attachment #1.1: Type: text/plain, Size: 6894 bytes --]\n\nHi Russell,\n\nthanks for taking a look at the code.\n\nIn interpreter.cpp the static function EvalChecksigTapscript(...) is \nresponsible for subtracting from execdata.m_validation_weight_left, for the \noriginal SigVersion::TAPSCRIPT this is still the case, but Tapscript v2 is \nimplemented as a new SigVersion::TAPSCRIPT_V2 and therefore it will not \ntake the original sigops constraint into account (there is an if condition \nright above checking for the SigVersion).\n\nThe new varops budget replaces this sigops constraint and is contained in \nthe new EvalScript(...) overload. Currently it will only subtract from the \nbudget if the checksig succeeds, but I think this should be moved up a \nstatement, such that it will always subtract the varops cost, making the \ncost calculation more static.\n\nThe changes have not been reviewed in depth and I am looking for someone \ninterested in helping me with that.\n\n\n\nOn Monday, 10 November 2025 at 15:48:27 UTC+1 Russell O'Connor wrote:\n\nMy understanding is that in order to avoid block assembly becoming an \nNP-hard packing problem, there must be only one dimension of constraint \nsolving.  However, AFAICT, in your tarscript V2 code you have both the new \nvarops constraint and the original sigops constraint.\n\nFWIW, in Simplicity we reuse the same budget mechanism introduced in \ntapscript (V1) with our cost calculations (though our costs are computed \nstatically instead of dynamically at runtime for better or for worse).\n\nOn Fri, Nov 7, 2025 at 11:06 AM 'Julian' via Bitcoin Development Mailing \nList <bitco...@googlegroups•com> wrote:\n\nHello everyone interested in Great Script Restoration and the Varops Budget,\n\nThe main concerns that led to the disabling of many opcodes in v0.3.1 were \ndenial-of-service attacks through excessive computational time and memory \nusage in Bitcoin script execution. To mitigate these risks, we propose to \ngeneralize the sigops budget in a new Tapscript le",
          "drama_signals": {
            "drama_keywords": 1,
            "positive_keywords": 1,
            "text_length": 2084,
            "has_nack": false,
            "has_ack": true
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 1
      }
    },
    {
      "title": "[bitcoindev] Re: AI-assisted drafts and disclosure",
      "message_count": 1,
      "participants": [
        "Oghenovo Usiwoma"
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/CAOCjZ9RmtKWALV033EENs=wzt43GUG9k4mOeJkAzvyp2e2xj9w@mail.gmail.com",
          "title": "[bitcoindev] Re: AI-assisted drafts and disclosure",
          "author": "Oghenovo Usiwoma <eunovo9@gmail@com>",
          "date": "Thu, 20 Nov 2025 18:48:02 +0100",
          "body": "[-- Attachment #1: Type: text/plain, Size: 2166 bytes --]\n\n> Clear disclosure of AI assistance as a process note, not a stigma.\n\nI agree with you, but I think disclosure of AI assistance will be treated\nas \"stigma\", even if that was not its intention. This is my issue with the\n\"AI-label\". If I use AI for research, do I have to add \"AI label\" to my\nBIP? at what point do I have to add the label?\n\n- Novo\n\nOn Thu, Nov 20, 2025 at 1:16 PM nt yl <wrapperband@googlemail•com> wrote:\n\n> Hi Oghenovo Usiwoma and Bitcoin Mechanic,\n>\n> You wrote:\n>\n> In my humble opinion, I believe that humans will continue to use the\n> easiest method available to them to achieve their goals. If we agree that\n> humans will do this, then there will be a lot of AI-assisted content. If I\n> did write an AI-assisted BIP draft, why would I add this \"AI-label\" to my\n> BIP when I know that it will cause reviewers to ignore it?\n>\n> As a disabled person who uses AI tools, my view is that AI will soon be\n> part of most serious workflows, much like reading the manuals and prior\n> discussions is today. Used well, it can summarise long threads, prioritise\n> issues, deduplicate proposals, and help check code for obvious bugs.\n> Refusing to use any such tools can be a step backward in productivity.\n>\n> The key is how we use them. I would support:\n>\n>    -\n>\n>    Clear disclosure of AI assistance as a process note, not a stigma.\n>    -\n>\n>    Strong norms that final authorship, technical accuracy, and\n>    accountability rest with the human proposer.\n>    -\n>\n>    Encouraging A.I. for review support, not for replacing understanding.\n>\n> This balances transparency with practical benefits and keeps the bar on\n> rigour where it belongs.\n>\n> Best,\n> Wrapper\n>\n> https://www.zerogpt.com/   0% A.I.\n>\n\n-- \nYou received this message because you are subscribed to the Google Groups \"Bitcoin Development Mailing List\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to bitcoindev+unsubsc",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 2,
            "text_length": 2051,
            "has_nack": false,
            "has_ack": true
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 0
      }
    },
    {
      "title": "[bitcoindev] Announcing Penlock v1: Paper-Based Secret Splitting for BIP39 Seed Phrases",
      "message_count": 1,
      "participants": [
        "\"'Rama Gan' via Bitcoin Development Mailing List\""
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/GbNAPQX2Q4TzZvS6aWLPf3iy7z1yTXVrgnPpXazBjdcWH-zROBEBieE02r6GX128LM7mml6oTzAmlboV97EWpG1ujLcVZ6fx6uihUMXxCEo=@proton.me",
          "title": "[bitcoindev] Announcing Penlock v1: Paper-Based Secret Splitting for BIP39 Seed Phrases",
          "author": "\"'Rama Gan' via Bitcoin Development Mailing List\" <bitcoindev@googlegroups.com>",
          "date": "Thu, 20 Nov 2025 09:04:24 +0000",
          "body": "Hello everyone,\n\nI am thrilled to announce the public release of Penlock!\n\nThe goal is achieved! If you have a printer, scissors, a craft knife,\nand a pin, you can mechanically secret-split a 12-word seed phrase\nin under two hours. This includes the entire process—learning,\nprinting, assembling, executing, and storing the shares.\n\nPenlock is a printable paper calculator that guides you through\nsplitting a seed phrase into a 2-of-3 backup. It is open-source,\nuses straightforward and robust cryptography, and includes various\nfail-safes that protect against errors. A beta was announced on\nthis list last year, and the public release is now available at:\n<https://v1.penlock.io/en/>\n\nThis release breaks backward-compatibility with the beta, allowing for\nenhancements that make Penlock significantly easier to operate. Here\nare the main improvements in v1:\n\n- Faster Secret-Splitting: Penlock now focuses exclusively on producing\n2-of-3 backups using its own paper-optimized splitting algorithm. The\nprevious iteration supported K-of-M splitting with Shamir Secret\nSharing, but at the cost of more complexity and a clunkier 2-of-3\nprocess. Since 2-of-3 covers nearly all use cases, optimizing for it\nseemed like the right approach.\n\n- Backup Strategy Template: Penlock now suggests a generic, adaptable\nbackup strategy that helps set up offsite recovery and trust-minimized\ninheritance. In short, each share is tied to a different type of\nstorage: Digital, Social, and (optionally) Legal. This ensures an\nattacker would have to run two different types of attacks, and makes\nit hard for a party holding one share to obtain a second one. You\ncan find more details at <https://v1.penlock.io/en/split#strategy>.\n\n- On-Paper Error Correction: Penlock v1 introduces what I believe to\nbe the first on-paper error correction algorithm. Each BIP39 word is\nextended with two pre-computed parity symbols, guaranteeing per-word\nunambiguous correction of 1 error and detection of 2. In practice,\nit's also poss",
          "drama_signals": {
            "drama_keywords": 1,
            "positive_keywords": 1,
            "text_length": 2088,
            "has_nack": false,
            "has_ack": true
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 1
      }
    },
    {
      "title": "Re: [bitcoindev] Improve Bitcoin’s resilience to large-scale power grid failures and Carrington-type solar storms",
      "message_count": 1,
      "participants": [
        "\"Edil Guimarães de Medeiros\""
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/CANJiN3LnCFJxJpGxScLTqT2JF4iQNsX3hipiNiBK_OLMkkXb1g@mail.gmail.com",
          "title": "Re: [bitcoindev] Improve Bitcoin’s resilience to large-scale power grid failures and Carrington-type solar storms",
          "author": "\"Edil Guimarães de Medeiros\" <jose.edil@gmail@com>",
          "date": "Wed, 19 Nov 2025 14:04:36 -0300",
          "body": "[-- Attachment #1: Type: text/plain, Size: 4863 bytes --]\n\nI don't see any specific measure that would require specific support from\nBitcoin Core, maybe you can point to more specific requirements.\nThe canonical approach is to maintain specific projects that solve specific\nproblems using one of the node interfaces (e.g. RPC).\nBut of course, anyone is free to contribute patches that might help handle\nthis kind of situation.\n\nAs you said, reorgs are expected to be gracefully handled already by the\nnode implementations.\nMost software that is tested in testnet probably also was exposed to harsh\nconditions like deep reorgs and long periods without any block being mined.\nHaving said that, the potential problem you describe is not specific to\nBitcoin and having alternative critical communication mechanisms is\ndesirable.\nBut they probably fall under the economically not viable kind of\ninfrastructure that humans have relied on governments to implement and\nmaintain, which is far from an ideal approach.\n\nAnd by the way, this is the mailing list.\n\nRegards.\n\nEm dom., 16 de nov. de 2025 às 20:00, Alexandre <alexandre.lg99@gmail•com>\nescreveu:\n\n> Hi,\n> I’m submitting this feature request to explore how Bitcoin could better\n> withstand extreme, long-lasting infrastructure failures caused by major\n> solar events. Before explaining the request itself, I want to provide a\n> brief overview of what these events are, because their scale matters.\n>\n> A large solar storm occurs when the Sun emits an intense burst of charged\n> particles and electromagnetic energy. When this material reaches Earth, it\n> can disturb the magnetic field and induce strong electric currents in long\n> conductors such as power lines. In extreme cases, this can damage\n> transformers, overload electrical grids, interrupt satellite operations,\n> and disrupt long-distance communication systems. The most famous historical\n> example is the Carrington Event of 1859, the largest geomagnetic storm ever\n> recorded. It trigge",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 1,
            "text_length": 2114,
            "has_nack": false,
            "has_ack": false
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 0
      }
    },
    {
      "title": "[bitcoindev] [BIP Proposal] Standardization of On-Chain Identity Publication",
      "message_count": 1,
      "participants": [
        "\"'Edyth Kylak Johnson' via Bitcoin Development Mailing List\""
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/2acc9c28-6413-4147-8d11-e1ae0a677b75n@googlegroups.com",
          "title": "[bitcoindev] [BIP Proposal] Standardization of On-Chain Identity Publication",
          "author": "\"'Edyth Kylak Johnson' via Bitcoin Development Mailing List\" <bitcoindev@googlegroups.com>",
          "date": "Wed, 19 Nov 2025 03:54:25 -0800 (PST)",
          "body": "[-- Attachment #1.1: Type: text/plain, Size: 943 bytes --]\n\nDear bitcoin-dev,\nI am submitting a draft Bitcoin Improvement Proposal titled *“Standardization \nof On-Chain Identity Publication”* for discussion. The draft specifies \ncanonical CBOR payloads, Poseidon-based `nullifier_hash` domain separation \n(`v0iden` / `v0corp`), and an optional Ed25519 signature wrapper. The \nproposal text and implementation notes are available in this \nPR: https://github.com/bitcoin/bips/pull/2038 . I welcome review and \nfeedback on interoperability, canonicalization (deterministic CBOR), and \nsecurity considerations.\n\n-- \nYou received this message because you are subscribed to the Google Groups \"Bitcoin Development Mailing List\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to bitcoindev+unsubscribe@googlegroups•com.\nTo view this discussion visit https://groups.google.com/d/msgid/bitcoindev/2acc9c28-6413-4147-8d11-e1ae0a677b75n%40googlegroups.com.\n\n[-- Attachment #1.2: Type: text/html, Size: 1196 bytes --]",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 1,
            "text_length": 1121,
            "has_nack": false,
            "has_ack": true
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 0
      }
    },
    {
      "title": "[bitcoindev] New bitcoin backbone code release + Tx relay v2 update",
      "message_count": 1,
      "participants": [
        "Antoine Riard"
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/299d4f39-b8cd-4736-b6bb-71def4d85f74n@googlegroups.com",
          "title": "[bitcoindev] New bitcoin backbone code release + Tx relay v2 update",
          "author": "Antoine Riard <antoine.riard@gmail@com>",
          "date": "Tue, 18 Nov 2025 16:01:33 -0800 (PST)",
          "body": "[-- Attachment #1.1: Type: text/plain, Size: 3679 bytes --]\n\nHello devs,\n\nShared new code for bitcoin backbone available on the website\n(bitcoinbackbone.org). Biggest changes from latest release has\nbeen mostly working on BIP324 re-implementation, cleaning bugs\nimplementing a simple tx-relay stack, a little mempoool buffer\nand some groundworks on address management. Tx syncing works with\nvanilla bitcoin core v0.30 software.\n\nI did a layout of the process architecture on the website, but the\nmempool is fully living in its own mempool process, fully separate\nfrom the block pipeline. In case of mempool DoS for whatever reasons,\nthe full-node keeps processing blocks. This also opens the door to\nhave *multiple* mempools with incompatible policies among themselves,\nand just select the highest fees paying graph of consensus-valid\ntransactions, after sanitizing out conflicts.\n\nAs I was writing in my latest email about bitcoin backbone, of course\nthere are some trade-offs with the mempool not living in the same memory\nspace than the validation engine, though I think you have practical\nimprovements on this area.\n\nThe simple tx-relay stack also implements a basic implementation of the\nproposed overhaul of the tx-relay v2 [0]. Currently, the tx flow is\nINV(txid) -> ; <- GETDATA(inv(txid)) ; TX(tx) -> . With the proposed \ntx-relay\nv2 overhaul, if an INV for the txid has not previously received for the\ntransaction, i.e the transaction processing has not been requested, the\ntransaction is strictly rejected, without further processing. This more\nstricter tx processing can be activated with a setting option in bitcoin\nbackbone.\n\nLong-term, I think some form of tx-relay link-level mitigation is a strong\nnecessity to diminish the surface attack of time-sensitive contracting\nprotocol in face of tx-relay throughput overflow, where a malicious peer\nis buying out your full-node tx bandwidth to tamper with the propagation\nof a time-sensitive tx (e.g a lightning's HTLC-preimage) [1].\n\nThe d",
          "drama_signals": {
            "drama_keywords": 3,
            "positive_keywords": 1,
            "text_length": 2068,
            "has_nack": false,
            "has_ack": true
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 3
      }
    },
    {
      "title": "[bitcoindev] OP_CHECKUTXOSETHASH idea",
      "message_count": 1,
      "participants": [
        "Erik Aronesty"
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/CAJowKgLE4kb7qT1NxXrmEssr8+fQGd-=7=m-BAsjePoti8TRRg@mail.gmail.com",
          "title": "[bitcoindev] OP_CHECKUTXOSETHASH idea",
          "author": "Erik Aronesty <erik@q32@com>",
          "date": "Mon, 29 Sep 2025 17:09:15 -0700",
          "body": "[-- Attachment #1: Type: text/plain, Size: 1671 bytes --]\n\nA soft fork could introduce a new opcode, `OP_CHECKUTXOSETHASH`, allowing\nminers to optionally commit a deterministic hash of the current UTXO set\ninto a block. If present, all nodes must verify its correctness or reject\nthe block; if absent, the block is still valid. Old nodes treat the opcode\nas unspendable, so backward compatibility is preserved.\n\nBecause computing the full UTXO root is costly, this makes each checkpoint\nintentionally expensive to produce, ensuring that miners will only include\nthem when compensated with sufficient fees. Additionally, it could be\nlimited to one per block.\n\nThe result is a voluntary, self-limiting, incentive-aligned, fee-driven\nsystem where checkpoints are cheaply consensus-enforced when included but\nnever mandatory.\n\nMost nodes could operate on a rolling history validated by occasional,\nhigh-value commitments, while archival nodes remain free to preserve the\nfull chain. This reduces the burden of initial sync and resource use\nwithout sacrificing Bitcoin’s security model, since any invalid checkpoint\nwould invalidate its block.\n\nIn practice, the chain becomes more efficient for everyday use while the\nhistorical record remains intact for those willing to bear the expense of\nmaintaining it.\n\n-- \nYou received this message because you are subscribed to the Google Groups \"Bitcoin Development Mailing List\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to bitcoindev+unsubscribe@googlegroups•com.\nTo view this discussion visit https://groups.google.com/d/msgid/bitcoindev/CAJowKgLE4kb7qT1NxXrmEssr8%2BfQGd-%3D7%3Dm-BAsjePoti8TRRg%40mail.gmail.com.\n\n[-- Attachment #2: Type: text/html, Size: 2187 bytes --]",
          "drama_signals": {
            "drama_keywords": 2,
            "positive_keywords": 1,
            "text_length": 1794,
            "has_nack": false,
            "has_ack": true
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 2
      }
    },
    {
      "title": "[bitcoindev] Re: OP_CHECKUTXOSETHASH idea",
      "message_count": 1,
      "participants": [
        "Eric Voskuil"
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/76c1b5a1-5b78-4576-981f-4df69aefc9a6n@googlegroups.com",
          "title": "[bitcoindev] Re: OP_CHECKUTXOSETHASH idea",
          "author": "Eric Voskuil <eric@voskuil@org>",
          "date": "Sun, 16 Nov 2025 11:11:16 -0800 (PST)",
          "body": "[-- Attachment #1.1: Type: text/plain, Size: 2923 bytes --]\n\nHi Erik,\n\n>  Most nodes could operate on a rolling history validated by occasional, \nhigh-value commitments, while archival nodes remain free to preserve the \nfull chain.\n\nThis is an old big-blocker idea, and still a terrible one. It effectively \nreduces what we now call validation to majority hash power control. IOW \nfunctionally equivalent to SPV. A few actual full nodes (maybe) validating \ndoes not have the implied effect. For a node's validation to matter, the \nnode has to be accepting coin in trade. SPV entirely relies on the \npresumption that a very large portion of economic activity is actually \nvalidated. Very large means enough that majority hash power has a true \ndisincentive to intentionally mine invalid blocks, despite the reward for \ndoing so (e.g. unlimited inflation). What you are calling \"archival nodes\" \ndon't actually \"preserve the full chain\" for everyone else, because their \neffect is limited to their own transactions. Otherwise we are talking about \nfraud proofs, which is a conversation that doesn't end well.\n\n>  Because computing the full UTXO root is costly...\n\nIt is not, it's getting cheaper every year.\n\ne\n\nOn Monday, September 29, 2025 at 8:11:51 PM UTC-4 Erik Aronesty wrote:\n\nA soft fork could introduce a new opcode, `OP_CHECKUTXOSETHASH`, allowing \nminers to optionally commit a deterministic hash of the current UTXO set \ninto a block. If present, all nodes must verify its correctness or reject \nthe block; if absent, the block is still valid. Old nodes treat the opcode \nas unspendable, so backward compatibility is preserved. \n\nBecause computing the full UTXO root is costly, this makes each checkpoint \nintentionally expensive to produce, ensuring that miners will only include \nthem when compensated with sufficient fees. Additionally, it could be \nlimited to one per block.\n\nThe result is a voluntary, self-limiting, incentive-aligned, fee-driven \nsystem where checkpoints are cheaply ",
          "drama_signals": {
            "drama_keywords": 2,
            "positive_keywords": 1,
            "text_length": 2042,
            "has_nack": false,
            "has_ack": true
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 2
      }
    },
    {
      "title": "Re: [bitcoindev] [BIP Proposal] Reduced Data Temporary Softfork",
      "message_count": 1,
      "participants": [
        "\"/dev /fd0\""
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/CALiT-ZrT+R7ApsQJOtKM5h5xTThGL-WMyCRDwt29sXmt4AA+Rg@mail.gmail.com",
          "title": "Re: [bitcoindev] [BIP Proposal] Reduced Data Temporary Softfork",
          "author": "\"/dev /fd0\" <alicexbtong@gmail@com>",
          "date": "Tue, 28 Oct 2025 14:46:26 +0530",
          "body": "[-- Attachment #1: Type: text/plain, Size: 9718 bytes --]\n\nHi Greg,\n\n> There are also proposals such as BIP 337:\nhttps://github.com/bitcoin/bips/blob/master/bip-0337.mediawiki  none of\nthese things are consensus changes-- many aren't even bippable because they\ndon't have interoperability considerations (e.g. representations on\ndisk/memory).\n\n[Compression][0] of raw transactions is interesting although it won't be\nhelpful in this case. I had reviewed the pull request that implemented BIP\n337 in bitcoin core and was closed. Maybe we can add it in knots.\n\n> Forget for a moment the (un)likelyhood that the concerns being discussed\nare meaningfully modulated by exactly how data is represented in p2p,\nmemory, rpc, disk, etc.. for assumption just assume they are.\n\n> If so, the correct move would be to change those encodings rather than\nany consensus rule change--- particularly because any consensus rule method\nwill simply be evaded, and can't provide the level of swizzling that\nchanging the encoding can accomplish.  Encoding changes are also\nsubstantially non-coercive: people who think they're valuable can adopt\nthem and leave other people alone.\n\nI am not sure if encoding or encryption would be the right approach but\nthis is worth trying. I have opened an [issue][1] in knots repository to\ndiscuss these ideas and also created a web [page][2] that shows the binary\nfor OP_RETURN in a transaction.\n\n[0[: https://github.com/bitcoin/bitcoin/pull/29134\n[1]: https://github.com/bitcoinknots/bitcoin/issues/229\n[2]: https://opreturn01.github.io/\n\n/dev/fd0\nfloppy disk guy\n\nOn Tue, Oct 28, 2025 at 1:28 AM Greg Maxwell <gmaxwell@gmail•com> wrote:\n\n> The only consensus normative data encoding in Bitcoin is the order data\n> goes into hashes.  The representations in memory, rpc, in the p2p network,\n> etc. are already different and could be made arbitrarily different without\n> any consensus change.  Case in point:  the data is now normally encrypted\n> on disk and in P2P.  There are also prop",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 2064,
            "has_nack": false,
            "has_ack": false
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 0
      }
    },
    {
      "title": "[bitcoindev] BIP54 implementation and test vectors",
      "message_count": 1,
      "participants": [
        "\"'Antoine Poinsot' via Bitcoin Development Mailing List\""
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/V0qeILOW1CuH3NS2O8IUdQBK8i3o8LwzLNGf7xh1UO0S_Gzui1CpdP5NhdT3EtrW6NgqxJ538egeag6bVZoBX8C8E46ZYTCyPg1qBxkwCXs=@protonmail.com",
          "title": "[bitcoindev] BIP54 implementation and test vectors",
          "author": "\"'Antoine Poinsot' via Bitcoin Development Mailing List\" <bitcoindev@googlegroups.com>",
          "date": "Tue, 21 Oct 2025 15:46:04 +0000",
          "body": "Hi everyone,\n\nI'd like to give an update on my Consensus Cleanup work, now BIP54.\n\nI opened an implementation against Bitcoin Inquisition v29.1 at [0]. It contains extensive testing\nof each of the four proposed mitigations, and was used as a basis to generate test vectors for\nBIP54. I opened a PR against the BIPs repository to add them to BIP54 [1].\n\nThe test vectors for the transaction-level sigops limit contain a wide variety of usage combinations\nas well as ways of running into the limit. They also include some historical violations as well as\npathological transactions demonstrating the implementation details of the sigop accounting logic\n(which was itself borrowed from that of BIP16, which all Bitcoin implementations presumably already\nhave).\n\nThe test vectors for the new witness-stripped transaction size restriction similarly exercise the\nbounds of the check under various conditions (e.g. transactions with/without a witness). All\nhistorical violations were also added to the test vectors, thanks to Chris Stewart for digging those\nup.\n\nBecause the new timestamp restrictions are tailor-made to the mainnet difficulty adjustment\nparameters, the test vectors for those contain a number of chains of mainnet headers (from genesis).\nEach test case contains a full header chain and whether it is valid according to BIP54. These chains\nwere generated using a custom miner available in [2] and added to the implementation as a JSON data\nfile.\n\nThe test vectors for the coinbase restriction similarly include a chain of mainnet blocks, because\nthe timelock check is context-dependent. These were generated using a similar miner also available\nat [2].\n\nI'm seeking feedback on these test vectors from everybody but in particular developers of\nalternative Bitcoin clients, as compatibility with other Bitcoin implementations than Bitcoin Core\nwas a design goal.\n\nBest,\nAntoine Poinsot\n\n[0]: https://github.com/bitcoin-inquisition/bitcoin/pull/99\n[1]: https://github.com/bitcoin/bips/pull/201",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 1,
            "text_length": 2051,
            "has_nack": false,
            "has_ack": true
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 0
      }
    },
    {
      "title": "Re: [bitcoindev] Re: BIP54 implementation and test vectors",
      "message_count": 1,
      "participants": [
        "Antoine Riard"
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/5fe13896-913c-4b16-8507-69809b369612n@googlegroups.com",
          "title": "Re: [bitcoindev] Re: BIP54 implementation and test vectors",
          "author": "Antoine Riard <antoine.riard@gmail@com>",
          "date": "Sun, 9 Nov 2025 17:40:58 -0800 (PST)",
          "body": "[-- Attachment #1.1: Type: text/plain, Size: 11336 bytes --]\n\nHi Poinsot,\n\nThanks for the precision. Yes my wonder is more if you put yourself in the \nshoes of an attacker,\nand you have to calculate your cost for an attack, what is the most \ninteresting between playing on\nthe number of prevout lookups and maximizing the quadratic hashing. I do \nbelieve the proposed 2500\nsigops limit is slashing the quadratic hashing worst-case concern, while at \nthe same time not providing\nan advantage to the attacker on the prevout lookup cost. Say differently, I \nbelieve we should ensure that\nany introduced DoS limit in the goal to reduce worst-case for a DoS vector \nA do not downgrade the worst-case\nfor another DoS vector B. \n\nPreviously, as the way the novel limit was proposed in abstracto, I had a \nconcern with given that\nif you take for example bitcoin core multiple input checks where made \n(first all scripts flags and\nthen for consensus mandatory script flags) [0], a DoS attacker could have \ndeliberately make the\nscript failed on a policy flag and then make it hard fails on the novel \n2500 limit, _at a cheaper\nprice_ (less CHECKMULTISIG bytes to pack in the tx). I don't think it's a \nconcern anymore as after\n[1] and others, there is no double validation anymore and \n`CheckSigOpsBIP54` has been implemented\nwith the other policy check limits.\n\nOf course the number of CHECKMULTISIG bytes to pack is only a concern for \nan attacker in the situation\nwhere satoshis have to be provided to pass the `min_relay_feerate` policy \nrule, but it's a realistic\nlimit one has to reason when you're considering the cost of network-wide \nDoS. Somehow, you're maximizing\nthe higher DoS cost per byte per satoshi you might have to commit in a \nsingle tx.\n\nDisagree with you on the prevout lookup cost exploitation, as I think there \nis at least variant to\nattempt to slash the cost for an attacker for some categories of DoS. But \nyes seen the calculations\nfor various DoS blocks, and that can be discussed",
          "drama_signals": {
            "drama_keywords": 2,
            "positive_keywords": 2,
            "text_length": 2059,
            "has_nack": false,
            "has_ack": true
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 2
      }
    },
    {
      "title": "Re: [bitcoindev] LNHANCE a soft-fork package",
      "message_count": 1,
      "participants": [
        "Brandon Black"
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/aQzcGITQE3ckHB7K@console",
          "title": "Re: [bitcoindev] LNHANCE a soft-fork package",
          "author": "Brandon Black <freedom@reardencode@com>",
          "date": "Thu, 6 Nov 2025 09:34:16 -0800",
          "body": "As the original proposer of LNHANCE, let it be known that I still think\nit's a great direction for bitcoin script improvements. This set of\nopcodes is carefully crafted to offer significant utility with minimal\nfootguns or even sharp corners.\n\nI am also a fan of the competing TEMPLATEHASH+CSFS+IKEY proposal which\nremoves the blunted corner of modifying legacy script while adding the\nblunted corner of committing to the annex. It also removes the hacky\nsibling commitment via scriptSig, loses the ability to use simple vaults\nwith legacy ECDSA-only signing infrastructure, and saves just under 8vB\nin a typical lightning symmetry uncontested force close scenario.\n\nBoth are excellent directions to move bitcoin script and I hope to see\none of the two enforced on the network in the (not too distant) future.\n\nBest,\n\n--Brandon\n\n-- \nYou received this message because you are subscribed to the Google Groups \"Bitcoin Development Mailing List\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to bitcoindev+unsubscribe@googlegroups•com.\nTo view this discussion visit https://groups.google.com/d/msgid/bitcoindev/aQzcGITQE3ckHB7K%40console.",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 2,
            "text_length": 1220,
            "has_nack": false,
            "has_ack": true
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 0
      }
    },
    {
      "title": "Re: [bitcoindev] [Pre-BIP Discussion] Bitcoin Node Repository Consensus-Policy Separation",
      "message_count": 1,
      "participants": [
        "Juan Aleman"
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/73a08ea3-b9be-424b-a1cb-beac3206723cn@googlegroups.com",
          "title": "Re: [bitcoindev] [Pre-BIP Discussion] Bitcoin Node Repository Consensus-Policy Separation",
          "author": "Juan Aleman <bitcoindev@juanaleman@com>",
          "date": "Fri, 31 Oct 2025 11:51:18 -0700 (PDT)",
          "body": "[-- Attachment #1.1: Type: text/plain, Size: 7260 bytes --]\n\nHello Matt, thanks for your response.\n\nI searched about libbitcoinkernel and it does look like some effort is \nbeing put into the creation of this library.\n\nBut again, my focus is SPECIFICALLY on the powerful influence of the \nbitcoin/bitcoin repo itself. If you don't this merits a BIP, what would be \nthe appropriate avenue to address and potentially do something about \nreorganizing the repo itself?\n\nOn Friday, October 31, 2025 at 2:41:30 PM UTC-4 Matt Corallo wrote:\n\n> You should probably dig into the libbitcoinkernel project (and the immense \n> amount of work that has gone into it, as well as the immense amount of work \n> that it requires). Also this is not anything that would merit a BIP.\n>\n> On Oct 31, 2025, at 14:20, Juan Aleman <bitco...@juanaleman•com> wrote:\n>\n> ﻿Hello bitcoin developers,\n>\n>\n>\n> My name is Juan Alemán, and this is my first post to the mailing list. But \n> I've been involved with Bitcoin since 2017. First only as a hard money \n> investor, but later also as a developer, specially fascinated by this \n> permanent medium. I hope this proposal can be appreciated by all \n> perspectives as a pragmatic (maybe unorthodox, but timely) solution to move \n> forward in agreement.\n>\n> The changes in v30 defaults got my attention (similar to many of you), as \n> they are completely opposite to what has historically been \"standard\" \n> practice. A highly controversial change that surfaces the influence over \n> default policy in the network, escalating to the point of a fork proposal \n> <https://github.com/bitcoin/bips/pull/2017>.\n>\n> First, it must be reminded that a fork should be unnecessary if defaults \n> are simply reverted <https://github.com/bitcoin/bitcoin/pull/33682>, \n> while still allowing all policy possibilities.\n>\n> After my second PR <https://github.com/bitcoin/bitcoin/pull/33690> \n> attempt was (also) closed (and I was blocked from the repo), I realized \n> that the main issue here is s",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 1,
            "text_length": 2090,
            "has_nack": false,
            "has_ack": false
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 0
      }
    },
    {
      "title": "Re: [bitcoindev] On (in)ability to embed data into Schnorr",
      "message_count": 1,
      "participants": [
        "waxwing/ AdamISZ"
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/31d18bd9-62e0-4035-b04f-f70ff4253257n@googlegroups.com",
          "title": "Re: [bitcoindev] On (in)ability to embed data into Schnorr",
          "author": "waxwing/ AdamISZ <ekaggata@gmail@com>",
          "date": "Sun, 2 Nov 2025 05:30:44 -0800 (PST)",
          "body": "[-- Attachment #1.1: Type: text/plain, Size: 4249 bytes --]\n\n> I already told you, when I said \"known plaintext attack\". If you want to \nput random data into private keys or signatures, then things are hard to \nbreak. However, if it is something useful for the reader, then usually, \nthat kind of data are non-random. For example: some users store \ntransactions inside OP_RETURNs, and they use ASCII hex representation. If \nthey would use binary encoding, then they would save 50% space. But people \nsimply don't care.\n\n> And the similar case is possible here: if you want to store random data, \nthen it is hard to use this method. However, if you want to store ASCII \ntext, where many words can be found in a dictionary, or where the format of \nthe data is known upfront, or can be easily guessed, then the security of \nthe keys, is comparable to the brainwallets.\n\n> Which means, that you can just put your data into the private key of the \nuser, and a \"signature nonce\" (which is nothing else, but yet another \nprivate key, placed on secp256k1). And then, if you know, that your data, \nis for example \"ASCII string\", then it means, that each and every key, that \nyou produce, simply leaks at least 32 bits per 256-bit key, if not more.\n\nAh, right; I had originally written a response to this idea but then \ndiscarded it on the basis that it's kinda \"obvious\" that we shouldn't think \nabout that, and focused on the more in-the-weeds concept of a lattice \nattack instead.\n\nBut it isn't obvious.\n\nSo let's think of the spectrum here. First, the most trivial nonce to \nbreak: one consisting of a single bit (OK technically you can't encode k=0, \nheh, but, whatever, put it in the second bit of the string). Obviously that \nis extractable, getting 32 bytes plus one bit. That one extra bit above the \n33% is achievable because of \"grinding\" except here grinding is the most \ntrivial version possible: trying 2 alternatives. This still fits my \noriginal claim, which is \"33% plus whatever you can get f",
          "drama_signals": {
            "drama_keywords": 1,
            "positive_keywords": 1,
            "text_length": 2059,
            "has_nack": false,
            "has_ack": true
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 1
      }
    },
    {
      "title": "[bitcoindev] segOP potential BIP discussion",
      "message_count": 1,
      "participants": [
        "defenwycke"
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/c38d00f7-a42b-4f7b-899e-11e823a43d7dn@googlegroups.com",
          "title": "[bitcoindev] segOP potential BIP discussion",
          "author": "defenwycke <cal.defenwycke@gmail@com>",
          "date": "Wed, 29 Oct 2025 16:40:50 -0700 (PDT)",
          "body": "[-- Attachment #1.1: Type: text/plain, Size: 13060 bytes --]\n\nHi all,\n\nI attempted to propose a BIP earlier today. I was notified of my incorrect \nactions versus core procedures.\n\nSee attached or below - my proposal for discussion.\n\nRegards\n\nDefenwycke.\n\n---\n\nA proposed discussion of segOP\nAuthor: Defenwycke / defenwycke@icloud•com\n29.10.2025\n\nsegOP (Segregated OP_RETURN)\n\n    A proposed extension to Bitcoin transactions. It introduces a \ndedicated, structured, full-fee data lane for on-chain data, without \ndisrupting existing transaction rules. Think of it like this - segOP is to \narbitrary data what SegWit was to signatures — a clean, isolated, \nforward-compatible path that preserves old-node harmony while restoring fee \nfairness.\n\nAbstract:\n\n    segOP defines a new segregated data section for Bitcoin transactions, \ncryptographically committed via OP_SUCCESS184. It standardizes on-chain \nmetadata storage by enforcing full fees, structured TLV encoding, and a 100 \nKB cap, while remaining backward-compatible with legacy nodes.\n\nIt is not:\n\n    - A replacement for OP_RETURN\n    - An off-chain mechanism\n    - A hard fork\n\nIt is:\n\n    - A soft-fork-safe, future-proof, SegWit-style data section\n    - Full-fee (4 weight units per byte)\n    - Structured (TLV + Merkle-root verified)\n    - Limited to 100 KB per transaction\n\nWhat issues could segOP rectify?:\n\n    1. Ordinals abuse witness discount. segOP will apply full fee rate for \nlarge data.\n    2. No structured metadata lane. segOP introduces TLV-encoded + \nMerkle-verified section.\n    3. Witness discount abused for megabytes. segOP Enforces 100 KB cap.\n    4. OP_RETURN unstructured & limited. segOP = structured + verifiable.\n    5. Spam cheap storage. segOP deters spam with fees.\n\n    In short - segOP restores fairness — data pays its real weight cost, \npreserving block space for financial use.\n\nWhere segOP lives:\n\n    Transaction Layout (Post-segOP)\n    Transaction\n    ├── nVersion\n    ├── vin (inputs)\n    ├── vout (o",
          "drama_signals": {
            "drama_keywords": 2,
            "positive_keywords": 1,
            "text_length": 2044,
            "has_nack": false,
            "has_ack": true
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 2
      }
    },
    {
      "title": "Re: [bitcoindev] Re: segOP potential BIP discussion",
      "message_count": 1,
      "participants": [
        "\"'moonsettler' via Bitcoin Development Mailing List\""
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/3S1IMGSH6z-3Ho81Ugp9o-ltTwpIC-4ow6bn6aEAtK4XrkG5HTkvTw0BeZFpPILfabdp7rz_LDHEBWX_XZk0a7nKR4sJRUp_3B7pAMaJ86I=@protonmail.com",
          "title": "Re: [bitcoindev] Re: segOP potential BIP discussion",
          "author": "\"'moonsettler' via Bitcoin Development Mailing List\" <bitcoindev@googlegroups.com>",
          "date": "Sat, 01 Nov 2025 12:00:26 +0000",
          "body": "Hi Defenwycke,\n\nI think this is not a horrible idea, there might be future demand for a pruneable proof of publication space.\n\nBut, your proposal does not provide an incentive for anyone to adopt it. If it was cheaper than witness space,\nI think it would be a serious consideration especially for rollups.\nThe idea that the bytes incur full cost (4WU) makes it on arrival economically speaking.\n\nAlso it's a bit unclear how consensus and nodes in sync would interact with the \"recent window\".\n\nIt's a reasonable approach to mandate the presence of such an extension block near the chain-tip, but\nnodes by default should not download or verify it during IBD. This would only add a constant burden to nodes,\nwhile allowing bitcoin to scale on higher layers more that require such proof of publication mechanism for\ntheir security.\n\nThinking that this would be used by graffiti type payloads that especially are seeking the permanence, persistence\nand replication of bitcoin transactional data, or that metaprotocols that want to use the block space as an archival\nlayer for their own token ledgers is I'm afraid completely misguided or even delusional.\n\nBR,\nmoonsettler\n\n-- \nYou received this message because you are subscribed to the Google Groups \"Bitcoin Development Mailing List\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to bitcoindev+unsubscribe@googlegroups•com.\nTo view this discussion visit https://groups.google.com/d/msgid/bitcoindev/3S1IMGSH6z-3Ho81Ugp9o-ltTwpIC-4ow6bn6aEAtK4XrkG5HTkvTw0BeZFpPILfabdp7rz_LDHEBWX_XZk0a7nKR4sJRUp_3B7pAMaJ86I%3D%40protonmail.com.",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 1668,
            "has_nack": false,
            "has_ack": false
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 0
      }
    },
    {
      "title": "Re: [bitcoindev] [BIP Proposal] Soft Fork Compromise on op_return to Resolve Current Bitcoin Controversies",
      "message_count": 1,
      "participants": [
        "Melvin Carvalho"
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/CAKaEYhKdhJ1vP5BBYNAMF7557M9dSoQx8_uqjdAzRM3DgA6ajg@mail.gmail.com",
          "title": "Re: [bitcoindev] [BIP Proposal] Soft Fork Compromise on op_return to Resolve Current Bitcoin Controversies",
          "author": "Melvin Carvalho <melvincarvalho@gmail@com>",
          "date": "Fri, 31 Oct 2025 07:31:03 +0100",
          "body": "[-- Attachment #1: Type: text/plain, Size: 13893 bytes --]\n\nčt 30. 10. 2025 v 20:35 odesílatel Martin Habovštiak <\nmartin.habovstiak@gmail•com> napsal:\n\n> \"Honest\" only refers to miners not trying to reverse the transactions by\n> making an alternative chain. It has nothing to do with your subjective\n> evaluation of the transactions worth trying to censor the \"bad ones\".\n>\n\nHi Martin,\n\nSmall clarification per the white paper: \"honest\" isn’t only about\nreversing transactions. Section 8 of the white paper also discusses honest\nnodes and which transactions are accepted into blocks.\n\nSatoshi further clarified standardness:\n\"The design supports a broad range of transaction types that are possible,\nbut not all are standard. Standard transactions are the ones that are\ndesigned to be used for the common case.\"\n-- June 2010\n\nPer Section 11, the white paper's security model shows honest miners (e.g.\nfollowing standard operation) outpacing alternatives. In practice, miners\nfollowing a standardness agreement or soft fork for OP_RETURN would, on\naffected blocks, earn around $100,000 more, than under a mixed policy,\nmaking prioritizing standard transactions the economically optimal strategy.\n\nThis is because there are a finite number of blocks before each halving, so\nthe opportunity cost of non-standard payloads is half the subsidy, which is\nmore than enough economic upside to offset fees. A standardness agreement\nsoft fork is therefore economically compelling for miners: it aligns with\nlong-standing practice and provides long-term incentives.\n\nBest,\nMelvin\n\n\n>\n> Bitcoin was specifically designed to prevent censorship of transactions\n> that follow the consensus rules. Trying to go against it makes you a fool\n> at best or an attacker at worst.\n>\n> Dňa št 30. 10. 2025, 18:37 Melvin Carvalho <melvincarvalho@gmail•com>\n> napísal(a):\n>\n>>\n>>\n>> čt 30. 10. 2025 v 3:16 odesílatel Greg Maxwell <gmaxwell@gmail•com>\n>> napsal:\n>>\n>>> I searched for the source of your quotation and am unable",
          "drama_signals": {
            "drama_keywords": 3,
            "positive_keywords": 2,
            "text_length": 2107,
            "has_nack": false,
            "has_ack": true
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 3
      }
    },
    {
      "title": "[bitcoindev] [BIP Proposal] Limit ScriptPubkey Size >= 520 Bytes Consensus.",
      "message_count": 1,
      "participants": [
        "PortlandHODL"
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/6f6b570f-7f9d-40c0-a771-378eb2c0c701n@googlegroups.com",
          "title": "[bitcoindev] [BIP Proposal] Limit ScriptPubkey Size >= 520 Bytes Consensus.",
          "author": "PortlandHODL <admin@qrsnap@io>",
          "date": "Thu, 2 Oct 2025 13:42:06 -0700 (PDT)",
          "body": "[-- Attachment #1.1: Type: text/plain, Size: 1881 bytes --]\n\nProposing: Softfork to after (n) block height; the creation of outpoints \nwith greater than 520 bytes in the ScriptPubkey would be consensus invalid. \n\nThis is my gathering of information per BIP 0002\n\nAfter doing some research into the number of outpoints that would have \nviolated the proposed rule there are exactly 169 outpoints. With only 8 \nbeing non OP_RETURN. I think after 15 years and not having discovered use \nfor 'large' ScriptPubkeys; the reward for not invalidating them at the \nconsensus level is lower than the risk of their abuse. \n\n   - \n*Reasons for *\n      - Makes DoS blocks likely impossible to create that would have any \n      sufficient negative impact on the network.\n      - Leaves enough room for hooks long term\n      - Would substantially reduce the divergence between consensus  and \n      relay policy\n      - Incredibly little use onchain as evidenced above.\n      - Could possibly reduce codebase complexity. Legacy Script is largely \n      considered a mess though this isn't a complete disablement it should reduce \n      the total surface that is problematic.\n      - Would make it harder to use the ScriptPubkey as a 'large' \n      datacarrier.\n      - Possible UTXO set size bloat reduction.\n      \n      - *Reasons Against *\n      - Bitcoin could need it in the future? Quantum?\n      - Users could just create more outpoints.\n   \nThoughts?\n\nsource of onchain data  \n<https://github.com/portlandhodl/portlandhodl/blob/main/greater_520_pubkeys.csv>\n\nPortlandHODL\n\n-- \nYou received this message because you are subscribed to the Google Groups \"Bitcoin Development Mailing List\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to bitcoindev+unsubscribe@googlegroups•com.\nTo view this discussion visit https://groups.google.com/d/msgid/bitcoindev/6f6b570f-7f9d-40c0-a771-378eb2c0c701n%40googlegroups.com.\n\n[-- Attachment #1.2: Type: text/html, Size: 2232 bytes --",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 2076,
            "has_nack": false,
            "has_ack": false
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 0
      }
    },
    {
      "title": "Re: [bitcoindev] Re: [BIP Proposal] Limit ScriptPubkey Size >= 520 Bytes Consensus.",
      "message_count": 1,
      "participants": [
        "Garlo Nicon"
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/CAN7kyNi2xxEY1LZTb_WMXKtFiDf8Epi3VN7HLhsNimOAEMH1xg@mail.gmail.com",
          "title": "Re: [bitcoindev] Re: [BIP Proposal] Limit ScriptPubkey Size >= 520 Bytes Consensus.",
          "author": "Garlo Nicon <garlonicon@gmail@com>",
          "date": "Tue, 21 Oct 2025 21:05:55 +0200",
          "body": "[-- Attachment #1: Type: text/plain, Size: 11189 bytes --]\n\n> bare multisigs can easily violate the proposed limit\n\nGood point. I can imagine a Script, which could allow something like\n1-of-460 multisig: if all public keys would be hashed, then it would take\n21 bytes to push any given hash. And then, pushing 460 hashes would take\n460*21=9660 bytes. Then, to clean it up, OP_2DROP would consume two\nelements, so that means 230 bytes, to clean it up with OP_2DROPs, and a\nsingle OP_DROP.\n\nI think it could be something like that:\n\nOP_TOALTSTACK <9660 bytes> OP_FROMALTSTACK OP_PICK OP_TOALTSTACK\n<OP_2DROP*229> OP_DROP\nOP_DUP OP_HASH160 OP_FROMALTSTACK OP_EQUALVERIFY OP_CHECKSIG\n\nAnd then, this is how it could be executed:\n\n<sig> <pubkey> <number>                    //input stack\n<sig> <pubkey>                             //toaltstack\n<sig> <pubkey> <lots_of_hashes>            //pushing hashes\n<sig> <pubkey> <lots_of_hashes> <number>   //pushing number\n<sig> <pubkey> <lots_of_hashes> <hash>     //picking hash\n<sig> <pubkey> <lots_of_hashes>            //toaltstack\n<sig> <pubkey>                             //dropping the rest\n<sig> <pubkey> <pubkey>                    //dup\n<sig> <pubkey> <hash>                      //hash160\n<sig> <pubkey> <hash> <hash>               //fromaltstack\n<sig> <pubkey>                             //equalverify\nOP_TRUE                                    //checksig\n\nWhich means, that it would take 9660 bytes, plus 230 bytes of dropping, so\nit sums up to 9890 bytes. Then, 9 bytes are consumed by the rest of the\nenvelope, and that leaves 101 bytes for signature and public key. Seems\ndoable. But I didn't check it in regtest yet.\n\npon., 20 paź 2025 o 18:48 Greg Maxwell <gmaxwell@gmail•com> napisał(a):\n\n> Perhaps it's also worth explicitly pointing out for people following at\n> home how this proposal has a very real confiscation risk:  bare\n> multisigs can easily violate the proposed limit-- if uncompressed points\n> are used an \"of 8\" policy is suffici",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 2,
            "text_length": 2084,
            "has_nack": false,
            "has_ack": true
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 0
      }
    },
    {
      "title": "[bitcoindev] Policy restrictions Was: [BIP Proposal] Limit ScriptPubkey Size >= 520 Bytes Consensus.",
      "message_count": 1,
      "participants": [
        "\"'Russell O'Connor' via Bitcoin Development Mailing List\""
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/CAMZUoKnyYpfJ1fZRan7BzyGvMitxznoSyCXkjxc2Qy5Z9pNvMA@mail.gmail.com",
          "title": "[bitcoindev] Policy restrictions Was: [BIP Proposal] Limit ScriptPubkey Size >= 520 Bytes Consensus.",
          "author": "\"'Russell O'Connor' via Bitcoin Development Mailing List\" <bitcoindev@googlegroups.com>",
          "date": "Thu, 30 Oct 2025 16:27:34 -0400",
          "body": "[-- Attachment #1: Type: text/plain, Size: 1423 bytes --]\n\nOn Thu, Oct 30, 2025 at 2:40 AM Greg Maxwell <gmaxwell@gmail•com> wrote:\n\n> I don't even think bitcoin has ever policy restricted something that was\n> in active use, much less softforked out something like that.\n>\n\nI invite the Bitcore lore experts to correct me here, but I recall someone\nmany years ago finding that their bare multisig funds (likely related to\nthe Counterparty nonsense) were stuck by policy due to some new policy\nbeing enacted to mandate that pubkeys in bare multisigs must now all be\non-curve points ... or something like that.  I do hope that they managed to\nget their funds recovered by now with direct miner intervention.\n\nI really ought to vet my claim above by going through my IRC logs and\nBitcoin development history ... but a quicker way is to post a claim\npublicly on the internet and wait for someone else to call it out as being\nwrong.\n\nAlso, I think this type of policy change quite harmful and shouldn't be\nreplicated, and ideally reverted, assuming my story is correct.\n\n-- \nYou received this message because you are subscribed to the Google Groups \"Bitcoin Development Mailing List\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to bitcoindev+unsubscribe@googlegroups•com.\nTo view this discussion visit https://groups.google.com/d/msgid/bitcoindev/CAMZUoKnyYpfJ1fZRan7BzyGvMitxznoSyCXkjxc2Qy5Z9pNvMA%40mail.gmail.com.\n\n[-- Attachment #2: Type: text/html, Size: 2077 bytes --]",
          "drama_signals": {
            "drama_keywords": 1,
            "positive_keywords": 0,
            "text_length": 1613,
            "has_nack": false,
            "has_ack": false
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 1
      }
    },
    {
      "title": "[bitcoindev] Re: Policy restrictions Was: [BIP Proposal] Limit ScriptPubkey Size >= 520 Bytes Consensus.",
      "message_count": 1,
      "participants": [
        "\"'Russell O'Connor' via Bitcoin Development Mailing List\""
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/CAMZUoKmPfbwJApAeYkXs6U9Syuj4KjbcsH3aFJ7desFnHxyyTw@mail.gmail.com",
          "title": "[bitcoindev] Re: Policy restrictions Was: [BIP Proposal] Limit ScriptPubkey Size >= 520 Bytes Consensus.",
          "author": "\"'Russell O'Connor' via Bitcoin Development Mailing List\" <bitcoindev@googlegroups.com>",
          "date": "Thu, 30 Oct 2025 18:23:20 -0400",
          "body": "[-- Attachment #1: Type: text/plain, Size: 2548 bytes --]\n\nFine, I ended up looking into it.\n\nPR 5247 <http://github.com/bitcoin/bitcoin/pull/5247> changed the semantics\nof STRICTENC policy in late 2014 so that during a CHECKMULTISIGVERIFY, if a\npubkey with an incorrect prefix is encountered, the script fails.  The\nprevious behaviour was that if a pubkey was invalid, the check failed for\nonly that pubkey and processing continued on to the next pubkey.\n\nI still have to go through my IRC logs, but my recollection is there is at\nleast one person who had their funds \"soft confiscated\" in the sense that\nthey were now, by policy only, unable to spend their UTXOs and would\nrequire bypassing policy to retrieve their funds.\n\nPeople who have better databases than me are welcome to search through bare\nmultisig UTXOs to see if there are any having a strict subset of malformed\npubkeys in them.\n\nSo one minor correction to my story: it wasn't a matter of the pubkey being\noff-curve, but rather having an invalid prefix / invalid encoding.\n\nOn Thu, Oct 30, 2025 at 4:27 PM Russell O'Connor <roconnor@blockstream•com>\nwrote:\n\n> On Thu, Oct 30, 2025 at 2:40 AM Greg Maxwell <gmaxwell@gmail•com> wrote:\n>\n>> I don't even think bitcoin has ever policy restricted something that was\n>> in active use, much less softforked out something like that.\n>>\n>\n> I invite the Bitcore lore experts to correct me here, but I recall someone\n> many years ago finding that their bare multisig funds (likely related to\n> the Counterparty nonsense) were stuck by policy due to some new policy\n> being enacted to mandate that pubkeys in bare multisigs must now all be\n> on-curve points ... or something like that.  I do hope that they managed to\n> get their funds recovered by now with direct miner intervention.\n>\n> I really ought to vet my claim above by going through my IRC logs and\n> Bitcoin development history ... but a quicker way is to post a claim\n> publicly on the internet and wait for someone else to call it ou",
          "drama_signals": {
            "drama_keywords": 1,
            "positive_keywords": 0,
            "text_length": 2105,
            "has_nack": false,
            "has_ack": false
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 1
      }
    },
    {
      "title": "[bitcoindev] By: Doctorbuzz1 {GitHub} Limit \"Bulk Dust\" with a default filter or consensus.",
      "message_count": 1,
      "participants": [
        "Frenchanfry"
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/b0dee827-c2fe-4a68-9b41-f1447ba3c1d3n@googlegroups.com",
          "title": "[bitcoindev] By: Doctorbuzz1 {GitHub} Limit \"Bulk Dust\" with a default filter or consensus.",
          "author": "Frenchanfry <andujon18@gmail@com>",
          "date": "Wed, 29 Oct 2025 17:03:45 -0700 (PDT)",
          "body": "[-- Attachment #1.1: Type: text/plain, Size: 10004 bytes --]\n\nA proposal on GitHub I found Highly interesting and a better improvement, \ndealing with spammers/congestion.\n\nI’m exploring a potential default filter or consensus-level rule (since a \nlarge number of people believe that default filters don't work) to \ndiscourage UTXO-bloat patterns without touching Script, witness data, or \nthe block size limit.\n\nThe idea is to target “bulk dust” transactions — those that create large \nnumbers of extremely small outputs — which are the main cause of long-term \nUTXO set growth.\n\nThese types of \"bulk dust\" transactions have been the No. 1 reason cited \nfor wanting to expand the default OP_RETURN limit... and removing that \nlimit obviously influenced BIP 444. So it appears to me that there is \noverwhelming majority support for limiting these types of \"bulk dust\" \ntransactions, as they do present a legitimate concern for node runners.\n\nConcept\n\nFlag a transaction as “bulk dust” if:\n\n   - It has >=100 outputs each below a dynamically defined TinyTx \n   threshold, and\n   - Those tiny outputs make up >=60% of all outputs in the transaction.\n\nWhen flagged, it would be considered nonstandard (relay policy) or invalid \n(if soft-forked into consensus).\n\nTinyTx threshold (dynamic halving schedule)\n\nI originally considered a constant definition of what was a \"tiny\" Tx to be \n1,000 sats... but some might still just use 1,001 sats, right? Plus there \nvery likely will be a time where there is a valid use-case of >100 outputs \nunder 1,000 sats.\n\nRather than fixing the “tiny” threshold to a constant like 1,000 sats, the \nrule defines it as a decreasing function of block height, starting high and \ngradually tightening over time.\n\n   - Starts at 4096 sats when activated (target ~2028).\n   - Halves every 210,000 blocks (~4 years).\n   - Never falls below 1 sat (hard floor).\n\nYear ---- Block Height -- TinyTx Threshold\n2028 --- ~activation ---- 4096 sats\n2032 --- ~1,260,000 ---- 2048 sats\n2036 ",
          "drama_signals": {
            "drama_keywords": 2,
            "positive_keywords": 0,
            "text_length": 2092,
            "has_nack": false,
            "has_ack": false
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 2
      }
    },
    {
      "title": "[bitcoindev] Re: By: Doctorbuzz1 {GitHub} Limit \"Bulk Dust\" with a default filter or consensus.",
      "message_count": 1,
      "participants": [
        "Doctor Buzz"
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/15849db6-67af-4a03-86a4-bb5a288b9ad1n@googlegroups.com",
          "title": "[bitcoindev] Re: By: Doctorbuzz1 {GitHub} Limit \"Bulk Dust\" with a default filter or consensus.",
          "author": "Doctor Buzz <buzzheavy@gmail@com>",
          "date": "Thu, 30 Oct 2025 07:14:32 -0700 (PDT)",
          "body": "[-- Attachment #1.1: Type: text/plain, Size: 12555 bytes --]\n\nA preemptive response to those who might say that a conservative \n\"tiny_count\" of 100 \"wouldn't do anything\":\n\nThe point is to add friction without inhibiting any non-data Txs. The image \nof Pepe pumping iron with \"UTXO\" on top was stored in 1,859 fake pubkeys / \nUTXOs. The proposed tiny_count of 100 would split that particular image \nacross at least 19 Txs (likely a lot more if on-chain indexing were used), \nwhich only adds at least +6% to fees, but it does ruin \"atomicity\" (images \nall-in-one Tx) by adding complexity of needing some type of index to link \nthem, causes confirmation risk, & pushes data abusers toward OP_RETURN or \nwitness space.\n\nChanging the 100 tiny_count to 50 ≈ +11% fees; to 30 ≈ +16% fees; & to 20 ≈ \n+24% fees (this only takes into account an extra 200 bytes per additional \ninput Tx and does not consider any additional indexing needs) . Perhaps a \ntiny_count could be 20 with a higher ratio of 70%?? ~24% extra fees + added \ncomplexity could definitely prevent a lot of UTXO abuse.  I was obviously \njust trying to avoid ALL false positives, but there definitely seems like \nthere's room to move the tiny_count lower.\n\nOn Wednesday, October 29, 2025 at 8:15:08 PM UTC-5 Doctor Buzz wrote:\n\n> Thanks!  I came here to post it myself.  I just want to point out that \n> it's awfully discouraging for a GitHub mod to \"close\" my 90% developed \n> code, asking me to post it elsewhere... but anyway!\n>\n> Original GitHub post here:\n> https://github.com/bitcoin/bitcoin/issues/33737#issuecomment-3465288829\n>\n> The first concept of this with static definition of a \"tiny\" Tx was posted \n> here (with no responses):  \n> https://bitcoin.stackexchange.com/questions/129139/would-a-bulk-dust-relay-consensus-rule-limiting-100-sub-1-000-sat-outputs-p\n>\n> Pastebin code probably looks better here than what I can see in the OP of \n> this thread:  https://pastebin.com/9qdQCH83\n> On Wednesday, October 29, 2025 at 7:47:08",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 1,
            "text_length": 2096,
            "has_nack": false,
            "has_ack": true
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 0
      }
    },
    {
      "title": "[bitcoindev] [Concept] Anticipation Pool - Off-chain scaling using miner-validated transaction forwarding",
      "message_count": 1,
      "participants": [
        "Jakob Widmann"
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/55e36b59-76c2-4ffc-8f36-9a9a0c2fc02bn@googlegroups.com",
          "title": "[bitcoindev] [Concept] Anticipation Pool - Off-chain scaling using miner-validated transaction forwarding",
          "author": "Jakob Widmann <jweb.git@gmail@com>",
          "date": "Wed, 29 Oct 2025 03:22:41 -0700 (PDT)",
          "body": "[-- Attachment #1.1: Type: text/plain, Size: 5302 bytes --]\n\n\n\nHello everyone,\n\nI've been thinking extensively about how we could scale Bitcoin, \nparticularly looking for alternatives to Lightning's limitations. My main \nfrustrations with Lightning are the need for watchtowers, and routing \ncomplexities that arise from channel liquidity limitations. I wondered if \nwe could leverage the existing mining infrastructure instead of building \nseparate systems.\n\nMy goal was to find a scaling solution that:\n\n\n   - Eliminates watchtower requirements by using miners (who are already \n   always online)\n   - Avoids channel-based routing complexities\n   - Guarantees eventual settlement\n   - Creates natural economic incentives for all participants\n\nI'm not a Bitcoin developer, but I wanted to share this concept with the \ncommunity to see if others think it could work and how it might be \nimplemented. I'm particularly interested in feedback on the technical \nfeasibility.\n\nHere's the concept:\n\n*Anticipation Pool*\n\nAnticipation transactions (ATX) are pre-signed Bitcoin transactions that \nare forwardable to anyone without immediate blockchain settlement.\n\nHere's how it works:\n\nInstead of publishing transactions to the mempool to add them to the \nblockchain, you create a pre-signed ATX that goes into a special pool \ncalled the anticipation pool, managed by miners. Once enough miners have \nvalidated your ATX and added it to the pool (validation threshold to be \ndetermined, e.g., 10-30% of hashpower depending on amount), the recipient \ncan consider it \"confirmed\". Double-spending is prevented because miners \ntimestamp and validate each forward, with first-seen winning, and check \nagainst the anticipation pool, mempool and blockchain to ensure the outputs \nhaven't been spent elsewhere.\n\nThe ATX has a timelock (e.g. 30 days), during this time only the recipient \ncan publish it to the mempool, forward it to someone else (including to \nthemselves), or split it to multiple people. Each forwa",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 1,
            "text_length": 2106,
            "has_nack": false,
            "has_ack": true
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 0
      }
    },
    {
      "title": "Re: [bitcoindev] op_return spam is beneficial",
      "message_count": 1,
      "participants": [
        "Erik Aronesty"
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/CAJowKg+Yzdfq+7fW0AVDxZ3HLe7UbPoOUeh6ejScaqqC=zdFrw@mail.gmail.com",
          "title": "Re: [bitcoindev] op_return spam is beneficial",
          "author": "Erik Aronesty <erik@q32@com>",
          "date": "Sun, 26 Oct 2025 20:39:39 -0700",
          "body": "[-- Attachment #1: Type: text/plain, Size: 3499 bytes --]\n\nwhen blocks aren't full, fees are negligible, miners aren't sufficiently\nrewarded, and we risk hashpower decline\n\nif they remain not full, we should probably reduce the block size\n\nand improve the ability to securely share UTXOs so more people can use them\n\nOn Sun, Oct 26, 2025, 5:52 PM nt yl <wrapperband@googlemail•com> wrote:\n\n> Hi Erik\n>\n> As far as I understand -\n>\n> Bitcoin blocks aren’t all the same size -  they only share a maximum size\n> limit.\n>\n> OP_RETURN outputs don’t actually “save” UTXOs; they just create\n> unspendable outputs that never enter the UTXO set, while still adding to\n> the permanent blockchain size.\n>\n> That reduces memory load slightly but increases disk and bandwidth\n> requirements. So it’s not a free trade-off, and it’s misleading to suggest\n> OP_RETURN use has no decentralization cost.\n>\n> Dr D\n>\n> On Sun, Oct 26, 2025 at 7:46 PM Erik Aronesty <erik@q32•com> wrote:\n>\n>> There is a strictly limited supply of money available for spam.  (\n>> Because if we don't assume this, then we have to assume that all financial\n>> transactions will get crowded out by infinite funds. )\n>>\n>> Every time OP_RETURN is used, instead of other UTXO bearing transactions,\n>> at least one UTXO is saved.  UTXO spam is dangerous for decentralization\n>> (blockchain spam isn't since blocks are always the same size)\n>>\n>> Since spam funds are limited, this is money spent to boost hashpower, and\n>> removes money from the spam pool that would otherwise create UTXO bloat.\n>>\n>> Therefore it's irrational to prevent large (or at least multiple) being\n>> used for spam.\n>>\n>> The only reason to limit this opcode is to \"virtue signal that Bitcoin is\n>> not intended to store data\".\n>>\n>> And while this may have some benefits in the short term given the ongoing\n>> social attack against core maintainers, in the long term I think it might\n>> have very negative repercussions to cave to a social attack that has little\n>> t",
          "drama_signals": {
            "drama_keywords": 3,
            "positive_keywords": 1,
            "text_length": 2046,
            "has_nack": false,
            "has_ack": true
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 3
      }
    },
    {
      "title": "[bitcoindev] Full Disclosure: Debug console history storing sensitive info in bitcoin core v24.0-v30.0",
      "message_count": 1,
      "participants": [
        "\"/dev /fd0\""
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/CALiT-ZoGahaZye2UikvMQ0uuMn-LKrMVGJ6PLVHwO3BwvO5dwg@mail.gmail.com",
          "title": "[bitcoindev] Full Disclosure: Debug console history storing sensitive info in bitcoin core v24.0-v30.0",
          "author": "\"/dev /fd0\" <alicexbtong@gmail@com>",
          "date": "Fri, 24 Oct 2025 21:29:09 +0530",
          "body": "[-- Attachment #1: Type: text/plain, Size: 2586 bytes --]\n\nHi everyone,\n\nThis is a disclosure of a low-severity vulnerability that exists in all\nbitcoin core versions from v24.0 to v30.0. It has already been reported in\na GitHub [issue][0] and shared on social media. However, I wanted to\nformally disclose it on the mailing list so that all users are aware of the\nrisks. The full disclosure approach is primarily used when vulnerabilities\nare ignored. It is exactly what happened in this case although it has been\n[fixed][1] in bitcoin knots which also persists the history to disk.\n\nSome RPC commands use private keys, wallet passphrase etc. in their\narguments and this remained in the debug console history until [2016][2].\nAn attacker can no longer see the history and get sensitive information\nwith the history filter. However, [`migratewallet`][3] wasn't added in the\nhistory filter. This allows an attacker with access to the victim's machine\nto get the wallet passphrase from the history. GUI has an option to migrate\nthe wallet without using RPC commands in the debug console since v26.0 but\nsome users may prefer RPC over it.\n\n```\n// don't add private key handling cmd's to the history\nconst QStringList historyFilter = QStringList()\n    << \"signmessagewithprivkey\"\n    << \"signrawtransactionwithkey\"\n    << \"walletpassphrase\"\n    << \"walletpassphrasechange\"\n    << \"encryptwallet\";\n\n\n}\n```\n\nTimeline:\n02 October 2025: User [reported][4] the issue in bitcoin knots telegram\ngroup\n02 October 2025: I opened the pull request to fix the issue in knots repo\n11 October 2025: [knots v29.2][5] released with the fix\n11 October 2025: I acknowledged the bug in bitcoin core repo and\nwaketraindev opened a [pull request][6] to fix it\n24 October 2025: Full disclosure as bitcoin core remains vulnerable\n\nCredits:\nwaketraindev\nlukedashjr\n\n[0]: https://github.com/bitcoin-core/gui/issues/897\n[1]: https://github.com/bitcoinknots/bitcoin/pull/203\n[2]: https://github.com/bitcoin/bitcoin/pull/8877\n[3]: h",
          "drama_signals": {
            "drama_keywords": 1,
            "positive_keywords": 1,
            "text_length": 2103,
            "has_nack": false,
            "has_ack": true
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 1
      }
    },
    {
      "title": "[bitcoindev] Public disclosure of 4 Bitcoin Core security advisories",
      "message_count": 1,
      "participants": [
        "\"'Antoine Poinsot' via Bitcoin Development Mailing List\""
      ],
      "messages": [
        {
          "url": "https://gnusha.org/pi/bitcoindev/I5lwexjm1EkKFZpV4_A4b6XvYXvIGjJZ3UpYhfzeC4rXmnNDVQ0Mob4X1We1hmWaisx_0ZSNn6BKH99kfig6rTChHbsCPMZBk2k0ua1E8Ng=@protonmail.com",
          "title": "[bitcoindev] Public disclosure of 4 Bitcoin Core security advisories",
          "author": "\"'Antoine Poinsot' via Bitcoin Development Mailing List\" <bitcoindev@googlegroups.com>",
          "date": "Fri, 24 Oct 2025 15:53:15 +0000",
          "body": "Hi everyone,\n\nIn accordance with our security disclosure policy, i am sharing today four advisories for\n*low-severity* security vulnerabilities fixed in Bitcoin Core version 30.0.\n\nTwo weeks ago we pre-announced that we would release advisories for five low-severity\nvulnerabilities. One of these has since been promoted to medium severity, and its public\ndisclosure has therefore been rescheduled in accordance with our policy.\n\nThe four vulnerabilities publicly disclosed today are the following:\n- CVE-2025-54604: Disk filling from spoofed self connections [0]\n- CVE-2025-54605: Disk filling from invalid blocks [1]\n- CVE-2025-46597: Highly unlikely remote crash on 32-bit systems [2]\n- CVE-2025-46598: CPU DoS from unconfirmed transaction processing [3]\n\nThe fixes for CVE-2025-54604, CVE-2025-54605 and CVE-2025-46597 are also included in Bitcoin Core\nversion 29.1 and later minor releases. Thanks to Eugene Siegel, Niklas Goegge and Pieter Wuille for\nreporting these issues and to everyone involved in fixing them.\n\nOur disclosure policy as well as previously disclosed vulnerabilities are available on the Bitcoin\nCore website at [4].\n\nAntoine Poinsot\n\n[0]: https://bitcoincore.org/en/2025/10/24/disclose-cve-2025-54604/\n[1]: https://bitcoincore.org/en/2025/10/24/disclose-cve-2025-54605/\n[2]: https://bitcoincore.org/en/2025/10/24/disclose-cve-2025-46597/\n[3]: https://bitcoincore.org/en/2025/10/24/disclose-cve-2025-46598/\n[4]: https://bitcoincore.org/en/security-advisories/\n\n-- \nYou received this message because you are subscribed to the Google Groups \"Bitcoin Development Mailing List\" group.\nTo unsubscribe from this group and stop receiving emails from it, send an email to bitcoindev+unsubscribe@googlegroups•com.\nTo view this discussion visit https://groups.google.com/d/msgid/bitcoindev/I5lwexjm1EkKFZpV4_A4b6XvYXvIGjJZ3UpYhfzeC4rXmnNDVQ0Mob4X1We1hmWaisx_0ZSNn6BKH99kfig6rTChHbsCPMZBk2k0ua1E8Ng%3D%40protonmail.com.",
          "drama_signals": {
            "drama_keywords": 0,
            "positive_keywords": 0,
            "text_length": 2002,
            "has_nack": false,
            "has_ack": false
          }
        }
      ],
      "drama_signals": {
        "drama_keywords": 0
      }
    }
  ],
  "summary": {
    "total_threads": 50,
    "total_messages": 50,
    "unique_participants": 37
  }
}